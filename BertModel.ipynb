{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the position embedding matrix such that `pos_embed[i]` is the embedding for position `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_encoding_size = 256\n",
    "\n",
    "def position_embeddings(max_pos, size):\n",
    "    embeddings = np.zeros((max_pos, size))\n",
    "    w = 1 / (10000 ** (2*np.arange(size // 2 )/size))\n",
    "    for pos in range(max_pos):\n",
    "        embeddings[pos,0::2] = np.sin(w*pos)\n",
    "        embeddings[pos,1::2] = np.cos(w*pos)\n",
    "    return torch.Tensor(embeddings)\n",
    "    \n",
    "pos_embed = position_embeddings(10000, position_encoding_size)\n",
    "pos_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used on top of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, bert_size, position_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculates the attention value\n",
    "        self.attention = nn.Linear(bert_size + position_size, 1)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "        # Makes the prediction\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(bert_size, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 6),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    '''\n",
    "    embeddings: shape (segment count, 512, bert_hidden_size)\n",
    "        The output of BERT\n",
    "    position_encodings:  shape (segment count, 512, position_encoding_size)\n",
    "        The position encodings of the tokens\n",
    "    comment_bounds: Array of tuples of the form [(start, end)]. \n",
    "        comment_bounds[i] = (a, b) indicates that comment i's embeddings can be extracted as embeddings[a:b]\n",
    "        if not provided, all embeddings and positions are assumed to be a single comment\n",
    "    '''\n",
    "    def forward(self, embeddings, position_encodings, comment_bounds = None):\n",
    "        # Concatenate each BERT output with its position encoding\n",
    "        attention_input = torch.cat([embeddings, position_encodings], dim=2) # (batch, 512, position_size + bert_hidden_size)\n",
    "\n",
    "        # Get the attention weights for each concatenated vector\n",
    "        attentions = self.attention(attention_input) #  (batch size, 512, 1)\n",
    "        \n",
    "        # If no bounds are probided, assume the input is all one comment\n",
    "        if comment_bounds is None:\n",
    "            # Softmax over attentions\n",
    "            attentions = self.softmax(attentions) # (batch, 512, 1)\n",
    "            \n",
    "            # Calculate the total embedding as a weighted sum of embeddings without the positional encodings\n",
    "            vecs = torch.sum(attentions * embeddings, dim=1) # (batch, bert_hidden_size)\n",
    "            return self.prediction(vecs) # (batch, 1)\n",
    "\n",
    "        # Otherwise, get the outputs for each comment\n",
    "        vecs = []\n",
    "        for (a,b) in comment_bounds:\n",
    "            # Get the embeddings and attentions for the current comment\n",
    "            comment_embeddings = embeddings[a:b] # (segment_count, 512, bert_hidden_size)\n",
    "            comment_attentions = attentions[a:b] # (segment_count, 512, 1)\n",
    "            \n",
    "            # softmax over the attentions for the comment\n",
    "            attention_weights = self.softmax(comment_attentions) # (segment_count, 512, 1)\n",
    "            \n",
    "            # Calculate the total embedding as a weighted sum over the embeddings of the comment\n",
    "            weighted_embeddings = attention_weights * embeddings[a:b] # (segment_count, 512, bert_hidden_size)\n",
    "            vec = torch.sum(weighted_embeddings.view(-1, weighted_embeddings.shape[-1]), dim=0, keepdim=True) # (segment_count, bert_hidden_size)\n",
    "            \n",
    "            vecs.append(vec)\n",
    "            \n",
    "        # Stack the total embedding vectors and give them to the prediction network to calculate the output\n",
    "        return self.prediction(torch.cat(vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset that does the data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    # file_format is a Python format string with a single variable to be inserted. It's used to get the paths of the files\n",
    "    # normalize indicates whether to perform data normalization procedure\n",
    "    def __init__(self, file_format, normalize):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load the data from files\n",
    "        input_ids = torch.load(file_format.format(\"input_ids\"))\n",
    "        positions = torch.load(file_format.format(\"positions\"))\n",
    "        comment_ids = torch.load(file_format.format(\"ids\"))\n",
    "        targets = torch.load(file_format.format(\"targets\"))\n",
    "\n",
    "        # Treat the targets as binary to separate the possible outputs\n",
    "        target_ids = torch.sum(torch.Tensor([32, 16, 8, 4, 2, 1]) * targets, axis=1)\n",
    "\n",
    "        # Store the data according to the target. Useful for normalization\n",
    "        self.data = [[] for i in range(64)]\n",
    "\n",
    "        # Load the data into the array\n",
    "        start_index = 0\n",
    "        end_index = 0\n",
    "        n_comments = comment_ids.shape[0]\n",
    "        # Group the items by which comment they're part of\n",
    "        while start_index < n_comments:\n",
    "            # Get the current comment id\n",
    "            curr_id = comment_ids[start_index]\n",
    "            \n",
    "            # Find end_index such that input_ids[end_index-1] is the last segment of the comment\n",
    "            while end_index < n_comments and comment_ids[end_index] == comment_ids[start_index]:\n",
    "                end_index += 1\n",
    "            \n",
    "            # Get the number with a binary representation that's the same as the comment's true labels\n",
    "            target_id = int(target_ids[curr_id].item())\n",
    "            \n",
    "            # Get the comment as a tuple containing (token ids, positions, true labels)\n",
    "            data = (input_ids[start_index:end_index], positions[start_index:end_index], targets[curr_id])\n",
    "            self.data[target_id].append(data)\n",
    "\n",
    "            start_index = end_index\n",
    "\n",
    "        # Remove the empty arrays from the data\n",
    "        self.data = [data for data in self.data if data]\n",
    "        \n",
    "        # Calculate how many comments are nontoxic\n",
    "        n_nontoxic = len(self.data[0])\n",
    "        \n",
    "        # Calculate how many comments there need to be with each combination of labels\n",
    "        # The goal is for there to be as many toxic comments as nontoxic comments\n",
    "        # Also, each combination of labels should be present an equal number of times\n",
    "        n_of_each = n_nontoxic // (len(self.data)-1)\n",
    "        \n",
    "        # Calculate how many copies need to be made from each combination of labels\n",
    "        n_copies = np.array([1]+[n_of_each // len(self.data[i]) for i in range(1,len(self.data))])\n",
    "        \n",
    "        # If normalization shouldn't be done, just replace n_copies with a bunch of ones so there is one copy of each comment\n",
    "        if not normalize:\n",
    "            n_copies = np.ones_like(n_copies)\n",
    "        \n",
    "        # The number of comments with each combination of labels\n",
    "        self.data_length = np.array([len(data) for data in self.data])\n",
    "\n",
    "        # The data is organized into segments each of which contains some number of copies of the comments with a specific combination of labels\n",
    "        segment_lengths = n_copies*self.data_length\n",
    "\n",
    "        # The total length of the normalized dataset\n",
    "        self.length = int(np.sum(segment_lengths))\n",
    "\n",
    "        # The indices bounding the segments\n",
    "        self.boundaries = np.zeros(segment_lengths.shape[0]+1)\n",
    "        self.boundaries[1:] = np.cumsum(segment_lengths)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Find the segment that the index is in\n",
    "        for i in range(self.boundaries.shape[0] - 1):\n",
    "            if index >= self.boundaries[i] and index < self.boundaries[i+1]:\n",
    "                # index - self.boundaries[i] calculates the index into the segment\n",
    "                # The segment is a bunch of copies of the same data, but it's inefficient to actually copy the data\n",
    "                # Therefore, \"% self.data.length[i]\" is used to convert the index into the segment into the index into the data\n",
    "                inner_index = int((index - self.boundaries[i]) % self.data_length[i])\n",
    "                return self.data[i][inner_index]\n",
    "        print(\"Index: \", index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. \n",
    "\n",
    "The test data isn't normalized to give a realistic view of the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275845, 153164)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = MyDataset(\"train_{}.pt\", True)\n",
    "test_dataset = MyDataset(\"test_{}.pt\", False)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the data into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to format the data in a way that the model can use\n",
    "# batch is a list of tuples (tokenized_comment, positions, true_labels)\n",
    "def collate_samples(batch):\n",
    "    # Split the tuples into three variables\n",
    "    split_comments, positions, targets = zip(*batch)\n",
    "    \n",
    "    # An array of tuples (a,b) so that input_ids[a:b] are all part of a single comment\n",
    "    comment_bounds = np.zeros(len(split_comments)+1, dtype=np.int32)\n",
    "    comment_bounds[1:] = np.cumsum(list(map(len, split_comments)))\n",
    "    comment_bounds = np.array([comment_bounds[:-1], comment_bounds[1:]], dtype=np.int32).transpose()\n",
    "    \n",
    "    # For parallelism, stack the inputs into single tensors\n",
    "    input_ids = torch.cat(split_comments, dim=0)\n",
    "    \n",
    "    # Stack the position embeddings as well so they can be easily concatenated with the BERT output\n",
    "    encoded_positions = torch.cat([\n",
    "                          # Use the position array as indices into the position embedding\n",
    "                          pos_embed[position_arr]\n",
    "                          # For each comment in the batch\n",
    "                          for position_arr in positions                     \n",
    "                      ])\n",
    "  \n",
    "    # Stack the true labels\n",
    "    targets = torch.stack(targets)\n",
    "    return input_ids, encoded_positions, comment_bounds, targets\n",
    "\n",
    "batch_size = 72\n",
    "\n",
    "train = DataLoader(train_dataset, \n",
    "                   batch_size = batch_size,\n",
    "                   shuffle=True,\n",
    "                   collate_fn = collate_samples)\n",
    "\n",
    "test = DataLoader(test_dataset, \n",
    "                   batch_size = batch_size,\n",
    "                   shuffle=True,\n",
    "                   collate_fn = collate_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([78, 512]) torch.Size([78, 512, 256]) (72, 2) torch.Size([72, 6])\n"
     ]
    }
   ],
   "source": [
    "for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(train):\n",
    "    print(input_ids.shape, encoded_position.shape, comment_bounds.shape, target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Kaiming normal weight initialization when possibe and when not, just use normal initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model, stdv):\n",
    "    for param in model.parameters():\n",
    "        if len(param.shape) >= 2:\n",
    "            nn.init.kaiming_normal_(param.data)\n",
    "        else:\n",
    "            param.data.normal_(0.0, stdv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate AUC given predicted and true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_auc(pred, target):\n",
    "    result = []\n",
    "    for i in range(pred.shape[1]):\n",
    "        # If the true labels only has one value in a column, add a fake item to make AUC a valid operation\n",
    "        if len(np.unique(target[:,i])) == 2:\n",
    "            result.append(roc_auc_score(target[:,i], pred[:,i], labels=[0,1]))\n",
    "        else:\n",
    "            extra = np.array([1-target[0,i]])\n",
    "            target_i = np.concatenate((target[:,i], extra))\n",
    "            pred_i = np.concatenate((pred[:,i], extra))\n",
    "            result.append(roc_auc_score(target_i, pred_i, labels=[0,1]))\n",
    "            \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to save and load the model. Because ADAM has its own internal state, it's saved as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(tag):\n",
    "    model.load_state_dict(torch.load(model_path.format(tag)))\n",
    "    optimizer.load_state_dict(torch.load(adam_path.format(tag)))\n",
    "    \n",
    "def save_model(tag):\n",
    "    torch.save(model.state_dict(), model_path.format(tag))\n",
    "    torch.save(optimizer.state_dict(), adam_path.format(tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format strings used to generate the model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/model_{}.pt\"\n",
    "adam_path = \"./adam/adam_{}.pt\"\n",
    "train_loss_path = \"./train_loss.pt\"\n",
    "test_loss_path = \"./train_loss.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the pretrained BERT model and freeze it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "bert_hidden_size = 768\n",
    "\n",
    "bert.eval()\n",
    "\n",
    "# Put BERT on the GPU\n",
    "bert = bert.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the model to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_epoch = 0\n",
    "epochs = 30\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model = Classifier(bert_hidden_size, position_encoding_size)\n",
    "init_weights(model, 0.2)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to load saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = None #\"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(epoch, batch, loss)\n",
    "\n",
    "if load is not None:\n",
    "    load_model(load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training prints too many lines, so this class is used to write them to a file after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger:\n",
    "    def __init__(self, path):\n",
    "        self.file = open(path, \"w\")\n",
    "    \n",
    "    def log(self, string):\n",
    "        print(string)\n",
    "        self.file.write(string)\n",
    "        self.file.write(\"\\n\")\n",
    "    \n",
    "    def close(self):\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 0: 00000/3832 (24285.3s remaining)\t BCE Loss: 1.0032\n",
      "\tAUC: Toxic: 0.5000 Severe Toxic: 0.6032 Obscene: 0.4914 Threat: 0.4615 Insult: 0.5169 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.2639 Severe Toxic: 0.8889 Obscene: 0.7917 Threat: 0.1667 Insult: 0.2083 Identity-based Hate: 0.2083\n",
      "Epoch 0: 00001/3832 (23134.3s remaining)\t BCE Loss: 0.9881\n",
      "\tAUC: Toxic: 0.5000 Severe Toxic: 0.5311 Obscene: 0.4957 Threat: 0.4815 Insult: 0.4782 Identity-based Hate: 0.4734\n",
      "\tAccuracy: Toxic: 0.2778 Severe Toxic: 0.8333 Obscene: 0.7986 Threat: 0.1806 Insult: 0.2639 Identity-based Hate: 0.3194\n",
      "Epoch 0: 00002/3832 (22351.2s remaining)\t BCE Loss: 0.6578\n",
      "\tAUC: Toxic: 0.4989 Severe Toxic: 0.5179 Obscene: 0.4971 Threat: 0.4642 Insult: 0.5313 Identity-based Hate: 0.4761\n",
      "\tAccuracy: Toxic: 0.3056 Severe Toxic: 0.8194 Obscene: 0.7963 Threat: 0.3796 Insult: 0.4259 Identity-based Hate: 0.4537\n",
      "Epoch 0: 00003/3832 (21793.9s remaining)\t BCE Loss: 0.5639\n",
      "\tAUC: Toxic: 0.4979 Severe Toxic: 0.5237 Obscene: 0.5055 Threat: 0.4907 Insult: 0.5335 Identity-based Hate: 0.4792\n",
      "\tAccuracy: Toxic: 0.3993 Severe Toxic: 0.8264 Obscene: 0.7743 Threat: 0.4965 Insult: 0.5104 Identity-based Hate: 0.5243\n",
      "Epoch 0: 00004/3832 (22231.2s remaining)\t BCE Loss: 0.5160\n",
      "\tAUC: Toxic: 0.4902 Severe Toxic: 0.5426 Obscene: 0.5104 Threat: 0.4807 Insult: 0.5352 Identity-based Hate: 0.5015\n",
      "\tAccuracy: Toxic: 0.4528 Severe Toxic: 0.8333 Obscene: 0.7750 Threat: 0.5500 Insult: 0.5639 Identity-based Hate: 0.5861\n",
      "Epoch 0: 00005/3832 (22717.1s remaining)\t BCE Loss: 0.4973\n",
      "\tAUC: Toxic: 0.5221 Severe Toxic: 0.5491 Obscene: 0.5096 Threat: 0.4914 Insult: 0.5543 Identity-based Hate: 0.5132\n",
      "\tAccuracy: Toxic: 0.5139 Severe Toxic: 0.8380 Obscene: 0.7940 Threat: 0.5995 Insult: 0.6157 Identity-based Hate: 0.6296\n",
      "Epoch 0: 00006/3832 (22228.5s remaining)\t BCE Loss: 0.5247\n",
      "\tAUC: Toxic: 0.5179 Severe Toxic: 0.5500 Obscene: 0.5079 Threat: 0.4903 Insult: 0.5604 Identity-based Hate: 0.5110\n",
      "\tAccuracy: Toxic: 0.5397 Severe Toxic: 0.8452 Obscene: 0.7857 Threat: 0.6270 Insult: 0.6508 Identity-based Hate: 0.6508\n",
      "Epoch 0: 00007/3832 (21739.7s remaining)\t BCE Loss: 0.6755\n",
      "\tAUC: Toxic: 0.5038 Severe Toxic: 0.5463 Obscene: 0.5067 Threat: 0.4837 Insult: 0.5507 Identity-based Hate: 0.5073\n",
      "\tAccuracy: Toxic: 0.5469 Severe Toxic: 0.8368 Obscene: 0.7795 Threat: 0.6389 Insult: 0.6649 Identity-based Hate: 0.6632\n",
      "Epoch 0: 00008/3832 (21431.5s remaining)\t BCE Loss: 0.5583\n",
      "\tAUC: Toxic: 0.5077 Severe Toxic: 0.5418 Obscene: 0.5060 Threat: 0.4861 Insult: 0.5424 Identity-based Hate: 0.5066\n",
      "\tAccuracy: Toxic: 0.5679 Severe Toxic: 0.8380 Obscene: 0.7809 Threat: 0.6574 Insult: 0.6744 Identity-based Hate: 0.6759\n",
      "Epoch 0: 00009/3832 (21187.5s remaining)\t BCE Loss: 0.5856\n",
      "\tAUC: Toxic: 0.5104 Severe Toxic: 0.5415 Obscene: 0.5051 Threat: 0.4903 Insult: 0.5393 Identity-based Hate: 0.5067\n",
      "\tAccuracy: Toxic: 0.5833 Severe Toxic: 0.8375 Obscene: 0.7708 Threat: 0.6764 Insult: 0.6861 Identity-based Hate: 0.6875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f1ff46b1f20c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Get the outputs from the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_comments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_position\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_bounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# How often to print\n",
    "log_frequency = 1\n",
    "\n",
    "# How many batches to run between saving the model\n",
    "save_frequency = 10\n",
    "\n",
    "# How many epochs to run before clearing the output\n",
    "clear_frequency = 3\n",
    "\n",
    "# Maximum batches per epoch\n",
    "batches_per_train_epoch = 100\n",
    "batches_per_test_epoch = 10\n",
    "\n",
    "batches_per_train_epoch = min(batches_per_train_epoch, len(train))\n",
    "batches_per_test_epoch = min(batches_per_test_epoch, len(test))\n",
    "\n",
    "# The format string used to generate paths for the log files\n",
    "log_path = \"./logs/epoch_{}.txt\"\n",
    "\n",
    "# Store the times to help forecast how long training will take\n",
    "epoch_times = []\n",
    "batch_times = []\n",
    "\n",
    "# Store the training loss after each batch\n",
    "train_losses = []\n",
    "for epoch in range(first_epoch, epochs):\n",
    "    # initialize the logger object\n",
    "    logger = EpochLogger(log_path.format(epoch))\n",
    "    \n",
    "    # Record the time at the start of the epoch\n",
    "    epoch_start = time()\n",
    "    \n",
    "    # Predictions and true labels used to calculate the AUC\n",
    "    predicted = None\n",
    "    true = None\n",
    "    \n",
    "    # Recording how many were correct and the total number of predictions to calculate the accuracy\n",
    "    n_correct = 0\n",
    "    n_processed = 0\n",
    "    print(\"Training\")\n",
    "    for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(train):\n",
    "        batch_start = time()\n",
    "        # Get the BERT output\n",
    "        with torch.no_grad():\n",
    "            encoded_comments = bert(input_ids.to(device))[0]\n",
    "    \n",
    "        # Get the outputs from the network\n",
    "        output = model(encoded_comments, encoded_position.to(device), comment_bounds)\n",
    "\n",
    "        # Gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Store how long this batch took to run\n",
    "        batch_times.append(time() - batch_start)\n",
    "        \n",
    "        # Only use the last 100 batches to estimate the time remaining\n",
    "        batch_times = batch_times[-100:]\n",
    "        \n",
    "        # Calculate the predicted labels by rounding to zero or 1\n",
    "        pred = torch.round(output.cpu().detach())\n",
    "        \n",
    "        # Add the predicted and true labels to the arrays\n",
    "        if predicted is None:\n",
    "            predicted = pred.clone()\n",
    "            true = target.clone()\n",
    "        else:\n",
    "            predicted = torch.cat((predicted, pred), dim=0)\n",
    "            true = torch.cat((true, target), dim=0)\n",
    "\n",
    "        # Calculate the number that were correct\n",
    "        n_correct += torch.sum(pred == target, axis=0).numpy()\n",
    "        n_processed += pred.shape[0]\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "        # Print debugging information\n",
    "        if i % log_frequency == 0:\n",
    "            logger.log(\"Epoch {}: {:05d}/{} ({:7.01f}s remaining)\\t BCE Loss: {:.04f}\".format(epoch, i, batches_per_train_epoch, np.mean(batch_times)*(batches_per_train_epoch - i), loss.item()))\n",
    "            auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "            logger.log(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "            acc = n_correct / n_processed\n",
    "            logger.log(\"\\tAccuracy: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "        # Save the model\n",
    "        if i % save_frequency == 0:\n",
    "            save_model(\"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(epoch, i, loss.item()))\n",
    "            torch.save(torch.Tensor(train_losses), train_loss_path)\n",
    "        # Break early \n",
    "        if i % batches_per_train_epoch == 0 and i != 0:\n",
    "            break\n",
    "    \n",
    "    # Make sure that the model is saved at the end of the epoch\n",
    "    save_model(\"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(epoch, i, loss.item()))\n",
    "    \n",
    "    # Save the training losses\n",
    "    torch.save(torch.Tensor(train_losses), train_loss_path)\n",
    "    \n",
    "    # Test the model\n",
    "    with torch.no_grad():\n",
    "        print(\"Testing\")\n",
    "        predicted = None\n",
    "        for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(test):\n",
    "            batch_start = time()\n",
    "            # Get the BERT output\n",
    "            encoded_comments = bert(input_ids.to(device))[0]\n",
    "\n",
    "            # Get the outputs from the network\n",
    "            output = model(encoded_comments, encoded_position.to(device), comment_bounds)\n",
    "\n",
    "            #print(output[0], target[0])\n",
    "            # Gradient descent\n",
    "            pred = torch.round(output.cpu().detach())\n",
    "            if predicted is None:\n",
    "                predicted = pred.clone()\n",
    "                true = target.clone()\n",
    "            else:\n",
    "                predicted = torch.cat((predicted, pred), dim=0)\n",
    "                true = torch.cat((true, target), dim=0)\n",
    "\n",
    "            n_correct += torch.sum(pred == target, axis=0).numpy()\n",
    "            n_processed += pred.shape[0]\n",
    "\n",
    "            predicted = predicted[-1000:]\n",
    "            true = true[-1000:]\n",
    "\n",
    "            if i % log_frequency == 0:\n",
    "                logger.log(\"Epoch {}: {:05d}/{} ({:7.01f}s remaining)\\t BCE Loss: {:.04f}\".format(epoch, i, batches_per_test_epoch, np.mean(batch_times)*(batches_per_test_epoch - i), loss.item()))\n",
    "                auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "                logger.log(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "                acc = n_correct / n_processed\n",
    "                logger.log(\"\\tAccuracy: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "            if i % batches_per_test_epoch == 0 and i != 0:\n",
    "                break\n",
    "                \n",
    "        auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "        acc = n_correct / n_processed\n",
    "        epoch_time = time() - epoch_start\n",
    "        logger.log(\"Epoch {} took {:7.01f}s. Test Values:\".format(epoch, epoch_time))\n",
    "        \n",
    "        logger.log(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "        \n",
    "        logger.log(\"\\tAccuracy:  Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "    logger.close()\n",
    "    if epoch % clear_frequency == 0 and epoch != 0:\n",
    "        clear_output()\n",
    "    epoch_times.append(epoch_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
