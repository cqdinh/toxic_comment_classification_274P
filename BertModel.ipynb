{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_encoding_size = 256\n",
    "\n",
    "def position_embeddings(max_pos, size):\n",
    "    embeddings = np.zeros((max_pos, size))\n",
    "    w = 1 / (10000 ** (2*np.arange(size // 2 )/size))\n",
    "    for pos in range(max_pos):\n",
    "        embeddings[pos,0::2] = np.sin(w*pos)\n",
    "        embeddings[pos,1::2] = np.cos(w*pos)\n",
    "    return torch.Tensor(embeddings)\n",
    "    \n",
    "pos_embed = position_embeddings(10000, position_encoding_size)\n",
    "pos_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "bert_hidden_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, bert_size, position_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = nn.Linear(bert_size + position_size, 1)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(bert_size, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 6),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    '''\n",
    "    embeddings: shape (segment count, 512, bert_hidden_size)\n",
    "    position_encodings:  shape (segment count, 512, position_encoding_size)\n",
    "    comment_bounds: Array of tuples of the form [(start, end)]. comment_bounds[i] = (a, b) indicates that comment i's embeddings can be extracted as embeddings[a:b]\n",
    "    '''\n",
    "    def forward(self, embeddings, position_encodings, comment_bounds = None):\n",
    "        attention_input = torch.cat([embeddings, position_encodings], dim=2) # (batch, 512, position_size + bert_hidden_size)\n",
    "\n",
    "        # (batch, 512, 1)\n",
    "        attentions = self.attention(attention_input)\n",
    "        if comment_bounds is None:\n",
    "            attentions = self.softmax(attentions) # (batch, 512, 1)\n",
    "            vecs = torch.sum(attentions * embeddings, dim=1) # (batch, bert_hidden_size)\n",
    "            return self.prediction(vecs) # (batch, 1)\n",
    "\n",
    "        vecs = []\n",
    "        for (a,b) in comment_bounds:\n",
    "            comment_embeddings = embeddings[a:b] # (segment_count, 512, bert_hidden_size)\n",
    "            comment_attentions = attentions[a:b] # (segment_count, 512, 1)\n",
    "            attention_weights = self.softmax(comment_attentions) # (segment_count, 512, 1)\n",
    "            weighted_embeddings = attention_weights * embeddings[a:b] # (segment_count, 512, bert_hidden_size)\n",
    "            vec = torch.sum(weighted_embeddings.view(-1, weighted_embeddings.shape[-1]), dim=0, keepdim=True) # (segment_count, bert_hidden_size)\n",
    "            vecs.append(vec)\n",
    "        return self.prediction(torch.cat(vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_format, normalize):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load the data from files\n",
    "        input_ids = torch.load(file_format.format(\"input_ids\"))\n",
    "        positions = torch.load(file_format.format(\"positions\"))\n",
    "        comment_ids = torch.load(file_format.format(\"ids\"))\n",
    "        targets = torch.load(file_format.format(\"targets\"))\n",
    "\n",
    "        # Treat the targets as binary to separate the possible outputs\n",
    "        target_ids = torch.sum(torch.Tensor([32, 16, 8, 4, 2, 1]) * targets, axis=1)\n",
    "\n",
    "        # Store the data according to the target. Useful for normalization\n",
    "        self.data = [[] for i in range(64)]\n",
    "\n",
    "        # Load the data into the array\n",
    "        curr_id = 0\n",
    "        start_index = 0\n",
    "        for i in range(comment_ids.shape[0]):\n",
    "            if comment_ids[i] != curr_id:\n",
    "                target_id = int(target_ids[curr_id].item())\n",
    "                data = (input_ids[start_index:i], positions[start_index:i], targets[curr_id])\n",
    "                self.data[target_id].append(data)\n",
    "\n",
    "                curr_id = comment_ids[i]\n",
    "                start_index = i\n",
    "\n",
    "        target_id = int(target_ids[curr_id].item())\n",
    "        data = (input_ids[start_index:i], positions[start_index:i], targets[curr_id])\n",
    "        self.data[target_id].append(data)\n",
    "\n",
    "        n_nontoxic = len(self.data[0])\n",
    "\n",
    "        n_of_each = n_nontoxic // (len(self.data)-1)\n",
    "\n",
    "        # Remove the empty arrays from the data\n",
    "        self.data = [data for data in self.data if data]\n",
    "\n",
    "        n_copies = np.array([1]+[n_of_each // len(self.data[i]) for i in range(1,len(self.data))])\n",
    "        \n",
    "        if not normalize:\n",
    "            n_copies = np.ones_like(n_copies)\n",
    "        \n",
    "        self.data_length = np.array([len(data) for data in self.data])\n",
    "\n",
    "        segment_lengths = n_copies*self.data_length\n",
    "\n",
    "        self.length = int(np.sum(segment_lengths))\n",
    "\n",
    "        self.boundaries = np.zeros(segment_lengths.shape[0]+1)\n",
    "        self.boundaries[1:] = np.cumsum(segment_lengths)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        for i in range(self.boundaries.shape[0] - 1):\n",
    "            if index >= self.boundaries[i] and index < self.boundaries[i+1]:\n",
    "                inner_index = int((index - self.boundaries[i]) % self.data_length[i])\n",
    "                return self.data[i][inner_index]\n",
    "        print(\"Index: \", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226493, 153164)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = MyDataset(\"train_{}.pt\", True)\n",
    "test_dataset = MyDataset(\"test_{}.pt\", False)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterable(obj):\n",
    "    try:\n",
    "        iter(obj)\n",
    "    except Exception:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def collate_samples(batch):\n",
    "    for i, item in enumerate(batch):\n",
    "        if not iterable(item):\n",
    "            print(i, item)\n",
    "    split_comments, positions, targets = zip(*batch)\n",
    "    input_ids = []\n",
    "    comment_bounds = []\n",
    "    start = 0\n",
    "    for comment in split_comments:\n",
    "        input_ids += comment\n",
    "        comment_bounds.append((start, start+len(comment)))\n",
    "        start += len(comment)\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    encoded_positions = torch.cat([\n",
    "                          # Use the position array as indices into the position embedding\n",
    "                          pos_embed[position_arr]\n",
    "                          # For each comment in the batch\n",
    "                          for position_arr in positions                     \n",
    "                      ])\n",
    "  \n",
    "    targets = [target.view(1, -1) for target in targets]\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    comment_bounds = np.array(comment_bounds, dtype=np.int32)\n",
    "    return input_ids, encoded_positions, comment_bounds, targets\n",
    "\n",
    "batch_size = 72\n",
    "\n",
    "train = DataLoader(train_dataset, \n",
    "                   batch_size = batch_size,\n",
    "                   shuffle=True,\n",
    "                   collate_fn = collate_samples)\n",
    "\n",
    "test = DataLoader(test_dataset, \n",
    "                   batch_size = batch_size,\n",
    "                   shuffle=True,\n",
    "                   collate_fn = collate_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 512]) torch.Size([74, 512, 256]) (72, 2) torch.Size([72, 6])\n"
     ]
    }
   ],
   "source": [
    "for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(train):\n",
    "    print(input_ids.shape, encoded_position.shape, comment_bounds.shape, target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model, stdv):\n",
    "    for param in model.parameters():\n",
    "        if len(param.shape) >= 2:\n",
    "            nn.init.kaiming_normal_(param.data)\n",
    "        else:\n",
    "            param.data.normal_(0.0, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/model_{}.pt\"\n",
    "adam_path = \"./adam/adam_{}.pt\"\n",
    "train_loss_path = \"./train_loss.pt\"\n",
    "test_loss_path = \"./train_loss.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_auc(pred, target):\n",
    "    result = []\n",
    "    for i in range(pred.shape[1]):\n",
    "        if len(np.unique(target[:,i])) == 2:\n",
    "            result.append(roc_auc_score(target[:,i], pred[:,i], labels=[0,1]))\n",
    "        else:\n",
    "            extra = np.array([1-target[0,i]])\n",
    "            target_i = np.concatenate((target[:,i], extra))\n",
    "            pred_i = np.concatenate((pred[:,i], extra))\n",
    "            result.append(roc_auc_score(target_i, pred_i, labels=[0,1]))\n",
    "            \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(tag):\n",
    "    model.load_state_dict(torch.load(model_path.format(tag)))\n",
    "    optimizer.load_state_dict(torch.load(adam_path.format(tag)))\n",
    "    \n",
    "def save_model(tag):\n",
    "    torch.save(model.state_dict(), model_path.format(tag))\n",
    "    torch.save(optimizer.state_dict(), adam_path.format(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.eval()\n",
    "device = torch.device(\"cuda:0\")\n",
    "bert = bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_epoch = 0\n",
    "epochs = 30\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model = Classifier(bert_hidden_size, position_encoding_size)\n",
    "init_weights(model, 0.2)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load = None #\"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(5, 100, 0.3493)\n",
    "\n",
    "if load is not None:\n",
    "    load_model(load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger:\n",
    "    def __init__(self, path):\n",
    "        self.file = open(path, \"w\")\n",
    "    \n",
    "    def log(self, string):\n",
    "        print(string)\n",
    "        self.file.write(string)\n",
    "        self.file.write(\"\\n\")\n",
    "    \n",
    "    def close(self):\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 28: 00000/100 (  532.5s remaining)\t BCE Loss: 0.0467\n",
      "\tAUC: Toxic: 0.9750 Severe Toxic: 1.0000 Obscene: 0.9911 Threat: 1.0000 Insult: 0.9911 Identity-based Hate: 0.9158\n",
      "\tAccuracy: Toxic: 0.9861 Severe Toxic: 1.0000 Obscene: 0.9861 Threat: 1.0000 Insult: 0.9861 Identity-based Hate: 0.9444\n",
      "Epoch 28: 00001/100 (  527.4s remaining)\t BCE Loss: 0.3318\n",
      "\tAUC: Toxic: 0.9771 Severe Toxic: 0.9917 Obscene: 0.9958 Threat: 0.9917 Insult: 0.9654 Identity-based Hate: 0.9500\n",
      "\tAccuracy: Toxic: 0.9792 Severe Toxic: 0.9861 Obscene: 0.9931 Threat: 0.9861 Insult: 0.9653 Identity-based Hate: 0.9583\n",
      "Epoch 28: 00002/100 (  523.1s remaining)\t BCE Loss: 0.0723\n",
      "\tAUC: Toxic: 0.9767 Severe Toxic: 0.9806 Obscene: 0.9944 Threat: 0.9916 Insult: 0.9509 Identity-based Hate: 0.9350\n",
      "\tAccuracy: Toxic: 0.9815 Severe Toxic: 0.9861 Obscene: 0.9907 Threat: 0.9861 Insult: 0.9583 Identity-based Hate: 0.9583\n",
      "Epoch 28: 00003/100 (  518.2s remaining)\t BCE Loss: 0.0638\n",
      "\tAUC: Toxic: 0.9686 Severe Toxic: 0.9741 Obscene: 0.9850 Threat: 0.9939 Insult: 0.9416 Identity-based Hate: 0.9406\n",
      "\tAccuracy: Toxic: 0.9792 Severe Toxic: 0.9861 Obscene: 0.9896 Threat: 0.9896 Insult: 0.9514 Identity-based Hate: 0.9618\n",
      "Epoch 28: 00004/100 (  512.0s remaining)\t BCE Loss: 0.0832\n",
      "\tAUC: Toxic: 0.9708 Severe Toxic: 0.9775 Obscene: 0.9865 Threat: 0.9821 Insult: 0.9335 Identity-based Hate: 0.9361\n",
      "\tAccuracy: Toxic: 0.9778 Severe Toxic: 0.9861 Obscene: 0.9889 Threat: 0.9833 Insult: 0.9500 Identity-based Hate: 0.9611\n",
      "Epoch 28: 00005/100 (  507.2s remaining)\t BCE Loss: 0.0672\n",
      "\tAUC: Toxic: 0.9633 Severe Toxic: 0.9665 Obscene: 0.9890 Threat: 0.9711 Insult: 0.9446 Identity-based Hate: 0.9361\n",
      "\tAccuracy: Toxic: 0.9745 Severe Toxic: 0.9838 Obscene: 0.9907 Threat: 0.9815 Insult: 0.9583 Identity-based Hate: 0.9606\n",
      "Epoch 28: 00006/100 (  501.8s remaining)\t BCE Loss: 0.1063\n",
      "\tAUC: Toxic: 0.9567 Severe Toxic: 0.9632 Obscene: 0.9892 Threat: 0.9694 Insult: 0.9412 Identity-based Hate: 0.9303\n",
      "\tAccuracy: Toxic: 0.9702 Severe Toxic: 0.9841 Obscene: 0.9901 Threat: 0.9821 Insult: 0.9583 Identity-based Hate: 0.9563\n",
      "Epoch 28: 00007/100 (  496.1s remaining)\t BCE Loss: 0.0641\n",
      "\tAUC: Toxic: 0.9546 Severe Toxic: 0.9608 Obscene: 0.9854 Threat: 0.9726 Insult: 0.9337 Identity-based Hate: 0.9275\n",
      "\tAccuracy: Toxic: 0.9705 Severe Toxic: 0.9844 Obscene: 0.9896 Threat: 0.9844 Insult: 0.9566 Identity-based Hate: 0.9583\n",
      "Epoch 28: 00008/100 (  489.8s remaining)\t BCE Loss: 0.0462\n",
      "\tAUC: Toxic: 0.9593 Severe Toxic: 0.9538 Obscene: 0.9822 Threat: 0.9762 Insult: 0.9298 Identity-based Hate: 0.9332\n",
      "\tAccuracy: Toxic: 0.9738 Severe Toxic: 0.9830 Obscene: 0.9892 Threat: 0.9861 Insult: 0.9568 Identity-based Hate: 0.9630\n",
      "Epoch 28: 00009/100 (  483.5s remaining)\t BCE Loss: 0.0982\n",
      "\tAUC: Toxic: 0.9556 Severe Toxic: 0.9452 Obscene: 0.9796 Threat: 0.9747 Insult: 0.9264 Identity-based Hate: 0.9339\n",
      "\tAccuracy: Toxic: 0.9694 Severe Toxic: 0.9806 Obscene: 0.9875 Threat: 0.9861 Insult: 0.9569 Identity-based Hate: 0.9625\n",
      "Epoch 28: 00010/100 (  477.7s remaining)\t BCE Loss: 0.0649\n",
      "\tAUC: Toxic: 0.9569 Severe Toxic: 0.9475 Obscene: 0.9740 Threat: 0.9766 Insult: 0.9254 Identity-based Hate: 0.9357\n",
      "\tAccuracy: Toxic: 0.9710 Severe Toxic: 0.9811 Obscene: 0.9861 Threat: 0.9874 Insult: 0.9583 Identity-based Hate: 0.9634\n",
      "Epoch 28: 00011/100 (  473.2s remaining)\t BCE Loss: 0.1378\n",
      "\tAUC: Toxic: 0.9549 Severe Toxic: 0.9504 Obscene: 0.9697 Threat: 0.9673 Insult: 0.9283 Identity-based Hate: 0.9339\n",
      "\tAccuracy: Toxic: 0.9699 Severe Toxic: 0.9815 Obscene: 0.9850 Threat: 0.9838 Insult: 0.9606 Identity-based Hate: 0.9641\n",
      "Epoch 28: 00012/100 (  467.8s remaining)\t BCE Loss: 0.0685\n",
      "\tAUC: Toxic: 0.9571 Severe Toxic: 0.9526 Obscene: 0.9719 Threat: 0.9628 Insult: 0.9249 Identity-based Hate: 0.9382\n",
      "\tAccuracy: Toxic: 0.9712 Severe Toxic: 0.9829 Obscene: 0.9861 Threat: 0.9829 Insult: 0.9583 Identity-based Hate: 0.9669\n",
      "Epoch 28: 00013/100 (  462.7s remaining)\t BCE Loss: 0.1189\n",
      "\tAUC: Toxic: 0.9590 Severe Toxic: 0.9524 Obscene: 0.9679 Threat: 0.9610 Insult: 0.9191 Identity-based Hate: 0.9337\n",
      "\tAccuracy: Toxic: 0.9702 Severe Toxic: 0.9821 Obscene: 0.9851 Threat: 0.9831 Insult: 0.9554 Identity-based Hate: 0.9663\n",
      "Epoch 28: 00014/100 (  456.3s remaining)\t BCE Loss: 0.0753\n",
      "\tAUC: Toxic: 0.9562 Severe Toxic: 0.9521 Obscene: 0.9623 Threat: 0.9615 Insult: 0.9116 Identity-based Hate: 0.9372\n",
      "\tAccuracy: Toxic: 0.9694 Severe Toxic: 0.9833 Obscene: 0.9833 Threat: 0.9833 Insult: 0.9556 Identity-based Hate: 0.9676\n",
      "Epoch 28: 00015/100 (  450.4s remaining)\t BCE Loss: 0.0851\n",
      "\tAUC: Toxic: 0.9542 Severe Toxic: 0.9447 Obscene: 0.9565 Threat: 0.9613 Insult: 0.9121 Identity-based Hate: 0.9324\n",
      "\tAccuracy: Toxic: 0.9696 Severe Toxic: 0.9826 Obscene: 0.9826 Threat: 0.9818 Insult: 0.9566 Identity-based Hate: 0.9670\n",
      "Epoch 28: 00016/100 (  445.9s remaining)\t BCE Loss: 0.0875\n",
      "\tAUC: Toxic: 0.9525 Severe Toxic: 0.9469 Obscene: 0.9521 Threat: 0.9615 Insult: 0.9105 Identity-based Hate: 0.9351\n",
      "\tAccuracy: Toxic: 0.9690 Severe Toxic: 0.9837 Obscene: 0.9820 Threat: 0.9828 Insult: 0.9575 Identity-based Hate: 0.9681\n",
      "Epoch 28: 00017/100 (  440.1s remaining)\t BCE Loss: 0.0607\n",
      "\tAUC: Toxic: 0.9529 Severe Toxic: 0.9507 Obscene: 0.9524 Threat: 0.9634 Insult: 0.9149 Identity-based Hate: 0.9324\n",
      "\tAccuracy: Toxic: 0.9691 Severe Toxic: 0.9830 Obscene: 0.9823 Threat: 0.9838 Insult: 0.9583 Identity-based Hate: 0.9676\n",
      "Epoch 28: 00018/100 (  434.5s remaining)\t BCE Loss: 0.1126\n",
      "\tAUC: Toxic: 0.9505 Severe Toxic: 0.9480 Obscene: 0.9480 Threat: 0.9670 Insult: 0.9190 Identity-based Hate: 0.9365\n",
      "\tAccuracy: Toxic: 0.9678 Severe Toxic: 0.9825 Obscene: 0.9795 Threat: 0.9832 Insult: 0.9591 Identity-based Hate: 0.9686\n",
      "Epoch 28: 00019/100 (  428.6s remaining)\t BCE Loss: 0.0747\n",
      "\tAUC: Toxic: 0.9515 Severe Toxic: 0.9466 Obscene: 0.9393 Threat: 0.9721 Insult: 0.9190 Identity-based Hate: 0.9383\n",
      "\tAccuracy: Toxic: 0.9681 Severe Toxic: 0.9819 Obscene: 0.9778 Threat: 0.9840 Insult: 0.9611 Identity-based Hate: 0.9694\n",
      "Epoch 28: 00020/100 (  423.3s remaining)\t BCE Loss: 0.0736\n",
      "\tAUC: Toxic: 0.9585 Severe Toxic: 0.9469 Obscene: 0.9419 Threat: 0.9750 Insult: 0.9200 Identity-based Hate: 0.9490\n",
      "\tAccuracy: Toxic: 0.9696 Severe Toxic: 0.9815 Obscene: 0.9788 Threat: 0.9841 Insult: 0.9610 Identity-based Hate: 0.9709\n",
      "Epoch 28: 00021/100 (  417.9s remaining)\t BCE Loss: 0.0451\n",
      "\tAUC: Toxic: 0.9604 Severe Toxic: 0.9498 Obscene: 0.9429 Threat: 0.9747 Insult: 0.9275 Identity-based Hate: 0.9521\n",
      "\tAccuracy: Toxic: 0.9703 Severe Toxic: 0.9817 Obscene: 0.9792 Threat: 0.9848 Insult: 0.9621 Identity-based Hate: 0.9716\n",
      "Epoch 28: 00022/100 (  412.5s remaining)\t BCE Loss: 0.0711\n",
      "\tAUC: Toxic: 0.9603 Severe Toxic: 0.9525 Obscene: 0.9417 Threat: 0.9683 Insult: 0.9249 Identity-based Hate: 0.9502\n",
      "\tAccuracy: Toxic: 0.9716 Severe Toxic: 0.9807 Obscene: 0.9789 Threat: 0.9843 Insult: 0.9620 Identity-based Hate: 0.9704\n",
      "Epoch 28: 00023/100 (  407.7s remaining)\t BCE Loss: 0.0650\n",
      "\tAUC: Toxic: 0.9579 Severe Toxic: 0.9617 Obscene: 0.9387 Threat: 0.9676 Insult: 0.9284 Identity-based Hate: 0.9565\n",
      "\tAccuracy: Toxic: 0.9705 Severe Toxic: 0.9815 Obscene: 0.9786 Threat: 0.9844 Insult: 0.9612 Identity-based Hate: 0.9711\n",
      "Epoch 28: 00024/100 (  402.2s remaining)\t BCE Loss: 0.0566\n",
      "\tAUC: Toxic: 0.9620 Severe Toxic: 0.9572 Obscene: 0.9454 Threat: 0.9711 Insult: 0.9323 Identity-based Hate: 0.9570\n",
      "\tAccuracy: Toxic: 0.9717 Severe Toxic: 0.9811 Obscene: 0.9789 Threat: 0.9850 Insult: 0.9622 Identity-based Hate: 0.9711\n",
      "Epoch 28: 00025/100 (  396.3s remaining)\t BCE Loss: 0.0868\n",
      "\tAUC: Toxic: 0.9632 Severe Toxic: 0.9567 Obscene: 0.9381 Threat: 0.9743 Insult: 0.9291 Identity-based Hate: 0.9599\n",
      "\tAccuracy: Toxic: 0.9722 Severe Toxic: 0.9802 Obscene: 0.9770 Threat: 0.9850 Insult: 0.9615 Identity-based Hate: 0.9717\n",
      "Epoch 28: 00026/100 (  390.9s remaining)\t BCE Loss: 0.0717\n",
      "\tAUC: Toxic: 0.9632 Severe Toxic: 0.9539 Obscene: 0.9345 Threat: 0.9775 Insult: 0.9295 Identity-based Hate: 0.9591\n",
      "\tAccuracy: Toxic: 0.9727 Severe Toxic: 0.9789 Obscene: 0.9774 Threat: 0.9851 Insult: 0.9604 Identity-based Hate: 0.9722\n",
      "Epoch 28: 00027/100 (  386.0s remaining)\t BCE Loss: 0.0596\n",
      "\tAUC: Toxic: 0.9619 Severe Toxic: 0.9570 Obscene: 0.9369 Threat: 0.9776 Insult: 0.9344 Identity-based Hate: 0.9653\n",
      "\tAccuracy: Toxic: 0.9727 Severe Toxic: 0.9797 Obscene: 0.9772 Threat: 0.9851 Insult: 0.9608 Identity-based Hate: 0.9732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 00028/100 (  380.3s remaining)\t BCE Loss: 0.1311\n",
      "\tAUC: Toxic: 0.9564 Severe Toxic: 0.9484 Obscene: 0.9360 Threat: 0.9742 Insult: 0.9322 Identity-based Hate: 0.9710\n",
      "\tAccuracy: Toxic: 0.9708 Severe Toxic: 0.9784 Obscene: 0.9761 Threat: 0.9842 Insult: 0.9598 Identity-based Hate: 0.9737\n",
      "Epoch 28: 00029/100 (  375.2s remaining)\t BCE Loss: 0.0415\n",
      "\tAUC: Toxic: 0.9593 Severe Toxic: 0.9562 Obscene: 0.9410 Threat: 0.9732 Insult: 0.9343 Identity-based Hate: 0.9726\n",
      "\tAccuracy: Toxic: 0.9718 Severe Toxic: 0.9787 Obscene: 0.9764 Threat: 0.9843 Insult: 0.9602 Identity-based Hate: 0.9745\n",
      "Epoch 28: 00030/100 (  369.0s remaining)\t BCE Loss: 0.0958\n",
      "\tAUC: Toxic: 0.9604 Severe Toxic: 0.9510 Obscene: 0.9434 Threat: 0.9666 Insult: 0.9323 Identity-based Hate: 0.9714\n",
      "\tAccuracy: Toxic: 0.9722 Severe Toxic: 0.9776 Obscene: 0.9763 Threat: 0.9834 Insult: 0.9601 Identity-based Hate: 0.9745\n",
      "Epoch 28: 00031/100 (  363.4s remaining)\t BCE Loss: 0.0759\n",
      "\tAUC: Toxic: 0.9597 Severe Toxic: 0.9480 Obscene: 0.9458 Threat: 0.9660 Insult: 0.9328 Identity-based Hate: 0.9718\n",
      "\tAccuracy: Toxic: 0.9718 Severe Toxic: 0.9766 Obscene: 0.9770 Threat: 0.9835 Insult: 0.9605 Identity-based Hate: 0.9744\n",
      "Epoch 28: 00032/100 (  358.3s remaining)\t BCE Loss: 0.0568\n",
      "\tAUC: Toxic: 0.9613 Severe Toxic: 0.9533 Obscene: 0.9458 Threat: 0.9641 Insult: 0.9319 Identity-based Hate: 0.9744\n",
      "\tAccuracy: Toxic: 0.9722 Severe Toxic: 0.9769 Obscene: 0.9769 Threat: 0.9836 Insult: 0.9609 Identity-based Hate: 0.9747\n",
      "Epoch 28: 00033/100 (  352.5s remaining)\t BCE Loss: 0.0555\n",
      "\tAUC: Toxic: 0.9644 Severe Toxic: 0.9586 Obscene: 0.9487 Threat: 0.9623 Insult: 0.9274 Identity-based Hate: 0.9782\n",
      "\tAccuracy: Toxic: 0.9726 Severe Toxic: 0.9775 Obscene: 0.9763 Threat: 0.9833 Insult: 0.9612 Identity-based Hate: 0.9755\n",
      "Epoch 28: 00034/100 (  347.2s remaining)\t BCE Loss: 0.0676\n",
      "\tAUC: Toxic: 0.9612 Severe Toxic: 0.9651 Obscene: 0.9474 Threat: 0.9633 Insult: 0.9339 Identity-based Hate: 0.9728\n",
      "\tAccuracy: Toxic: 0.9726 Severe Toxic: 0.9774 Obscene: 0.9766 Threat: 0.9837 Insult: 0.9619 Identity-based Hate: 0.9754\n",
      "Epoch 28: 00035/100 (  342.0s remaining)\t BCE Loss: 0.0390\n",
      "\tAUC: Toxic: 0.9602 Severe Toxic: 0.9662 Obscene: 0.9501 Threat: 0.9638 Insult: 0.9394 Identity-based Hate: 0.9731\n",
      "\tAccuracy: Toxic: 0.9726 Severe Toxic: 0.9780 Obscene: 0.9772 Threat: 0.9838 Insult: 0.9630 Identity-based Hate: 0.9757\n",
      "Epoch 28: 00036/100 (  336.9s remaining)\t BCE Loss: 0.0953\n",
      "\tAUC: Toxic: 0.9612 Severe Toxic: 0.9677 Obscene: 0.9500 Threat: 0.9693 Insult: 0.9351 Identity-based Hate: 0.9754\n",
      "\tAccuracy: Toxic: 0.9722 Severe Toxic: 0.9782 Obscene: 0.9764 Threat: 0.9839 Insult: 0.9628 Identity-based Hate: 0.9760\n",
      "Epoch 28: 00037/100 (  332.3s remaining)\t BCE Loss: 0.0720\n",
      "\tAUC: Toxic: 0.9653 Severe Toxic: 0.9588 Obscene: 0.9489 Threat: 0.9716 Insult: 0.9366 Identity-based Hate: 0.9677\n",
      "\tAccuracy: Toxic: 0.9730 Severe Toxic: 0.9777 Obscene: 0.9762 Threat: 0.9839 Insult: 0.9627 Identity-based Hate: 0.9755\n",
      "Epoch 28: 00038/100 (  327.1s remaining)\t BCE Loss: 0.0634\n",
      "\tAUC: Toxic: 0.9630 Severe Toxic: 0.9611 Obscene: 0.9446 Threat: 0.9677 Insult: 0.9331 Identity-based Hate: 0.9669\n",
      "\tAccuracy: Toxic: 0.9733 Severe Toxic: 0.9779 Obscene: 0.9761 Threat: 0.9836 Insult: 0.9630 Identity-based Hate: 0.9754\n",
      "Epoch 28: 00039/100 (  321.5s remaining)\t BCE Loss: 0.0771\n",
      "\tAUC: Toxic: 0.9616 Severe Toxic: 0.9583 Obscene: 0.9495 Threat: 0.9708 Insult: 0.9286 Identity-based Hate: 0.9669\n",
      "\tAccuracy: Toxic: 0.9729 Severe Toxic: 0.9774 Obscene: 0.9760 Threat: 0.9840 Insult: 0.9622 Identity-based Hate: 0.9757\n",
      "Epoch 28: 00040/100 (  316.5s remaining)\t BCE Loss: 0.0507\n",
      "\tAUC: Toxic: 0.9633 Severe Toxic: 0.9614 Obscene: 0.9468 Threat: 0.9735 Insult: 0.9314 Identity-based Hate: 0.9703\n",
      "\tAccuracy: Toxic: 0.9732 Severe Toxic: 0.9780 Obscene: 0.9756 Threat: 0.9844 Insult: 0.9624 Identity-based Hate: 0.9759\n",
      "Epoch 28: 00041/100 (  311.7s remaining)\t BCE Loss: 0.0973\n",
      "\tAUC: Toxic: 0.9652 Severe Toxic: 0.9538 Obscene: 0.9415 Threat: 0.9761 Insult: 0.9330 Identity-based Hate: 0.9664\n",
      "\tAccuracy: Toxic: 0.9732 Severe Toxic: 0.9778 Obscene: 0.9752 Threat: 0.9845 Insult: 0.9623 Identity-based Hate: 0.9759\n",
      "Epoch 28: 00042/100 (  306.4s remaining)\t BCE Loss: 0.0676\n",
      "\tAUC: Toxic: 0.9701 Severe Toxic: 0.9457 Obscene: 0.9407 Threat: 0.9775 Insult: 0.9339 Identity-based Hate: 0.9649\n",
      "\tAccuracy: Toxic: 0.9735 Severe Toxic: 0.9767 Obscene: 0.9748 Threat: 0.9845 Insult: 0.9625 Identity-based Hate: 0.9758\n",
      "Epoch 28: 00043/100 (  301.1s remaining)\t BCE Loss: 0.0978\n",
      "\tAUC: Toxic: 0.9659 Severe Toxic: 0.9432 Obscene: 0.9415 Threat: 0.9744 Insult: 0.9297 Identity-based Hate: 0.9646\n",
      "\tAccuracy: Toxic: 0.9732 Severe Toxic: 0.9770 Obscene: 0.9751 Threat: 0.9836 Insult: 0.9621 Identity-based Hate: 0.9760\n",
      "Epoch 28: 00044/100 (  295.5s remaining)\t BCE Loss: 0.0409\n",
      "\tAUC: Toxic: 0.9676 Severe Toxic: 0.9477 Obscene: 0.9444 Threat: 0.9776 Insult: 0.9329 Identity-based Hate: 0.9666\n",
      "\tAccuracy: Toxic: 0.9738 Severe Toxic: 0.9772 Obscene: 0.9756 Threat: 0.9840 Insult: 0.9623 Identity-based Hate: 0.9759\n",
      "Epoch 28: 00045/100 (  291.0s remaining)\t BCE Loss: 0.0878\n",
      "\tAUC: Toxic: 0.9676 Severe Toxic: 0.9507 Obscene: 0.9443 Threat: 0.9772 Insult: 0.9297 Identity-based Hate: 0.9618\n",
      "\tAccuracy: Toxic: 0.9734 Severe Toxic: 0.9777 Obscene: 0.9758 Threat: 0.9840 Insult: 0.9620 Identity-based Hate: 0.9752\n",
      "Epoch 28: 00046/100 (  285.8s remaining)\t BCE Loss: 0.0747\n",
      "\tAUC: Toxic: 0.9654 Severe Toxic: 0.9513 Obscene: 0.9430 Threat: 0.9801 Insult: 0.9303 Identity-based Hate: 0.9584\n",
      "\tAccuracy: Toxic: 0.9734 Severe Toxic: 0.9781 Obscene: 0.9758 Threat: 0.9843 Insult: 0.9622 Identity-based Hate: 0.9749\n",
      "Epoch 28: 00047/100 (  280.4s remaining)\t BCE Loss: 0.0886\n",
      "\tAUC: Toxic: 0.9636 Severe Toxic: 0.9442 Obscene: 0.9449 Threat: 0.9807 Insult: 0.9302 Identity-based Hate: 0.9505\n",
      "\tAccuracy: Toxic: 0.9731 Severe Toxic: 0.9777 Obscene: 0.9757 Threat: 0.9847 Insult: 0.9615 Identity-based Hate: 0.9742\n",
      "Epoch 28: 00048/100 (  274.9s remaining)\t BCE Loss: 0.0911\n",
      "\tAUC: Toxic: 0.9637 Severe Toxic: 0.9379 Obscene: 0.9464 Threat: 0.9807 Insult: 0.9311 Identity-based Hate: 0.9553\n",
      "\tAccuracy: Toxic: 0.9728 Severe Toxic: 0.9773 Obscene: 0.9759 Threat: 0.9850 Insult: 0.9617 Identity-based Hate: 0.9745\n",
      "Epoch 28: 00049/100 (  268.9s remaining)\t BCE Loss: 0.0734\n",
      "\tAUC: Toxic: 0.9657 Severe Toxic: 0.9346 Obscene: 0.9394 Threat: 0.9814 Insult: 0.9294 Identity-based Hate: 0.9536\n",
      "\tAccuracy: Toxic: 0.9731 Severe Toxic: 0.9772 Obscene: 0.9750 Threat: 0.9853 Insult: 0.9617 Identity-based Hate: 0.9747\n",
      "Epoch 28: 00050/100 (  264.4s remaining)\t BCE Loss: 0.0493\n",
      "\tAUC: Toxic: 0.9660 Severe Toxic: 0.9390 Obscene: 0.9428 Threat: 0.9816 Insult: 0.9314 Identity-based Hate: 0.9561\n",
      "\tAccuracy: Toxic: 0.9730 Severe Toxic: 0.9777 Obscene: 0.9755 Threat: 0.9853 Insult: 0.9616 Identity-based Hate: 0.9749\n",
      "Epoch 28: 00051/100 (  257.4s remaining)\t BCE Loss: 0.0738\n",
      "\tAUC: Toxic: 0.9656 Severe Toxic: 0.9442 Obscene: 0.9486 Threat: 0.9856 Insult: 0.9366 Identity-based Hate: 0.9637\n",
      "\tAccuracy: Toxic: 0.9733 Severe Toxic: 0.9770 Obscene: 0.9757 Threat: 0.9856 Insult: 0.9615 Identity-based Hate: 0.9754\n",
      "Epoch 28: 00052/100 (  252.0s remaining)\t BCE Loss: 0.0495\n",
      "\tAUC: Toxic: 0.9639 Severe Toxic: 0.9425 Obscene: 0.9488 Threat: 0.9865 Insult: 0.9320 Identity-based Hate: 0.9667\n",
      "\tAccuracy: Toxic: 0.9733 Severe Toxic: 0.9767 Obscene: 0.9759 Threat: 0.9858 Insult: 0.9617 Identity-based Hate: 0.9759\n",
      "Epoch 28: 00053/100 (  246.6s remaining)\t BCE Loss: 0.0820\n",
      "\tAUC: Toxic: 0.9627 Severe Toxic: 0.9489 Obscene: 0.9518 Threat: 0.9836 Insult: 0.9365 Identity-based Hate: 0.9561\n",
      "\tAccuracy: Toxic: 0.9730 Severe Toxic: 0.9771 Obscene: 0.9761 Threat: 0.9856 Insult: 0.9617 Identity-based Hate: 0.9753\n",
      "Epoch 28: 00054/100 (  241.7s remaining)\t BCE Loss: 0.0786\n",
      "\tAUC: Toxic: 0.9629 Severe Toxic: 0.9463 Obscene: 0.9569 Threat: 0.9807 Insult: 0.9308 Identity-based Hate: 0.9535\n",
      "\tAccuracy: Toxic: 0.9732 Severe Toxic: 0.9773 Obscene: 0.9760 Threat: 0.9856 Insult: 0.9606 Identity-based Hate: 0.9753\n",
      "Epoch 28: 00055/100 (  236.0s remaining)\t BCE Loss: 0.0759\n",
      "\tAUC: Toxic: 0.9619 Severe Toxic: 0.9565 Obscene: 0.9623 Threat: 0.9817 Insult: 0.9348 Identity-based Hate: 0.9548\n",
      "\tAccuracy: Toxic: 0.9730 Severe Toxic: 0.9777 Obscene: 0.9762 Threat: 0.9859 Insult: 0.9606 Identity-based Hate: 0.9754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 00056/100 (  230.9s remaining)\t BCE Loss: 0.1069\n",
      "\tAUC: Toxic: 0.9621 Severe Toxic: 0.9671 Obscene: 0.9638 Threat: 0.9821 Insult: 0.9279 Identity-based Hate: 0.9540\n",
      "\tAccuracy: Toxic: 0.9730 Severe Toxic: 0.9776 Obscene: 0.9759 Threat: 0.9859 Insult: 0.9600 Identity-based Hate: 0.9754\n",
      "Epoch 28: 00057/100 (  225.7s remaining)\t BCE Loss: 0.0741\n",
      "\tAUC: Toxic: 0.9626 Severe Toxic: 0.9648 Obscene: 0.9635 Threat: 0.9892 Insult: 0.9286 Identity-based Hate: 0.9462\n",
      "\tAccuracy: Toxic: 0.9729 Severe Toxic: 0.9773 Obscene: 0.9758 Threat: 0.9859 Insult: 0.9600 Identity-based Hate: 0.9751\n",
      "Epoch 28: 00058/100 (  221.1s remaining)\t BCE Loss: 0.0932\n",
      "\tAUC: Toxic: 0.9614 Severe Toxic: 0.9632 Obscene: 0.9597 Threat: 0.9870 Insult: 0.9279 Identity-based Hate: 0.9433\n",
      "\tAccuracy: Toxic: 0.9729 Severe Toxic: 0.9772 Obscene: 0.9755 Threat: 0.9859 Insult: 0.9597 Identity-based Hate: 0.9746\n",
      "Epoch 28: 00059/100 (  216.1s remaining)\t BCE Loss: 0.0747\n",
      "\tAUC: Toxic: 0.9628 Severe Toxic: 0.9609 Obscene: 0.9611 Threat: 0.9855 Insult: 0.9306 Identity-based Hate: 0.9430\n",
      "\tAccuracy: Toxic: 0.9729 Severe Toxic: 0.9764 Obscene: 0.9759 Threat: 0.9859 Insult: 0.9600 Identity-based Hate: 0.9741\n",
      "Epoch 28: 00060/100 (  210.3s remaining)\t BCE Loss: 0.0565\n",
      "\tAUC: Toxic: 0.9651 Severe Toxic: 0.9591 Obscene: 0.9635 Threat: 0.9856 Insult: 0.9324 Identity-based Hate: 0.9444\n",
      "\tAccuracy: Toxic: 0.9731 Severe Toxic: 0.9765 Obscene: 0.9761 Threat: 0.9861 Insult: 0.9604 Identity-based Hate: 0.9740\n",
      "Epoch 28: 00061/100 (  204.9s remaining)\t BCE Loss: 0.0707\n",
      "\tAUC: Toxic: 0.9663 Severe Toxic: 0.9563 Obscene: 0.9626 Threat: 0.9846 Insult: 0.9348 Identity-based Hate: 0.9462\n",
      "\tAccuracy: Toxic: 0.9731 Severe Toxic: 0.9765 Obscene: 0.9763 Threat: 0.9861 Insult: 0.9608 Identity-based Hate: 0.9736\n",
      "Epoch 28: 00062/100 (  200.1s remaining)\t BCE Loss: 0.0756\n",
      "\tAUC: Toxic: 0.9677 Severe Toxic: 0.9558 Obscene: 0.9663 Threat: 0.9846 Insult: 0.9297 Identity-based Hate: 0.9425\n",
      "\tAccuracy: Toxic: 0.9733 Severe Toxic: 0.9764 Obscene: 0.9766 Threat: 0.9863 Insult: 0.9605 Identity-based Hate: 0.9735\n",
      "Epoch 28: 00063/100 (  195.1s remaining)\t BCE Loss: 0.0578\n",
      "\tAUC: Toxic: 0.9687 Severe Toxic: 0.9530 Obscene: 0.9682 Threat: 0.9814 Insult: 0.9283 Identity-based Hate: 0.9395\n",
      "\tAccuracy: Toxic: 0.9737 Severe Toxic: 0.9766 Obscene: 0.9768 Threat: 0.9863 Insult: 0.9607 Identity-based Hate: 0.9735\n",
      "Epoch 28: 00064/100 (  189.9s remaining)\t BCE Loss: 0.0794\n",
      "\tAUC: Toxic: 0.9678 Severe Toxic: 0.9522 Obscene: 0.9669 Threat: 0.9819 Insult: 0.9156 Identity-based Hate: 0.9375\n",
      "\tAccuracy: Toxic: 0.9737 Severe Toxic: 0.9767 Obscene: 0.9769 Threat: 0.9865 Insult: 0.9596 Identity-based Hate: 0.9735\n",
      "Epoch 28: 00065/100 (  185.0s remaining)\t BCE Loss: 0.0820\n",
      "\tAUC: Toxic: 0.9687 Severe Toxic: 0.9579 Obscene: 0.9636 Threat: 0.9815 Insult: 0.9133 Identity-based Hate: 0.9337\n",
      "\tAccuracy: Toxic: 0.9741 Severe Toxic: 0.9769 Obscene: 0.9769 Threat: 0.9867 Insult: 0.9596 Identity-based Hate: 0.9735\n",
      "Epoch 28: 00066/100 (  179.6s remaining)\t BCE Loss: 0.0880\n",
      "\tAUC: Toxic: 0.9688 Severe Toxic: 0.9577 Obscene: 0.9620 Threat: 0.9796 Insult: 0.9119 Identity-based Hate: 0.9340\n",
      "\tAccuracy: Toxic: 0.9741 Severe Toxic: 0.9768 Obscene: 0.9768 Threat: 0.9863 Insult: 0.9592 Identity-based Hate: 0.9735\n",
      "Epoch 28: 00067/100 (  173.9s remaining)\t BCE Loss: 0.0620\n",
      "\tAUC: Toxic: 0.9715 Severe Toxic: 0.9554 Obscene: 0.9615 Threat: 0.9798 Insult: 0.9125 Identity-based Hate: 0.9409\n",
      "\tAccuracy: Toxic: 0.9743 Severe Toxic: 0.9771 Obscene: 0.9769 Threat: 0.9863 Insult: 0.9592 Identity-based Hate: 0.9737\n",
      "Epoch 28: 00068/100 (  168.3s remaining)\t BCE Loss: 0.0739\n",
      "\tAUC: Toxic: 0.9708 Severe Toxic: 0.9572 Obscene: 0.9564 Threat: 0.9805 Insult: 0.9168 Identity-based Hate: 0.9457\n",
      "\tAccuracy: Toxic: 0.9744 Severe Toxic: 0.9773 Obscene: 0.9764 Threat: 0.9863 Insult: 0.9591 Identity-based Hate: 0.9740\n",
      "Epoch 28: 00069/100 (  162.1s remaining)\t BCE Loss: 0.0531\n",
      "\tAUC: Toxic: 0.9715 Severe Toxic: 0.9523 Obscene: 0.9568 Threat: 0.9831 Insult: 0.9156 Identity-based Hate: 0.9401\n",
      "\tAccuracy: Toxic: 0.9746 Severe Toxic: 0.9772 Obscene: 0.9768 Threat: 0.9865 Insult: 0.9595 Identity-based Hate: 0.9740\n",
      "Epoch 28: 00070/100 (  156.6s remaining)\t BCE Loss: 0.0595\n",
      "\tAUC: Toxic: 0.9716 Severe Toxic: 0.9543 Obscene: 0.9581 Threat: 0.9828 Insult: 0.9190 Identity-based Hate: 0.9444\n",
      "\tAccuracy: Toxic: 0.9744 Severe Toxic: 0.9775 Obscene: 0.9767 Threat: 0.9865 Insult: 0.9593 Identity-based Hate: 0.9742\n",
      "Epoch 28: 00071/100 (  151.8s remaining)\t BCE Loss: 0.0708\n",
      "\tAUC: Toxic: 0.9743 Severe Toxic: 0.9590 Obscene: 0.9497 Threat: 0.9797 Insult: 0.9219 Identity-based Hate: 0.9497\n",
      "\tAccuracy: Toxic: 0.9747 Severe Toxic: 0.9778 Obscene: 0.9763 Threat: 0.9863 Insult: 0.9593 Identity-based Hate: 0.9743\n",
      "Epoch 28: 00072/100 (  146.6s remaining)\t BCE Loss: 0.0579\n",
      "\tAUC: Toxic: 0.9758 Severe Toxic: 0.9588 Obscene: 0.9497 Threat: 0.9816 Insult: 0.9284 Identity-based Hate: 0.9552\n",
      "\tAccuracy: Toxic: 0.9749 Severe Toxic: 0.9779 Obscene: 0.9760 Threat: 0.9865 Insult: 0.9597 Identity-based Hate: 0.9745\n",
      "Epoch 28: 00073/100 (  141.4s remaining)\t BCE Loss: 0.0694\n",
      "\tAUC: Toxic: 0.9788 Severe Toxic: 0.9625 Obscene: 0.9430 Threat: 0.9839 Insult: 0.9266 Identity-based Hate: 0.9592\n",
      "\tAccuracy: Toxic: 0.9750 Severe Toxic: 0.9782 Obscene: 0.9756 Threat: 0.9867 Insult: 0.9596 Identity-based Hate: 0.9745\n",
      "Epoch 28: 00074/100 (  136.4s remaining)\t BCE Loss: 0.0709\n",
      "\tAUC: Toxic: 0.9770 Severe Toxic: 0.9633 Obscene: 0.9392 Threat: 0.9839 Insult: 0.9212 Identity-based Hate: 0.9564\n",
      "\tAccuracy: Toxic: 0.9750 Severe Toxic: 0.9785 Obscene: 0.9752 Threat: 0.9869 Insult: 0.9596 Identity-based Hate: 0.9744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_frequency = 1\n",
    "save_frequency = 10\n",
    "\n",
    "clear_frequency = 3\n",
    "\n",
    "# Maximum batches per epoch\n",
    "batches_per_train_epoch = 100\n",
    "batches_per_test_epoch = 10\n",
    "\n",
    "\n",
    "batches_per_train_epoch = min(batches_per_train_epoch, len(train))\n",
    "batches_per_test_epoch = min(batches_per_test_epoch, len(test))\n",
    "\n",
    "log_path = \"./logs/epoch_{}.txt\"\n",
    "\n",
    "epoch_times = []\n",
    "batch_times = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(first_epoch, epochs):\n",
    "    logger = EpochLogger(log_path.format(epoch))\n",
    "    epoch_start = time()\n",
    "    predicted = None\n",
    "    true = None\n",
    "    n_correct = 0\n",
    "    n_processed = 0\n",
    "    print(\"Training\")\n",
    "    for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(train):\n",
    "        batch_start = time()\n",
    "        # Get the BERT output\n",
    "        with torch.no_grad():\n",
    "            encoded_comments = bert(input_ids.to(device))[0]\n",
    "    \n",
    "        # Get the outputs from the network\n",
    "        output = model(encoded_comments, encoded_position.to(device), comment_bounds)\n",
    "\n",
    "        #print(output.shape, target.shape)\n",
    "        # Gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_times.append(time() - batch_start)\n",
    "        batch_times = batch_times[-100:]\n",
    "        pred = torch.round(output.cpu().detach())\n",
    "        if predicted is None:\n",
    "            predicted = pred.clone()\n",
    "            true = target.clone()\n",
    "        else:\n",
    "            predicted = torch.cat((predicted, pred), dim=0)\n",
    "            true = torch.cat((true, target), dim=0)\n",
    "\n",
    "        n_correct += torch.sum(pred == target, axis=0).numpy()\n",
    "        n_processed += pred.shape[0]\n",
    "\n",
    "        predicted = predicted[-1000:]\n",
    "        true = true[-1000:]\n",
    "    \n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "        if i % log_frequency == 0:\n",
    "            logger.log(\"Epoch {}: {:05d}/{} ({:7.01f}s remaining)\\t BCE Loss: {:.04f}\".format(epoch, i, batches_per_train_epoch, np.mean(batch_times)*(batches_per_train_epoch - i), loss.item()))\n",
    "            auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "            logger.log(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "            acc = n_correct / n_processed\n",
    "            logger.log(\"\\tAccuracy: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "        if i % save_frequency == 0:\n",
    "            save_model(\"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(epoch, i, loss.item()))\n",
    "            torch.save(torch.Tensor(train_losses), train_loss_path)\n",
    "        if i % batches_per_train_epoch == 0 and i != 0:\n",
    "            break\n",
    "            \n",
    "    save_model(\"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(epoch, i, loss.item()))\n",
    "    torch.save(torch.Tensor(train_losses), train_loss_path)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        print(\"Testing\")\n",
    "        predicted = None\n",
    "        for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(test):\n",
    "            batch_start = time()\n",
    "            # Get the BERT output\n",
    "            encoded_comments = bert(input_ids.to(device))[0]\n",
    "\n",
    "            # Get the outputs from the network\n",
    "            output = model(encoded_comments, encoded_position.to(device), comment_bounds)\n",
    "\n",
    "            #print(output[0], target[0])\n",
    "            # Gradient descent\n",
    "            pred = torch.round(output.cpu().detach())\n",
    "            if predicted is None:\n",
    "                predicted = pred.clone()\n",
    "                true = target.clone()\n",
    "            else:\n",
    "                predicted = torch.cat((predicted, pred), dim=0)\n",
    "                true = torch.cat((true, target), dim=0)\n",
    "\n",
    "            n_correct += torch.sum(pred == target, axis=0).numpy()\n",
    "            n_processed += pred.shape[0]\n",
    "\n",
    "            predicted = predicted[-1000:]\n",
    "            true = true[-1000:]\n",
    "\n",
    "            if i % log_frequency == 0:\n",
    "                logger.log(\"Epoch {}: {:05d}/{} ({:7.01f}s remaining)\\t BCE Loss: {:.04f}\".format(epoch, i, batches_per_test_epoch, np.mean(batch_times)*(batches_per_test_epoch - i), loss.item()))\n",
    "                auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "                logger.log(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "                acc = n_correct / n_processed\n",
    "                logger.log(\"\\tAccuracy: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "            if i % batches_per_test_epoch == 0 and i != 0:\n",
    "                break\n",
    "                \n",
    "        auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "        acc = n_correct / n_processed\n",
    "        epoch_time = time() - epoch_start\n",
    "        logger.log(\"Epoch {} took {:7.01f}s. Test Values:\".format(epoch, epoch_time))\n",
    "        \n",
    "        logger.log(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "        \n",
    "        logger.log(\"\\tAccuracy:  Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "    logger.close()\n",
    "    if epoch % clear_frequency == 0 and epoch != 0:\n",
    "        clear_output()\n",
    "    epoch_times.append(epoch_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
