{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_encoding_size = 256\n",
    "\n",
    "def position_embeddings(max_pos, size):\n",
    "    embeddings = np.zeros((max_pos, size))\n",
    "    w = 1 / (10000 ** (2*np.arange(size // 2 )/size))\n",
    "    for pos in range(max_pos):\n",
    "        embeddings[pos,0::2] = np.sin(w*pos)\n",
    "        embeddings[pos,1::2] = np.cos(w*pos)\n",
    "    return torch.Tensor(embeddings)\n",
    "    \n",
    "pos_embed = position_embeddings(10000, position_encoding_size)\n",
    "pos_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "bert_hidden_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, bert_size, position_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = nn.Linear(bert_size + position_size, 1)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(bert_size, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 6),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    '''\n",
    "    embeddings: shape (segment count, 512, bert_hidden_size)\n",
    "    position_encodings:  shape (segment count, 512, position_encoding_size)\n",
    "    comment_bounds: Array of tuples of the form [(start, end)]. comment_bounds[i] = (a, b) indicates that comment i's embeddings can be extracted as embeddings[a:b]\n",
    "    '''\n",
    "    def forward(self, embeddings, position_encodings, comment_bounds = None):\n",
    "        attention_input = torch.cat([embeddings, position_encodings], dim=2) # (batch, 512, position_size + bert_hidden_size)\n",
    "\n",
    "        # (batch, 512, 1)\n",
    "        attentions = self.attention(attention_input)\n",
    "        if comment_bounds is None:\n",
    "            attentions = self.softmax(attentions) # (batch, 512, 1)\n",
    "            vecs = torch.sum(attentions * embeddings, dim=1) # (batch, bert_hidden_size)\n",
    "            return self.prediction(vecs) # (batch, 1)\n",
    "\n",
    "        vecs = []\n",
    "        for (a,b) in comment_bounds:\n",
    "            comment_embeddings = embeddings[a:b] # (segment_count, 512, bert_hidden_size)\n",
    "            comment_attentions = attentions[a:b] # (segment_count, 512, 1)\n",
    "            attention_weights = self.softmax(comment_attentions) # (segment_count, 512, 1)\n",
    "            weighted_embeddings = attention_weights * embeddings[a:b] # (segment_count, 512, bert_hidden_size)\n",
    "            vec = torch.sum(weighted_embeddings.view(-1, weighted_embeddings.shape[-1]), dim=0, keepdim=True) # (segment_count, bert_hidden_size)\n",
    "            vecs.append(vec)\n",
    "        return self.prediction(torch.cat(vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_format, normalize):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load the data from files\n",
    "        input_ids = torch.load(file_format.format(\"input_ids\"))\n",
    "        positions = torch.load(file_format.format(\"positions\"))\n",
    "        comment_ids = torch.load(file_format.format(\"ids\"))\n",
    "        targets = torch.load(file_format.format(\"targets\"))\n",
    "\n",
    "        # Treat the targets as binary to separate the possible outputs\n",
    "        target_ids = torch.sum(torch.Tensor([32, 16, 8, 4, 2, 1]) * targets, axis=1)\n",
    "\n",
    "        # Store the data according to the target. Useful for normalization\n",
    "        self.data = [[] for i in range(64)]\n",
    "\n",
    "        # Load the data into the array\n",
    "        curr_id = 0\n",
    "        start_index = 0\n",
    "        for i in range(comment_ids.shape[0]):\n",
    "            if comment_ids[i] != curr_id:\n",
    "                target_id = int(target_ids[curr_id].item())\n",
    "                data = (input_ids[start_index:i], positions[start_index:i], targets[curr_id])\n",
    "                self.data[target_id].append(data)\n",
    "\n",
    "                curr_id = comment_ids[i]\n",
    "                start_index = i\n",
    "\n",
    "        target_id = int(target_ids[curr_id].item())\n",
    "        data = (input_ids[start_index:i], positions[start_index:i], targets[curr_id])\n",
    "        self.data[target_id].append(data)\n",
    "\n",
    "        n_nontoxic = len(self.data[0])\n",
    "\n",
    "        n_of_each = n_nontoxic // (len(self.data)-1)\n",
    "\n",
    "        # Remove the empty arrays from the data\n",
    "        self.data = [data for data in self.data if data]\n",
    "\n",
    "        n_copies = np.array([1]+[n_of_each // len(self.data[i]) for i in range(1,len(self.data))])\n",
    "        \n",
    "        if not normalize:\n",
    "            n_copies = np.ones_like(n_copies)\n",
    "        \n",
    "        self.data_length = np.array([len(data) for data in self.data])\n",
    "\n",
    "        segment_lengths = n_copies*self.data_length\n",
    "\n",
    "        self.length = int(np.sum(segment_lengths))\n",
    "\n",
    "        self.boundaries = np.zeros(segment_lengths.shape[0]+1)\n",
    "        self.boundaries[1:] = np.cumsum(segment_lengths)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        for i in range(self.boundaries.shape[0] - 1):\n",
    "            if index >= self.boundaries[i] and index < self.boundaries[i+1]:\n",
    "                inner_index = int((index - self.boundaries[i]) % self.data_length[i])\n",
    "                return self.data[i][inner_index]\n",
    "        print(\"Index: \", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226493, 153164)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = MyDataset(\"train_{}.pt\", True)\n",
    "test_dataset = MyDataset(\"test_{}.pt\", False)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterable(obj):\n",
    "    try:\n",
    "        iter(obj)\n",
    "    except Exception:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def collate_samples(batch):\n",
    "    for i, item in enumerate(batch):\n",
    "        if not iterable(item):\n",
    "            print(i, item)\n",
    "    split_comments, positions, targets = zip(*batch)\n",
    "    input_ids = []\n",
    "    comment_bounds = []\n",
    "    start = 0\n",
    "    for comment in split_comments:\n",
    "        input_ids += comment\n",
    "        comment_bounds.append((start, start+len(comment)))\n",
    "        start += len(comment)\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    encoded_positions = torch.cat([\n",
    "                          # Use the position array as indices into the position embedding\n",
    "                          pos_embed[position_arr]\n",
    "                          # For each comment in the batch\n",
    "                          for position_arr in positions                     \n",
    "                      ])\n",
    "  \n",
    "    targets = [target.view(1, -1) for target in targets]\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    comment_bounds = np.array(comment_bounds, dtype=np.int32)\n",
    "    return input_ids, encoded_positions, comment_bounds, targets\n",
    "\n",
    "batch_size = 72\n",
    "\n",
    "train = DataLoader(train_dataset, \n",
    "                   batch_size = batch_size,\n",
    "                   shuffle=True,\n",
    "                   collate_fn = collate_samples)\n",
    "\n",
    "test = DataLoader(test_dataset, \n",
    "                   batch_size = batch_size,\n",
    "                   shuffle=True,\n",
    "                   collate_fn = collate_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 512]) torch.Size([74, 512, 256]) (72, 2) torch.Size([72, 6])\n"
     ]
    }
   ],
   "source": [
    "for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(train):\n",
    "    print(input_ids.shape, encoded_position.shape, comment_bounds.shape, target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.eval()\n",
    "device = torch.device(\"cuda:0\")\n",
    "bert = bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 3e-5\n",
    "\n",
    "model = Classifier(bert_hidden_size, position_encoding_size)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model, stdv):\n",
    "    for param in model.parameters():\n",
    "        if len(param.shape) >= 2:\n",
    "            nn.init.kaiming_normal_(param.data)\n",
    "        else:\n",
    "            param.data.normal_(0.0, stdv)\n",
    "\n",
    "init_weights(model, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/model_{}.pt\"\n",
    "adam_path = \"./adam/adam_{}.pt\"\n",
    "train_loss_path = \"./train_loss.pt\"\n",
    "test_loss_path = \"./train_loss.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = None\n",
    "\n",
    "def load_model(tag):\n",
    "    model.load_state_dict(torch.load(model_path.format(tag)))\n",
    "    optimizer.load_state_dict(torch.load(adam_path.format(tag)))\n",
    "    \n",
    "def save_model(tag):\n",
    "    torch.save(model.state_dict(), model_path.format(tag))\n",
    "    torch.save(optimizer.state_dict(), adam_path.format(tag))\n",
    "\n",
    "if load is not None:\n",
    "    load_model(load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_auc(pred, target):\n",
    "    result = []\n",
    "    for i in range(pred.shape[1]):\n",
    "        if len(np.unique(target[:,i])) == 2:\n",
    "            result.append(roc_auc_score(target[:,i], pred[:,i], labels=[0,1]))\n",
    "        else:\n",
    "            extra = np.array([1-target[0,i]])\n",
    "            target_i = np.concatenate((target[:,i], extra))\n",
    "            pred_i = np.concatenate((pred[:,i], extra))\n",
    "            result.append(roc_auc_score(target_i, pred_i, labels=[0,1]))\n",
    "            \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 0: 00000/100 (  484.3s remaining)\t BCE Loss: 0.6299\n",
      "\tAUC: Toxic: 0.7037 Severe Toxic: 0.4750 Obscene: 0.5000 Threat: 0.4844 Insult: 0.4754 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.8056 Severe Toxic: 0.6806 Obscene: 0.8333 Threat: 0.8611 Insult: 0.2083 Identity-based Hate: 0.8889\n",
      "Epoch 0: 00001/100 (  464.6s remaining)\t BCE Loss: 0.6322\n",
      "\tAUC: Toxic: 0.6797 Severe Toxic: 0.4643 Obscene: 0.5000 Threat: 0.4877 Insult: 0.4829 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.8056 Severe Toxic: 0.7292 Obscene: 0.8333 Threat: 0.8264 Insult: 0.2153 Identity-based Hate: 0.8542\n",
      "Epoch 0: 00002/100 (  479.1s remaining)\t BCE Loss: 0.6446\n",
      "\tAUC: Toxic: 0.6750 Severe Toxic: 0.5132 Obscene: 0.5000 Threat: 0.4890 Insult: 0.4920 Identity-based Hate: 0.5303\n",
      "\tAccuracy: Toxic: 0.8056 Severe Toxic: 0.7593 Obscene: 0.8102 Threat: 0.8194 Insult: 0.2222 Identity-based Hate: 0.8565\n",
      "Epoch 0: 00003/100 (  480.9s remaining)\t BCE Loss: 0.6191\n",
      "\tAUC: Toxic: 0.6510 Severe Toxic: 0.5126 Obscene: 0.4979 Threat: 0.4982 Insult: 0.4877 Identity-based Hate: 0.5192\n",
      "\tAccuracy: Toxic: 0.8056 Severe Toxic: 0.7708 Obscene: 0.8229 Threat: 0.8194 Insult: 0.2292 Identity-based Hate: 0.8507\n",
      "Epoch 0: 00004/100 (  505.4s remaining)\t BCE Loss: 0.6867\n",
      "\tAUC: Toxic: 0.6103 Severe Toxic: 0.5147 Obscene: 0.4966 Threat: 0.5153 Insult: 0.4636 Identity-based Hate: 0.5226\n",
      "\tAccuracy: Toxic: 0.7833 Severe Toxic: 0.7833 Obscene: 0.8139 Threat: 0.8333 Insult: 0.2111 Identity-based Hate: 0.8417\n",
      "Epoch 0: 00005/100 (  495.6s remaining)\t BCE Loss: 0.6415\n",
      "\tAUC: Toxic: 0.5866 Severe Toxic: 0.5013 Obscene: 0.4971 Threat: 0.5114 Insult: 0.4652 Identity-based Hate: 0.5161\n",
      "\tAccuracy: Toxic: 0.7616 Severe Toxic: 0.7801 Obscene: 0.8009 Threat: 0.8264 Insult: 0.2407 Identity-based Hate: 0.8287\n",
      "Epoch 0: 00006/100 (  486.3s remaining)\t BCE Loss: 0.5701\n",
      "\tAUC: Toxic: 0.5825 Severe Toxic: 0.5057 Obscene: 0.5025 Threat: 0.5095 Insult: 0.4761 Identity-based Hate: 0.5130\n",
      "\tAccuracy: Toxic: 0.7639 Severe Toxic: 0.7897 Obscene: 0.7996 Threat: 0.8254 Insult: 0.3175 Identity-based Hate: 0.8214\n",
      "Epoch 0: 00007/100 (  473.9s remaining)\t BCE Loss: 0.5630\n",
      "\tAUC: Toxic: 0.5719 Severe Toxic: 0.5041 Obscene: 0.5023 Threat: 0.5082 Insult: 0.4634 Identity-based Hate: 0.5117\n",
      "\tAccuracy: Toxic: 0.7604 Severe Toxic: 0.7882 Obscene: 0.8021 Threat: 0.8247 Insult: 0.3681 Identity-based Hate: 0.8247\n",
      "Epoch 0: 00008/100 (  472.8s remaining)\t BCE Loss: 0.5563\n",
      "\tAUC: Toxic: 0.5642 Severe Toxic: 0.5007 Obscene: 0.5012 Threat: 0.5120 Insult: 0.4682 Identity-based Hate: 0.5106\n",
      "\tAccuracy: Toxic: 0.7593 Severe Toxic: 0.7917 Obscene: 0.8056 Threat: 0.8287 Insult: 0.4151 Identity-based Hate: 0.8272\n",
      "Epoch 0: 00009/100 (  465.7s remaining)\t BCE Loss: 0.5309\n",
      "\tAUC: Toxic: 0.5589 Severe Toxic: 0.5011 Obscene: 0.5010 Threat: 0.5107 Insult: 0.4704 Identity-based Hate: 0.5084\n",
      "\tAccuracy: Toxic: 0.7611 Severe Toxic: 0.8014 Obscene: 0.8028 Threat: 0.8278 Insult: 0.4528 Identity-based Hate: 0.8222\n",
      "Epoch 0: 00010/100 (  457.5s remaining)\t BCE Loss: 0.5133\n",
      "\tAUC: Toxic: 0.5526 Severe Toxic: 0.5014 Obscene: 0.5009 Threat: 0.5095 Insult: 0.4797 Identity-based Hate: 0.5072\n",
      "\tAccuracy: Toxic: 0.7563 Severe Toxic: 0.8093 Obscene: 0.8043 Threat: 0.8258 Insult: 0.4886 Identity-based Hate: 0.8258\n",
      "Epoch 0: 00011/100 (  449.9s remaining)\t BCE Loss: 0.4897\n",
      "\tAUC: Toxic: 0.5486 Severe Toxic: 0.5002 Obscene: 0.5009 Threat: 0.5079 Insult: 0.4883 Identity-based Hate: 0.5065\n",
      "\tAccuracy: Toxic: 0.7569 Severe Toxic: 0.8125 Obscene: 0.8056 Threat: 0.8241 Insult: 0.5197 Identity-based Hate: 0.8241\n",
      "Epoch 0: 00012/100 (  452.0s remaining)\t BCE Loss: 0.4598\n",
      "\tAUC: Toxic: 0.5460 Severe Toxic: 0.5002 Obscene: 0.5009 Threat: 0.5074 Insult: 0.4933 Identity-based Hate: 0.5062\n",
      "\tAccuracy: Toxic: 0.7607 Severe Toxic: 0.8184 Obscene: 0.8098 Threat: 0.8259 Insult: 0.5449 Identity-based Hate: 0.8269\n",
      "Epoch 0: 00013/100 (  461.9s remaining)\t BCE Loss: 0.6491\n",
      "\tAUC: Toxic: 0.5396 Severe Toxic: 0.4971 Obscene: 0.5008 Threat: 0.5071 Insult: 0.4895 Identity-based Hate: 0.5058\n",
      "\tAccuracy: Toxic: 0.7579 Severe Toxic: 0.8194 Obscene: 0.8105 Threat: 0.8294 Insult: 0.5645 Identity-based Hate: 0.8274\n",
      "Epoch 0: 00014/100 (  461.0s remaining)\t BCE Loss: 0.6047\n",
      "\tAUC: Toxic: 0.5297 Severe Toxic: 0.4995 Obscene: 0.5034 Threat: 0.5082 Insult: 0.4862 Identity-based Hate: 0.5054\n",
      "\tAccuracy: Toxic: 0.7556 Severe Toxic: 0.8185 Obscene: 0.8083 Threat: 0.8315 Insult: 0.5750 Identity-based Hate: 0.8259\n",
      "Epoch 0: 00015/100 (  462.3s remaining)\t BCE Loss: 0.5910\n",
      "\tAUC: Toxic: 0.5220 Severe Toxic: 0.5030 Obscene: 0.5027 Threat: 0.5085 Insult: 0.4920 Identity-based Hate: 0.5054\n",
      "\tAccuracy: Toxic: 0.7535 Severe Toxic: 0.8194 Obscene: 0.8082 Threat: 0.8316 Insult: 0.5911 Identity-based Hate: 0.8255\n",
      "Epoch 0: 00016/100 (  454.0s remaining)\t BCE Loss: 0.4166\n",
      "\tAUC: Toxic: 0.5129 Severe Toxic: 0.4963 Obscene: 0.5024 Threat: 0.5091 Insult: 0.4995 Identity-based Hate: 0.4998\n",
      "\tAccuracy: Toxic: 0.7582 Severe Toxic: 0.8243 Obscene: 0.8129 Threat: 0.8366 Insult: 0.6087 Identity-based Hate: 0.8260\n",
      "Epoch 0: 00017/100 (  451.5s remaining)\t BCE Loss: 0.5419\n",
      "\tAUC: Toxic: 0.5108 Severe Toxic: 0.4955 Obscene: 0.5030 Threat: 0.5072 Insult: 0.4999 Identity-based Hate: 0.5010\n",
      "\tAccuracy: Toxic: 0.7569 Severe Toxic: 0.8256 Obscene: 0.8140 Threat: 0.8380 Insult: 0.6173 Identity-based Hate: 0.8248\n",
      "Epoch 0: 00018/100 (  448.0s remaining)\t BCE Loss: 0.4875\n",
      "\tAUC: Toxic: 0.5156 Severe Toxic: 0.4943 Obscene: 0.5025 Threat: 0.5014 Insult: 0.5146 Identity-based Hate: 0.4982\n",
      "\tAccuracy: Toxic: 0.7595 Severe Toxic: 0.8282 Obscene: 0.8165 Threat: 0.8385 Insult: 0.6272 Identity-based Hate: 0.8275\n",
      "Epoch 0: 00019/100 (  444.6s remaining)\t BCE Loss: 0.6600\n",
      "\tAUC: Toxic: 0.5154 Severe Toxic: 0.4973 Obscene: 0.5026 Threat: 0.5012 Insult: 0.5039 Identity-based Hate: 0.4988\n",
      "\tAccuracy: Toxic: 0.7535 Severe Toxic: 0.8257 Obscene: 0.8146 Threat: 0.8306 Insult: 0.6326 Identity-based Hate: 0.8285\n",
      "Epoch 0: 00020/100 (  438.3s remaining)\t BCE Loss: 0.6031\n",
      "\tAUC: Toxic: 0.5153 Severe Toxic: 0.4960 Obscene: 0.4991 Threat: 0.5011 Insult: 0.5006 Identity-based Hate: 0.4988\n",
      "\tAccuracy: Toxic: 0.7526 Severe Toxic: 0.8267 Obscene: 0.8095 Threat: 0.8280 Insult: 0.6382 Identity-based Hate: 0.8294\n",
      "Epoch 0: 00021/100 (  438.9s remaining)\t BCE Loss: 0.7735\n",
      "\tAUC: Toxic: 0.5206 Severe Toxic: 0.4953 Obscene: 0.4978 Threat: 0.4999 Insult: 0.5019 Identity-based Hate: 0.4988\n",
      "\tAccuracy: Toxic: 0.7506 Severe Toxic: 0.8245 Obscene: 0.8068 Threat: 0.8251 Insult: 0.6433 Identity-based Hate: 0.8289\n",
      "Epoch 0: 00022/100 (  430.3s remaining)\t BCE Loss: 0.4759\n",
      "\tAUC: Toxic: 0.5226 Severe Toxic: 0.4965 Obscene: 0.4985 Threat: 0.4970 Insult: 0.5026 Identity-based Hate: 0.4988\n",
      "\tAccuracy: Toxic: 0.7518 Severe Toxic: 0.8279 Obscene: 0.8086 Threat: 0.8213 Insult: 0.6522 Identity-based Hate: 0.8309\n",
      "Epoch 0: 00023/100 (  423.7s remaining)\t BCE Loss: 0.6887\n",
      "\tAUC: Toxic: 0.5243 Severe Toxic: 0.4977 Obscene: 0.4983 Threat: 0.4969 Insult: 0.5025 Identity-based Hate: 0.4994\n",
      "\tAccuracy: Toxic: 0.7506 Severe Toxic: 0.8293 Obscene: 0.8032 Threat: 0.8194 Insult: 0.6557 Identity-based Hate: 0.8287\n",
      "Epoch 0: 00024/100 (  415.0s remaining)\t BCE Loss: 0.6623\n",
      "\tAUC: Toxic: 0.5239 Severe Toxic: 0.4982 Obscene: 0.4983 Threat: 0.4969 Insult: 0.5024 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7478 Severe Toxic: 0.8294 Obscene: 0.8033 Threat: 0.8194 Insult: 0.6594 Identity-based Hate: 0.8272\n",
      "Epoch 0: 00025/100 (  409.0s remaining)\t BCE Loss: 0.4737\n",
      "\tAUC: Toxic: 0.5252 Severe Toxic: 0.4988 Obscene: 0.4984 Threat: 0.4976 Insult: 0.5024 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7495 Severe Toxic: 0.8317 Obscene: 0.8066 Threat: 0.8216 Insult: 0.6688 Identity-based Hate: 0.8269\n",
      "Epoch 0: 00026/100 (  401.2s remaining)\t BCE Loss: 0.5468\n",
      "\tAUC: Toxic: 0.5247 Severe Toxic: 0.5000 Obscene: 0.4983 Threat: 0.4976 Insult: 0.5024 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7485 Severe Toxic: 0.8333 Obscene: 0.8066 Threat: 0.8210 Insult: 0.6734 Identity-based Hate: 0.8272\n",
      "Epoch 0: 00027/100 (  397.9s remaining)\t BCE Loss: 0.5463\n",
      "\tAUC: Toxic: 0.5297 Severe Toxic: 0.5065 Obscene: 0.5009 Threat: 0.4976 Insult: 0.5024 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7480 Severe Toxic: 0.8348 Obscene: 0.8065 Threat: 0.8224 Insult: 0.6796 Identity-based Hate: 0.8269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 00028/100 (  390.5s remaining)\t BCE Loss: 0.5511\n",
      "\tAUC: Toxic: 0.5296 Severe Toxic: 0.5069 Obscene: 0.4983 Threat: 0.4975 Insult: 0.5025 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7476 Severe Toxic: 0.8372 Obscene: 0.8070 Threat: 0.8209 Insult: 0.6844 Identity-based Hate: 0.8290\n",
      "Epoch 0: 00029/100 (  384.6s remaining)\t BCE Loss: 0.5082\n",
      "\tAUC: Toxic: 0.5295 Severe Toxic: 0.5071 Obscene: 0.4990 Threat: 0.4982 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7491 Severe Toxic: 0.8389 Obscene: 0.8083 Threat: 0.8222 Insult: 0.6894 Identity-based Hate: 0.8296\n",
      "Epoch 0: 00030/100 (  386.3s remaining)\t BCE Loss: 0.6144\n",
      "\tAUC: Toxic: 0.5364 Severe Toxic: 0.5168 Obscene: 0.5021 Threat: 0.4988 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7487 Severe Toxic: 0.8387 Obscene: 0.8091 Threat: 0.8226 Insult: 0.6931 Identity-based Hate: 0.8306\n",
      "Epoch 0: 00031/100 (  380.6s remaining)\t BCE Loss: 0.6499\n",
      "\tAUC: Toxic: 0.5423 Severe Toxic: 0.5230 Obscene: 0.5070 Threat: 0.4988 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7483 Severe Toxic: 0.8385 Obscene: 0.8090 Threat: 0.8203 Insult: 0.6940 Identity-based Hate: 0.8281\n",
      "Epoch 0: 00032/100 (  375.4s remaining)\t BCE Loss: 0.5656\n",
      "\tAUC: Toxic: 0.5416 Severe Toxic: 0.5290 Obscene: 0.5077 Threat: 0.4988 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7483 Severe Toxic: 0.8396 Obscene: 0.8102 Threat: 0.8211 Insult: 0.6995 Identity-based Hate: 0.8279\n",
      "Epoch 0: 00033/100 (  369.4s remaining)\t BCE Loss: 0.5050\n",
      "\tAUC: Toxic: 0.5417 Severe Toxic: 0.5340 Obscene: 0.5106 Threat: 0.4988 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7492 Severe Toxic: 0.8411 Obscene: 0.8117 Threat: 0.8215 Insult: 0.7034 Identity-based Hate: 0.8280\n",
      "Epoch 0: 00034/100 (  364.1s remaining)\t BCE Loss: 0.6972\n",
      "\tAUC: Toxic: 0.5401 Severe Toxic: 0.5338 Obscene: 0.5114 Threat: 0.4988 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7484 Severe Toxic: 0.8409 Obscene: 0.8107 Threat: 0.8214 Insult: 0.7056 Identity-based Hate: 0.8274\n",
      "Epoch 0: 00035/100 (  358.2s remaining)\t BCE Loss: 0.3674\n",
      "\tAUC: Toxic: 0.5407 Severe Toxic: 0.5407 Obscene: 0.5131 Threat: 0.5028 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7515 Severe Toxic: 0.8445 Obscene: 0.8133 Threat: 0.8237 Insult: 0.7099 Identity-based Hate: 0.8291\n",
      "Epoch 0: 00036/100 (  351.0s remaining)\t BCE Loss: 0.4255\n",
      "\tAUC: Toxic: 0.5393 Severe Toxic: 0.5417 Obscene: 0.5136 Threat: 0.5030 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7534 Severe Toxic: 0.8476 Obscene: 0.8153 Threat: 0.8251 Insult: 0.7140 Identity-based Hate: 0.8300\n",
      "Epoch 0: 00037/100 (  347.2s remaining)\t BCE Loss: 0.6746\n",
      "\tAUC: Toxic: 0.5373 Severe Toxic: 0.5457 Obscene: 0.5132 Threat: 0.5090 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7511 Severe Toxic: 0.8490 Obscene: 0.8147 Threat: 0.8249 Insult: 0.7160 Identity-based Hate: 0.8286\n",
      "Epoch 0: 00038/100 (  340.1s remaining)\t BCE Loss: 0.3970\n",
      "\tAUC: Toxic: 0.5391 Severe Toxic: 0.5476 Obscene: 0.5134 Threat: 0.5091 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7532 Severe Toxic: 0.8504 Obscene: 0.8162 Threat: 0.8266 Insult: 0.7201 Identity-based Hate: 0.8308\n",
      "Epoch 0: 00039/100 (  333.5s remaining)\t BCE Loss: 0.5607\n",
      "\tAUC: Toxic: 0.5381 Severe Toxic: 0.5509 Obscene: 0.5134 Threat: 0.5093 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7524 Severe Toxic: 0.8510 Obscene: 0.8170 Threat: 0.8281 Insult: 0.7233 Identity-based Hate: 0.8302\n",
      "Epoch 0: 00040/100 (  327.3s remaining)\t BCE Loss: 0.4452\n",
      "\tAUC: Toxic: 0.5451 Severe Toxic: 0.5639 Obscene: 0.5129 Threat: 0.5093 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7544 Severe Toxic: 0.8530 Obscene: 0.8171 Threat: 0.8286 Insult: 0.7270 Identity-based Hate: 0.8303\n",
      "Epoch 0: 00041/100 (  320.5s remaining)\t BCE Loss: 0.4951\n",
      "\tAUC: Toxic: 0.5475 Severe Toxic: 0.5561 Obscene: 0.5107 Threat: 0.5093 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7556 Severe Toxic: 0.8528 Obscene: 0.8194 Threat: 0.8287 Insult: 0.7305 Identity-based Hate: 0.8307\n",
      "Epoch 0: 00042/100 (  315.6s remaining)\t BCE Loss: 0.4714\n",
      "\tAUC: Toxic: 0.5550 Severe Toxic: 0.5668 Obscene: 0.5139 Threat: 0.5152 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7574 Severe Toxic: 0.8547 Obscene: 0.8201 Threat: 0.8295 Insult: 0.7326 Identity-based Hate: 0.8304\n",
      "Epoch 0: 00043/100 (  309.0s remaining)\t BCE Loss: 0.5457\n",
      "\tAUC: Toxic: 0.5572 Severe Toxic: 0.5674 Obscene: 0.5138 Threat: 0.5144 Insult: 0.5000 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.7576 Severe Toxic: 0.8554 Obscene: 0.8201 Threat: 0.8270 Insult: 0.7342 Identity-based Hate: 0.8311\n",
      "Epoch 0: 00044/100 (  303.2s remaining)\t BCE Loss: 0.4981\n",
      "\tAUC: Toxic: 0.5538 Severe Toxic: 0.5624 Obscene: 0.5101 Threat: 0.5141 Insult: 0.5030 Identity-based Hate: 0.4994\n",
      "\tAccuracy: Toxic: 0.7577 Severe Toxic: 0.8562 Obscene: 0.8201 Threat: 0.8275 Insult: 0.7358 Identity-based Hate: 0.8318\n",
      "Epoch 0: 00045/100 (  296.8s remaining)\t BCE Loss: 0.5351\n",
      "\tAUC: Toxic: 0.5517 Severe Toxic: 0.5603 Obscene: 0.5036 Threat: 0.5139 Insult: 0.5031 Identity-based Hate: 0.4994\n",
      "\tAccuracy: Toxic: 0.7572 Severe Toxic: 0.8566 Obscene: 0.8200 Threat: 0.8276 Insult: 0.7379 Identity-based Hate: 0.8312\n",
      "Epoch 0: 00046/100 (  292.6s remaining)\t BCE Loss: 0.5800\n",
      "\tAUC: Toxic: 0.5564 Severe Toxic: 0.5608 Obscene: 0.5066 Threat: 0.5169 Insult: 0.5083 Identity-based Hate: 0.5019\n",
      "\tAccuracy: Toxic: 0.7586 Severe Toxic: 0.8570 Obscene: 0.8194 Threat: 0.8283 Insult: 0.7391 Identity-based Hate: 0.8313\n",
      "Epoch 0: 00047/100 (  288.5s remaining)\t BCE Loss: 0.5745\n",
      "\tAUC: Toxic: 0.5548 Severe Toxic: 0.5636 Obscene: 0.5035 Threat: 0.5261 Insult: 0.5099 Identity-based Hate: 0.5020\n",
      "\tAccuracy: Toxic: 0.7584 Severe Toxic: 0.8571 Obscene: 0.8200 Threat: 0.8290 Insult: 0.7399 Identity-based Hate: 0.8319\n",
      "Epoch 0: 00048/100 (  283.1s remaining)\t BCE Loss: 0.5174\n",
      "\tAUC: Toxic: 0.5579 Severe Toxic: 0.5713 Obscene: 0.5036 Threat: 0.5301 Insult: 0.5092 Identity-based Hate: 0.5020\n",
      "\tAccuracy: Toxic: 0.7582 Severe Toxic: 0.8586 Obscene: 0.8203 Threat: 0.8299 Insult: 0.7401 Identity-based Hate: 0.8313\n",
      "Epoch 0: 00049/100 (  277.1s remaining)\t BCE Loss: 0.6072\n",
      "\tAUC: Toxic: 0.5608 Severe Toxic: 0.5644 Obscene: 0.5031 Threat: 0.5254 Insult: 0.5093 Identity-based Hate: 0.5019\n",
      "\tAccuracy: Toxic: 0.7578 Severe Toxic: 0.8578 Obscene: 0.8194 Threat: 0.8283 Insult: 0.7428 Identity-based Hate: 0.8311\n",
      "Epoch 0: 00050/100 (  271.9s remaining)\t BCE Loss: 0.5624\n",
      "\tAUC: Toxic: 0.5660 Severe Toxic: 0.5640 Obscene: 0.5058 Threat: 0.5266 Insult: 0.5110 Identity-based Hate: 0.5049\n",
      "\tAccuracy: Toxic: 0.7582 Severe Toxic: 0.8578 Obscene: 0.8189 Threat: 0.8276 Insult: 0.7437 Identity-based Hate: 0.8314\n",
      "Epoch 0: 00051/100 (  265.8s remaining)\t BCE Loss: 0.5274\n",
      "\tAUC: Toxic: 0.5714 Severe Toxic: 0.5631 Obscene: 0.5098 Threat: 0.5233 Insult: 0.5107 Identity-based Hate: 0.5049\n",
      "\tAccuracy: Toxic: 0.7583 Severe Toxic: 0.8582 Obscene: 0.8189 Threat: 0.8275 Insult: 0.7436 Identity-based Hate: 0.8309\n",
      "Epoch 0: 00052/100 (  260.1s remaining)\t BCE Loss: 0.5540\n",
      "\tAUC: Toxic: 0.5775 Severe Toxic: 0.5686 Obscene: 0.5118 Threat: 0.5247 Insult: 0.5122 Identity-based Hate: 0.5048\n",
      "\tAccuracy: Toxic: 0.7579 Severe Toxic: 0.8585 Obscene: 0.8171 Threat: 0.8257 Insult: 0.7445 Identity-based Hate: 0.8315\n",
      "Epoch 0: 00053/100 (  254.5s remaining)\t BCE Loss: 0.5368\n",
      "\tAUC: Toxic: 0.5884 Severe Toxic: 0.5718 Obscene: 0.5148 Threat: 0.5249 Insult: 0.5140 Identity-based Hate: 0.5078\n",
      "\tAccuracy: Toxic: 0.7582 Severe Toxic: 0.8591 Obscene: 0.8179 Threat: 0.8241 Insult: 0.7454 Identity-based Hate: 0.8308\n"
     ]
    }
   ],
   "source": [
    "log_frequency = 1\n",
    "save_frequency = 10\n",
    "\n",
    "# Maximum batches per epoch\n",
    "batches_per_train_epoch = 100\n",
    "batches_per_test_epoch = 10\n",
    "\n",
    "\n",
    "batches_per_train_epoch = min(batches_per_train_epoch, len(train))\n",
    "batches_per_test_epoch = min(batches_per_test_epoch, len(test))\n",
    "\n",
    "epoch_times = []\n",
    "batch_times = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time()\n",
    "    predicted = None\n",
    "    true = None\n",
    "    n_correct = 0\n",
    "    n_processed = 0\n",
    "    print(\"Training\")\n",
    "    for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(train):\n",
    "        batch_start = time()\n",
    "        # Get the BERT output\n",
    "        with torch.no_grad():\n",
    "            encoded_comments = bert(input_ids.to(device))[0]\n",
    "    \n",
    "        # Get the outputs from the network\n",
    "        output = model(encoded_comments, encoded_position.to(device), comment_bounds)\n",
    "\n",
    "        #print(output.shape, target.shape)\n",
    "        # Gradient descent\n",
    "        loss = criterion(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_times.append(time() - batch_start)\n",
    "        batch_times = batch_times[-100:]\n",
    "        pred = torch.round(output.cpu().detach())\n",
    "        if predicted is None:\n",
    "            predicted = pred.clone()\n",
    "            true = target.clone()\n",
    "        else:\n",
    "            predicted = torch.cat((predicted, pred), dim=0)\n",
    "            true = torch.cat((true, target), dim=0)\n",
    "\n",
    "        n_correct += torch.sum(pred == target, axis=0).numpy()\n",
    "        n_processed += pred.shape[0]\n",
    "\n",
    "        predicted = predicted[-1000:]\n",
    "        true = true[-1000:]\n",
    "    \n",
    "        test_losses.append(loss.item())\n",
    "    \n",
    "        if i % log_frequency == 0:\n",
    "            print(\"Epoch {}: {:05d}/{} ({:7.01f}s remaining)\\t BCE Loss: {:.04f}\".format(epoch, i, batches_per_train_epoch, np.mean(batch_times)*(batches_per_train_epoch - i), loss.item()))\n",
    "            auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "            print(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "            acc = n_correct / n_processed\n",
    "            print(\"\\tAccuracy: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "        if i % save_frequency == 0:\n",
    "            save_model(\"epoch{}_batch{}_bce{:.04f}\".format(epoch, i, loss.item()))\n",
    "            torch.save(torch.Tensor(test_losses), train_loss_path)\n",
    "        if i % batches_per_train_epoch == 0 and i != 0:\n",
    "            break\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        print(\"Testing\")\n",
    "        predicted = None\n",
    "        for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(test):\n",
    "            batch_start = time()\n",
    "            # Get the BERT output\n",
    "            encoded_comments = bert(input_ids.to(device))[0]\n",
    "\n",
    "            # Get the outputs from the network\n",
    "            output = model(encoded_comments, encoded_position.to(device), comment_bounds)\n",
    "\n",
    "            #print(output[0], target[0])\n",
    "            # Gradient descent\n",
    "            pred = torch.round(output.cpu().detach())\n",
    "            if predicted is None:\n",
    "                predicted = pred.clone()\n",
    "                true = target.clone()\n",
    "            else:\n",
    "                predicted = torch.cat((predicted, pred), dim=0)\n",
    "                true = torch.cat((true, target), dim=0)\n",
    "\n",
    "            n_correct += torch.sum(pred == target, axis=0).numpy()\n",
    "            n_processed += pred.shape[0]\n",
    "\n",
    "            predicted = predicted[-1000:]\n",
    "            true = true[-1000:]\n",
    "\n",
    "            if i % log_frequency == 0:\n",
    "                print(\"Epoch {}: {:05d}/{} ({:7.01f}s remaining)\\t BCE Loss: {:.04f}\".format(epoch, i, batches_per_test_epoch, np.mean(batch_times)*(batches_per_test_epoch - i), loss.item()))\n",
    "                auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "                print(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "                acc = n_correct / n_processed\n",
    "                print(\"\\tAccuracy: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "            if i % batches_per_test_epoch == 0:\n",
    "                break\n",
    "        print(\"Epoch {} Test Values:\".format(epoch))\n",
    "        auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "        print(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "        acc = n_correct / n_processed\n",
    "        print(\"\\tAccuracy:  Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "            \n",
    "    epoch_times.append(time() - epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"./\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
