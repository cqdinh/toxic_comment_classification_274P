{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the position embedding matrix such that `pos_embed[i]` is the embedding for position `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_encoding_size = 256\n",
    "\n",
    "def position_embeddings(max_pos, size):\n",
    "    embeddings = np.zeros((max_pos, size))\n",
    "    w = 1 / (10000 ** (2*np.arange(size // 2 )/size))\n",
    "    for pos in range(max_pos):\n",
    "        embeddings[pos,0::2] = np.sin(w*pos)\n",
    "        embeddings[pos,1::2] = np.cos(w*pos)\n",
    "    return torch.Tensor(embeddings)\n",
    "    \n",
    "pos_embed = position_embeddings(10000, position_encoding_size)\n",
    "pos_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used on top of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, bert_size, position_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculates the attention value\n",
    "        self.attention = nn.Linear(bert_size + position_size, 1)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "        # Makes the prediction\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(bert_size, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 6),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    '''\n",
    "    embeddings: shape (segment count, 512, bert_hidden_size)\n",
    "        The output of BERT\n",
    "    position_encodings:  shape (segment count, 512, position_encoding_size)\n",
    "        The position encodings of the tokens\n",
    "    comment_bounds: Array of tuples of the form [(start, end)]. \n",
    "        comment_bounds[i] = (a, b) indicates that comment i's embeddings can be extracted as embeddings[a:b]\n",
    "        if not provided, all embeddings and positions are assumed to be a single comment\n",
    "    '''\n",
    "    def forward(self, embeddings, position_encodings, comment_bounds = None):\n",
    "        # Concatenate each BERT output with its position encoding\n",
    "        attention_input = torch.cat([embeddings, position_encodings], dim=2) # (batch, 512, position_size + bert_hidden_size)\n",
    "\n",
    "        # Get the attention weights for each concatenated vector\n",
    "        attentions = self.attention(attention_input) #  (batch size, 512, 1)\n",
    "        \n",
    "        # If no bounds are probided, assume the input is all one comment\n",
    "        if comment_bounds is None:\n",
    "            # Softmax over attentions\n",
    "            attentions = self.softmax(attentions) # (batch, 512, 1)\n",
    "            \n",
    "            # Calculate the total embedding as a weighted sum of embeddings without the positional encodings\n",
    "            vecs = torch.sum(attentions * embeddings, dim=1) # (batch, bert_hidden_size)\n",
    "            return self.prediction(vecs) # (batch, 1)\n",
    "\n",
    "        # Otherwise, get the outputs for each comment\n",
    "        vecs = []\n",
    "        for (a,b) in comment_bounds:\n",
    "            # Get the embeddings and attentions for the current comment\n",
    "            comment_embeddings = embeddings[a:b] # (segment_count, 512, bert_hidden_size)\n",
    "            comment_attentions = attentions[a:b] # (segment_count, 512, 1)\n",
    "            \n",
    "            # softmax over the attentions for the comment\n",
    "            attention_weights = self.softmax(comment_attentions) # (segment_count, 512, 1)\n",
    "            \n",
    "            # Calculate the total embedding as a weighted sum over the embeddings of the comment\n",
    "            weighted_embeddings = attention_weights * embeddings[a:b] # (segment_count, 512, bert_hidden_size)\n",
    "            vec = torch.sum(weighted_embeddings.view(-1, weighted_embeddings.shape[-1]), dim=0, keepdim=True) # (segment_count, bert_hidden_size)\n",
    "            \n",
    "            vecs.append(vec)\n",
    "            \n",
    "        # Stack the total embedding vectors and give them to the prediction network to calculate the output\n",
    "        return self.prediction(torch.cat(vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset that does the data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    # file_format is a Python format string with a single variable to be inserted. It's used to get the paths of the files\n",
    "    # normalize indicates whether to perform data normalization procedure\n",
    "    def __init__(self, file_format, normalize):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load the data from files\n",
    "        input_ids = torch.load(file_format.format(\"input_ids\"))\n",
    "        positions = torch.load(file_format.format(\"positions\"))\n",
    "        comment_ids = torch.load(file_format.format(\"ids\"))\n",
    "        targets = torch.load(file_format.format(\"targets\"))\n",
    "\n",
    "        # Treat the targets as binary to separate the possible outputs\n",
    "        target_ids = torch.sum(torch.Tensor([32, 16, 8, 4, 2, 1]) * targets, axis=1)\n",
    "\n",
    "        # Store the data according to the target. Useful for normalization\n",
    "        self.data = [[] for i in range(64)]\n",
    "\n",
    "        # Load the data into the array\n",
    "        start_index = 0\n",
    "        end_index = 0\n",
    "        n_comments = comment_ids.shape[0]\n",
    "        # Group the items by which comment they're part of\n",
    "        while start_index < n_comments:\n",
    "            # Get the current comment id\n",
    "            curr_id = comment_ids[start_index]\n",
    "            \n",
    "            # Find end_index such that input_ids[end_index-1] is the last segment of the comment\n",
    "            while end_index < n_comments and comment_ids[end_index] == comment_ids[start_index]:\n",
    "                end_index += 1\n",
    "            \n",
    "            # Get the number with a binary representation that's the same as the comment's true labels\n",
    "            target_id = int(target_ids[curr_id].item())\n",
    "            \n",
    "            # Get the comment as a tuple containing (token ids, positions, true labels)\n",
    "            data = (input_ids[start_index:end_index], positions[start_index:end_index], targets[curr_id])\n",
    "            self.data[target_id].append(data)\n",
    "\n",
    "            start_index = end_index\n",
    "\n",
    "        # Remove the empty arrays from the data\n",
    "        self.data = [data for data in self.data if data]\n",
    "        \n",
    "        # Calculate how many comments are nontoxic\n",
    "        n_nontoxic = len(self.data[0])\n",
    "        \n",
    "        # Calculate how many comments there need to be with each combination of labels\n",
    "        # The goal is for there to be as many toxic comments as nontoxic comments\n",
    "        # Also, each combination of labels should be present an equal number of times\n",
    "        n_of_each = n_nontoxic // (len(self.data)-1)\n",
    "        \n",
    "        # Calculate how many copies need to be made from each combination of labels\n",
    "        n_copies = np.array([1]+[n_of_each // len(self.data[i]) for i in range(1,len(self.data))])\n",
    "        \n",
    "        # If normalization shouldn't be done, just replace n_copies with a bunch of ones so there is one copy of each comment\n",
    "        if not normalize:\n",
    "            n_copies = np.ones_like(n_copies)\n",
    "        \n",
    "        # The number of comments with each combination of labels\n",
    "        self.data_length = np.array([len(data) for data in self.data])\n",
    "\n",
    "        # The data is organized into segments each of which contains some number of copies of the comments with a specific combination of labels\n",
    "        segment_lengths = n_copies*self.data_length\n",
    "\n",
    "        # The total length of the normalized dataset\n",
    "        self.length = int(np.sum(segment_lengths))\n",
    "\n",
    "        # The indices bounding the segments\n",
    "        self.boundaries = np.zeros(segment_lengths.shape[0]+1)\n",
    "        self.boundaries[1:] = np.cumsum(segment_lengths)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Find the segment that the index is in\n",
    "        for i in range(self.boundaries.shape[0] - 1):\n",
    "            if index >= self.boundaries[i] and index < self.boundaries[i+1]:\n",
    "                # index - self.boundaries[i] calculates the index into the segment\n",
    "                # The segment is a bunch of copies of the same data, but it's inefficient to actually copy the data\n",
    "                # Therefore, \"% self.data.length[i]\" is used to convert the index into the segment into the index into the data\n",
    "                inner_index = int((index - self.boundaries[i]) % self.data_length[i])\n",
    "                return self.data[i][inner_index]\n",
    "        print(\"Index: \", index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. \n",
    "\n",
    "The test data isn't normalized to give a realistic view of the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275845, 153164)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = MyDataset(\"train_{}.pt\", True)\n",
    "test_dataset = MyDataset(\"test_{}.pt\", False)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the data into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to format the data in a way that the model can use\n",
    "# batch is a list of tuples (tokenized_comment, positions, true_labels)\n",
    "def collate_samples(batch):\n",
    "    # Split the tuples into three variables\n",
    "    split_comments, positions, targets = zip(*batch)\n",
    "    \n",
    "    # An array of tuples (a,b) so that input_ids[a:b] are all part of a single comment\n",
    "    comment_bounds = np.zeros(len(split_comments)+1, dtype=np.int32)\n",
    "    comment_bounds[1:] = np.cumsum(list(map(len, split_comments)))\n",
    "    comment_bounds = np.array([comment_bounds[:-1], comment_bounds[1:]], dtype=np.int32).transpose()\n",
    "    \n",
    "    # For parallelism, stack the inputs into single tensors\n",
    "    input_ids = torch.cat(split_comments, dim=0)\n",
    "    \n",
    "    # Stack the position embeddings as well so they can be easily concatenated with the BERT output\n",
    "    encoded_positions = torch.cat([\n",
    "                          # Use the position array as indices into the position embedding\n",
    "                          pos_embed[position_arr]\n",
    "                          # For each comment in the batch\n",
    "                          for position_arr in positions                     \n",
    "                      ])\n",
    "  \n",
    "    # Stack the true labels\n",
    "    targets = torch.stack(targets)\n",
    "    return input_ids, encoded_positions, comment_bounds, targets\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train = DataLoader(train_dataset, \n",
    "                   batch_size = batch_size,\n",
    "                   shuffle=True,\n",
    "                   collate_fn = collate_samples)\n",
    "\n",
    "test = DataLoader(test_dataset, \n",
    "                   batch_size = batch_size,\n",
    "                   shuffle=True,\n",
    "                   collate_fn = collate_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512]) torch.Size([4, 512, 256]) (4, 2) torch.Size([4, 6])\n"
     ]
    }
   ],
   "source": [
    "for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(train):\n",
    "    print(input_ids.shape, encoded_position.shape, comment_bounds.shape, target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Kaiming normal weight initialization when possibe and when not, just use normal initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model, stdv):\n",
    "    for param in model.parameters():\n",
    "        if len(param.shape) >= 2:\n",
    "            nn.init.kaiming_normal_(param.data)\n",
    "        else:\n",
    "            param.data.normal_(0.0, stdv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate AUC given predicted and true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_auc(pred, target):\n",
    "    result = []\n",
    "    for i in range(pred.shape[1]):\n",
    "        # If the true labels only has one value in a column, add a fake item to make AUC a valid operation\n",
    "        if len(np.unique(target[:,i])) == 2:\n",
    "            result.append(roc_auc_score(target[:,i], pred[:,i], labels=[0,1]))\n",
    "        else:\n",
    "            extra = np.array([1-target[0,i]])\n",
    "            target_i = np.concatenate((target[:,i], extra))\n",
    "            pred_i = np.concatenate((pred[:,i], extra))\n",
    "            result.append(roc_auc_score(target_i, pred_i, labels=[0,1]))\n",
    "            \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to save and load the model. Because ADAM has its own internal state, it's saved as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(tag):\n",
    "    model.load_state_dict(torch.load(model_path.format(tag)))\n",
    "    optimizer.load_state_dict(torch.load(adam_path.format(tag)))\n",
    "    \n",
    "def save_model(tag):\n",
    "    torch.save(model.state_dict(), model_path.format(tag))\n",
    "    torch.save(optimizer.state_dict(), adam_path.format(tag))\n",
    "    \n",
    "def load_bert(tag):\n",
    "    load_model(tag)\n",
    "    bert.load_state_dict(torch.load(bert_path.format(tag)))\n",
    "    \n",
    "def save_bert(tag):\n",
    "    save_model(tag)\n",
    "    torch.save(bert.state_dict(), bert_path.format(tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format strings used to generate the model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/model_{}.pt\"\n",
    "bert_path = \"./bert/bert_{}.pt\"\n",
    "adam_path = \"./adam/adam_{}.pt\"\n",
    "train_loss_path = \"./train_loss_unfrozen.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "bert_learning_rate = 1e-5\n",
    "\n",
    "bert_hidden_size = 768\n",
    "\n",
    "# Put BERT on the GPU\n",
    "bert = bert.to(device)\n",
    "\n",
    "bert_optimizer = optim.Adam(bert.parameters(), lr=bert_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The epoch to start at\n",
    "first_epoch = 30\n",
    "\n",
    "# The last epoch number\n",
    "epochs = 60\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model = Classifier(bert_hidden_size, position_encoding_size)\n",
    "init_weights(model, 0.2)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to load saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = \"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(29, 100, 0.0629)\n",
    "\n",
    "if load is not None:\n",
    "    load_model(load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training prints too many lines, so this class is used to write them to a file after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger:\n",
    "    def __init__(self, path):\n",
    "        self.file = open(path, \"w\")\n",
    "    \n",
    "    def log(self, string):\n",
    "        print(string)\n",
    "        self.file.write(string)\n",
    "        self.file.write(\"\\n\")\n",
    "    \n",
    "    def close(self):\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 30: 00000/1000 (  965.1s remaining)\t BCE Loss: 0.1778\n",
      "\tAUC: Toxic: 0.5000 Severe Toxic: 1.0000 Obscene: 1.0000 Threat: 1.0000 Insult: 1.0000 Identity-based Hate: 1.0000\n",
      "\tAccuracy: Toxic: 0.7500 Severe Toxic: 1.0000 Obscene: 1.0000 Threat: 1.0000 Insult: 1.0000 Identity-based Hate: 1.0000\n",
      "Epoch 30: 00001/1000 (  846.0s remaining)\t BCE Loss: 0.0001\n",
      "\tAUC: Toxic: 0.5000 Severe Toxic: 1.0000 Obscene: 1.0000 Threat: 1.0000 Insult: 1.0000 Identity-based Hate: 1.0000\n",
      "\tAccuracy: Toxic: 0.8750 Severe Toxic: 1.0000 Obscene: 1.0000 Threat: 1.0000 Insult: 1.0000 Identity-based Hate: 1.0000\n",
      "Epoch 30: 00002/1000 (  805.7s remaining)\t BCE Loss: 0.8617\n",
      "\tAUC: Toxic: 0.5000 Severe Toxic: 0.5000 Obscene: 0.8333 Threat: 1.0000 Insult: 0.7500 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.8333 Severe Toxic: 0.9167 Obscene: 0.9167 Threat: 1.0000 Insult: 0.9167 Identity-based Hate: 0.9167\n",
      "Epoch 30: 00003/1000 (  783.5s remaining)\t BCE Loss: 0.1222\n",
      "\tAUC: Toxic: 0.6282 Severe Toxic: 0.7500 Obscene: 0.8750 Threat: 1.0000 Insult: 0.8333 Identity-based Hate: 0.5000\n",
      "\tAccuracy: Toxic: 0.8125 Severe Toxic: 0.9375 Obscene: 0.9375 Threat: 1.0000 Insult: 0.9375 Identity-based Hate: 0.9375\n",
      "Epoch 30: 00004/1000 (  768.6s remaining)\t BCE Loss: 0.2500\n",
      "\tAUC: Toxic: 0.7333 Severe Toxic: 0.7222 Obscene: 0.8000 Threat: 1.0000 Insult: 0.9000 Identity-based Hate: 0.8333\n",
      "\tAccuracy: Toxic: 0.8000 Severe Toxic: 0.9000 Obscene: 0.9000 Threat: 1.0000 Insult: 0.9500 Identity-based Hate: 0.9500\n",
      "Epoch 30: 00005/1000 (  758.4s remaining)\t BCE Loss: 0.8868\n",
      "\tAUC: Toxic: 0.7395 Severe Toxic: 0.7619 Obscene: 0.8000 Threat: 1.0000 Insult: 0.8889 Identity-based Hate: 0.8500\n",
      "\tAccuracy: Toxic: 0.7500 Severe Toxic: 0.8333 Obscene: 0.9167 Threat: 1.0000 Insult: 0.9167 Identity-based Hate: 0.9167\n",
      "Epoch 30: 00006/1000 (  751.1s remaining)\t BCE Loss: 0.1047\n",
      "\tAUC: Toxic: 0.7619 Severe Toxic: 0.7733 Obscene: 0.8000 Threat: 1.0000 Insult: 0.9048 Identity-based Hate: 0.8542\n",
      "\tAccuracy: Toxic: 0.7857 Severe Toxic: 0.8571 Obscene: 0.9286 Threat: 1.0000 Insult: 0.9286 Identity-based Hate: 0.9286\n",
      "Epoch 30: 00007/1000 (  745.0s remaining)\t BCE Loss: 0.8912\n",
      "\tAUC: Toxic: 0.7802 Severe Toxic: 0.7816 Obscene: 0.7259 Threat: 1.0000 Insult: 0.8116 Identity-based Hate: 0.8036\n",
      "\tAccuracy: Toxic: 0.7812 Severe Toxic: 0.8750 Obscene: 0.8125 Threat: 1.0000 Insult: 0.8750 Identity-based Hate: 0.8438\n",
      "Epoch 30: 00008/1000 (  740.7s remaining)\t BCE Loss: 0.3049\n",
      "\tAUC: Toxic: 0.7891 Severe Toxic: 0.7516 Obscene: 0.7500 Threat: 1.0000 Insult: 0.7808 Identity-based Hate: 0.8355\n",
      "\tAccuracy: Toxic: 0.7778 Severe Toxic: 0.8611 Obscene: 0.8056 Threat: 1.0000 Insult: 0.8611 Identity-based Hate: 0.8611\n",
      "Epoch 30: 00009/1000 (  736.5s remaining)\t BCE Loss: 0.4462\n",
      "\tAUC: Toxic: 0.8120 Severe Toxic: 0.6688 Obscene: 0.6948 Threat: 1.0000 Insult: 0.7555 Identity-based Hate: 0.7745\n",
      "\tAccuracy: Toxic: 0.8000 Severe Toxic: 0.8250 Obscene: 0.7750 Threat: 1.0000 Insult: 0.8500 Identity-based Hate: 0.8500\n",
      "Epoch 30: 00010/1000 (  733.4s remaining)\t BCE Loss: 0.2589\n",
      "\tAUC: Toxic: 0.8263 Severe Toxic: 0.6737 Obscene: 0.7046 Threat: 1.0000 Insult: 0.7344 Identity-based Hate: 0.7317\n",
      "\tAccuracy: Toxic: 0.8182 Severe Toxic: 0.8409 Obscene: 0.7955 Threat: 1.0000 Insult: 0.8409 Identity-based Hate: 0.8409\n",
      "Epoch 30: 00011/1000 (  730.2s remaining)\t BCE Loss: 0.8208\n",
      "\tAUC: Toxic: 0.8046 Severe Toxic: 0.6500 Obscene: 0.6750 Threat: 1.0000 Insult: 0.7165 Identity-based Hate: 0.7369\n",
      "\tAccuracy: Toxic: 0.8125 Severe Toxic: 0.8333 Obscene: 0.7917 Threat: 1.0000 Insult: 0.8333 Identity-based Hate: 0.8542\n",
      "Epoch 30: 00012/1000 (  728.0s remaining)\t BCE Loss: 0.8597\n",
      "\tAUC: Toxic: 0.7856 Severe Toxic: 0.6534 Obscene: 0.6525 Threat: 0.9500 Insult: 0.7011 Identity-based Hate: 0.7045\n",
      "\tAccuracy: Toxic: 0.8077 Severe Toxic: 0.8462 Obscene: 0.7885 Threat: 0.9808 Insult: 0.8269 Identity-based Hate: 0.8462\n",
      "Epoch 30: 00013/1000 (  725.6s remaining)\t BCE Loss: 1.1750\n",
      "\tAUC: Toxic: 0.7266 Severe Toxic: 0.6174 Obscene: 0.6848 Threat: 0.9545 Insult: 0.6937 Identity-based Hate: 0.6565\n",
      "\tAccuracy: Toxic: 0.7679 Severe Toxic: 0.8214 Obscene: 0.8036 Threat: 0.9821 Insult: 0.8036 Identity-based Hate: 0.8214\n",
      "Epoch 30: 00014/1000 (  723.7s remaining)\t BCE Loss: 0.2047\n",
      "\tAUC: Toxic: 0.7341 Severe Toxic: 0.6200 Obscene: 0.6900 Threat: 0.9167 Insult: 0.6847 Identity-based Hate: 0.6410\n",
      "\tAccuracy: Toxic: 0.7833 Severe Toxic: 0.8333 Obscene: 0.8167 Threat: 0.9667 Insult: 0.8000 Identity-based Hate: 0.8167\n",
      "Epoch 30: 00015/1000 (  721.7s remaining)\t BCE Loss: 0.1611\n",
      "\tAUC: Toxic: 0.7491 Severe Toxic: 0.6535 Obscene: 0.6923 Threat: 0.9231 Insult: 0.6927 Identity-based Hate: 0.6441\n",
      "\tAccuracy: Toxic: 0.7969 Severe Toxic: 0.8438 Obscene: 0.8125 Threat: 0.9688 Insult: 0.7969 Identity-based Hate: 0.8281\n",
      "Epoch 30: 00016/1000 (  719.7s remaining)\t BCE Loss: 0.3961\n",
      "\tAUC: Toxic: 0.7546 Severe Toxic: 0.6555 Obscene: 0.6762 Threat: 0.9231 Insult: 0.6863 Identity-based Hate: 0.6467\n",
      "\tAccuracy: Toxic: 0.8088 Severe Toxic: 0.8529 Obscene: 0.8088 Threat: 0.9706 Insult: 0.7941 Identity-based Hate: 0.8382\n",
      "Epoch 30: 00017/1000 (  718.1s remaining)\t BCE Loss: 0.9218\n",
      "\tAUC: Toxic: 0.7269 Severe Toxic: 0.6572 Obscene: 0.6474 Threat: 0.9333 Insult: 0.7160 Identity-based Hate: 0.6199\n",
      "\tAccuracy: Toxic: 0.7917 Severe Toxic: 0.8611 Obscene: 0.7917 Threat: 0.9722 Insult: 0.8056 Identity-based Hate: 0.8194\n",
      "Epoch 30: 00018/1000 (  800.4s remaining)\t BCE Loss: 8.6343\n",
      "\tAUC: Toxic: 0.7079 Severe Toxic: 0.6510 Obscene: 0.6687 Threat: 0.9242 Insult: 0.7399 Identity-based Hate: 0.6221\n",
      "\tAccuracy: Toxic: 0.7763 Severe Toxic: 0.8553 Obscene: 0.8026 Threat: 0.9474 Insult: 0.8026 Identity-based Hate: 0.8289\n",
      "Epoch 30: 00019/1000 (  794.8s remaining)\t BCE Loss: 0.8283\n",
      "\tAUC: Toxic: 0.7202 Severe Toxic: 0.7009 Obscene: 0.6583 Threat: 0.9094 Insult: 0.7342 Identity-based Hate: 0.6126\n",
      "\tAccuracy: Toxic: 0.7750 Severe Toxic: 0.8625 Obscene: 0.8000 Threat: 0.9250 Insult: 0.7875 Identity-based Hate: 0.8250\n",
      "Epoch 30: 00020/1000 (  789.8s remaining)\t BCE Loss: 0.6687\n",
      "\tAUC: Toxic: 0.7122 Severe Toxic: 0.6857 Obscene: 0.6611 Threat: 0.9039 Insult: 0.7142 Identity-based Hate: 0.6268\n",
      "\tAccuracy: Toxic: 0.7738 Severe Toxic: 0.8571 Obscene: 0.8095 Threat: 0.9167 Insult: 0.7619 Identity-based Hate: 0.8214\n",
      "Epoch 30: 00021/1000 (  784.9s remaining)\t BCE Loss: 0.3416\n",
      "\tAUC: Toxic: 0.7086 Severe Toxic: 0.6737 Obscene: 0.6670 Threat: 0.9087 Insult: 0.7120 Identity-based Hate: 0.6483\n",
      "\tAccuracy: Toxic: 0.7727 Severe Toxic: 0.8409 Obscene: 0.8068 Threat: 0.9205 Insult: 0.7614 Identity-based Hate: 0.8295\n",
      "Epoch 30: 00022/1000 (  780.4s remaining)\t BCE Loss: 0.6376\n",
      "\tAUC: Toxic: 0.7271 Severe Toxic: 0.7105 Obscene: 0.6833 Threat: 0.9063 Insult: 0.7029 Identity-based Hate: 0.6607\n",
      "\tAccuracy: Toxic: 0.7826 Severe Toxic: 0.8478 Obscene: 0.8152 Threat: 0.9130 Insult: 0.7500 Identity-based Hate: 0.8261\n",
      "Epoch 30: 00023/1000 (  776.3s remaining)\t BCE Loss: 0.4048\n",
      "\tAUC: Toxic: 0.7357 Severe Toxic: 0.7204 Obscene: 0.6981 Threat: 0.9105 Insult: 0.7144 Identity-based Hate: 0.6855\n",
      "\tAccuracy: Toxic: 0.7812 Severe Toxic: 0.8438 Obscene: 0.8229 Threat: 0.9167 Insult: 0.7500 Identity-based Hate: 0.8229\n",
      "Epoch 30: 00024/1000 (  772.5s remaining)\t BCE Loss: 0.7877\n",
      "\tAUC: Toxic: 0.7501 Severe Toxic: 0.7290 Obscene: 0.7115 Threat: 0.8706 Insult: 0.7188 Identity-based Hate: 0.6812\n",
      "\tAccuracy: Toxic: 0.7900 Severe Toxic: 0.8400 Obscene: 0.8300 Threat: 0.9000 Insult: 0.7500 Identity-based Hate: 0.8200\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.17 GiB total capacity; 10.60 GiB already allocated; 27.44 MiB free; 10.86 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bb388dd99620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mbatch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Get the BERT output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mencoded_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Get the outputs from the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         )\n\u001b[1;32m    736\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 408\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             )\n\u001b[1;32m    410\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 153\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \"\"\"\n\u001b[1;32m   1955\u001b[0m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[0;32m-> 1956\u001b[0;31m                             torch.backends.cudnn.enabled)\n\u001b[0m\u001b[1;32m   1957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.17 GiB total capacity; 10.60 GiB already allocated; 27.44 MiB free; 10.86 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# How often to print\n",
    "log_frequency = 1\n",
    "\n",
    "# How many batches to run between saving the model\n",
    "save_frequency = 10\n",
    "bert_save_frequency = 500\n",
    "\n",
    "# How many epochs to run before clearing the output\n",
    "clear_frequency_batches = 300\n",
    "clear_frequency_epochs = 1\n",
    "\n",
    "# Maximum batches per epoch\n",
    "batches_per_train_epoch = 1000\n",
    "batches_per_test_epoch = 10\n",
    "\n",
    "batches_per_train_epoch = min(batches_per_train_epoch, len(train))\n",
    "batches_per_test_epoch = min(batches_per_test_epoch, len(test))\n",
    "\n",
    "# The format string used to generate paths for the log files\n",
    "log_path = \"./logs/epoch_{}.txt\"\n",
    "\n",
    "# Store the times to help forecast how long training will take\n",
    "epoch_times = []\n",
    "batch_times = []\n",
    "\n",
    "# Store the training loss after each batch\n",
    "train_losses = []\n",
    "for epoch in range(first_epoch, epochs):\n",
    "    # initialize the logger object\n",
    "    logger = EpochLogger(log_path.format(epoch))\n",
    "    \n",
    "    # Record the time at the start of the epoch\n",
    "    epoch_start = time()\n",
    "    \n",
    "    # Predictions and true labels used to calculate the AUC\n",
    "    predicted = None\n",
    "    true = None\n",
    "    \n",
    "    # Recording how many were correct and the total number of predictions to calculate the accuracy\n",
    "    n_correct = 0\n",
    "    n_processed = 0\n",
    "    print(\"Training\")\n",
    "    for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(train):\n",
    "        batch_start = time()\n",
    "        # Get the BERT output\n",
    "        encoded_comments = bert(input_ids.to(device))[0]\n",
    "    \n",
    "        # Get the outputs from the network\n",
    "        output = model(encoded_comments, encoded_position.to(device), comment_bounds)\n",
    "\n",
    "        # Gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        bert_optimizer.step()\n",
    "        \n",
    "        # Store how long this batch took to run\n",
    "        batch_times.append(time() - batch_start)\n",
    "        \n",
    "        # Only use the last 100 batches to estimate the time remaining\n",
    "        batch_times = batch_times[-100:]\n",
    "        \n",
    "        # Calculate the predicted labels by rounding to zero or 1\n",
    "        pred = torch.round(output.cpu().detach())\n",
    "        \n",
    "        # Add the predicted and true labels to the arrays\n",
    "        if predicted is None:\n",
    "            predicted = pred.clone()\n",
    "            true = target.clone()\n",
    "        else:\n",
    "            predicted = torch.cat((predicted, pred), dim=0)\n",
    "            true = torch.cat((true, target), dim=0)\n",
    "\n",
    "        # Calculate the number that were correct\n",
    "        n_correct += torch.sum(pred == target, axis=0).numpy()\n",
    "        n_processed += pred.shape[0]\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "        # Print debugging information\n",
    "        if i % log_frequency == 0:\n",
    "            logger.log(\"Epoch {}: {:05d}/{} ({:7.01f}s remaining)\\t BCE Loss: {:.04f}\".format(epoch, i, batches_per_train_epoch, np.mean(batch_times)*(batches_per_train_epoch - i), loss.item()))\n",
    "            auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "            logger.log(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "            acc = n_correct / n_processed\n",
    "            logger.log(\"\\tAccuracy: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "        # Save the model\n",
    "        if i % save_frequency == 0:\n",
    "            save_model(\"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(epoch, i, loss.item()))\n",
    "            torch.save(torch.Tensor(train_losses), train_loss_path)\n",
    "        if i % bert_save_frequency == 0:\n",
    "            save_bert(\"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(epoch, i, loss.item()))\n",
    "            torch.save(torch.Tensor(train_losses), train_loss_path)\n",
    "        if i % clear_frequency_batches == 0 and i != 0:\n",
    "            clear_output()\n",
    "        # Break early \n",
    "        if i % batches_per_train_epoch == 0 and i != 0:\n",
    "            break\n",
    "    \n",
    "    # Make sure that the model is saved at the end of the epoch\n",
    "    save_model(\"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(epoch, i, loss.item()))\n",
    "    \n",
    "    # Save the training losses\n",
    "    torch.save(torch.Tensor(train_losses), train_loss_path)\n",
    "    \n",
    "    # Test the model\n",
    "    with torch.no_grad():\n",
    "        print(\"Testing\")\n",
    "        predicted = None\n",
    "        for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(test):\n",
    "            batch_start = time()\n",
    "            # Get the BERT output\n",
    "            encoded_comments = bert(input_ids.to(device))[0]\n",
    "\n",
    "            # Get the outputs from the network\n",
    "            output = model(encoded_comments, encoded_position.to(device), comment_bounds)\n",
    "\n",
    "            #print(output[0], target[0])\n",
    "            # Gradient descent\n",
    "            pred = torch.round(output.cpu().detach())\n",
    "            if predicted is None:\n",
    "                predicted = pred.clone()\n",
    "                true = target.clone()\n",
    "            else:\n",
    "                predicted = torch.cat((predicted, pred), dim=0)\n",
    "                true = torch.cat((true, target), dim=0)\n",
    "\n",
    "            n_correct += torch.sum(pred == target, axis=0).numpy()\n",
    "            n_processed += pred.shape[0]\n",
    "\n",
    "            predicted = predicted[-1000:]\n",
    "            true = true[-1000:]\n",
    "\n",
    "            if i % log_frequency == 0:\n",
    "                logger.log(\"Epoch {}: {:05d}/{} ({:7.01f}s remaining)\\t BCE Loss: {:.04f}\".format(epoch, i, batches_per_test_epoch, np.mean(batch_times)*(batches_per_test_epoch - i), loss.item()))\n",
    "                auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "                logger.log(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "                acc = n_correct / n_processed\n",
    "                logger.log(\"\\tAccuracy: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "            if i % batches_per_test_epoch == 0 and i != 0:\n",
    "                break\n",
    "                \n",
    "        auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "        acc = n_correct / n_processed\n",
    "        epoch_time = time() - epoch_start\n",
    "        logger.log(\"Epoch {} took {:7.01f}s. Test Values:\".format(epoch, epoch_time))\n",
    "        \n",
    "        logger.log(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "        \n",
    "        logger.log(\"\\tAccuracy:  Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "    logger.close()\n",
    "    if epoch % clear_frequency_epochs == 0 and epoch != 0:\n",
    "        clear_output()\n",
    "    epoch_times.append(epoch_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
