{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the position embedding matrix such that `pos_embed[i]` is the embedding for position `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_encoding_size = 256\n",
    "\n",
    "def position_embeddings(max_pos, size):\n",
    "    embeddings = np.zeros((max_pos, size))\n",
    "    w = 1 / (10000 ** (2*np.arange(size // 2 )/size))\n",
    "    for pos in range(max_pos):\n",
    "        embeddings[pos,0::2] = np.sin(w*pos)\n",
    "        embeddings[pos,1::2] = np.cos(w*pos)\n",
    "    return torch.Tensor(embeddings)\n",
    "    \n",
    "pos_embed = position_embeddings(10000, position_encoding_size)\n",
    "pos_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used on top of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, bert_size, position_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculates the attention value\n",
    "        self.attention = nn.Linear(bert_size + position_size, 1)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "        # Makes the prediction\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(bert_size, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 6),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    '''\n",
    "    embeddings: shape (segment count, 512, bert_hidden_size)\n",
    "        The output of BERT\n",
    "    position_encodings:  shape (segment count, 512, position_encoding_size)\n",
    "        The position encodings of the tokens\n",
    "    comment_bounds: Array of tuples of the form [(start, end)]. \n",
    "        comment_bounds[i] = (a, b) indicates that comment i's embeddings can be extracted as embeddings[a:b]\n",
    "        if not provided, all embeddings and positions are assumed to be a single comment\n",
    "    '''\n",
    "    def forward(self, embeddings, position_encodings, comment_bounds = None):\n",
    "        # Concatenate each BERT output with its position encoding\n",
    "        attention_input = torch.cat([embeddings, position_encodings], dim=2) # (batch, 512, position_size + bert_hidden_size)\n",
    "\n",
    "        # Get the attention weights for each concatenated vector\n",
    "        attentions = self.attention(attention_input) #  (batch size, 512, 1)\n",
    "        \n",
    "        # If no bounds are probided, assume the input is all one comment\n",
    "        if comment_bounds is None:\n",
    "            # Softmax over attentions\n",
    "            attentions = self.softmax(attentions) # (batch, 512, 1)\n",
    "            \n",
    "            # Calculate the total embedding as a weighted sum of embeddings without the positional encodings\n",
    "            vecs = torch.sum(attentions * embeddings, dim=1) # (batch, bert_hidden_size)\n",
    "            return self.prediction(vecs) # (batch, 1)\n",
    "\n",
    "        # Otherwise, get the outputs for each comment\n",
    "        vecs = []\n",
    "        for (a,b) in comment_bounds:\n",
    "            # Get the embeddings and attentions for the current comment\n",
    "            comment_embeddings = embeddings[a:b] # (segment_count, 512, bert_hidden_size)\n",
    "            comment_attentions = attentions[a:b] # (segment_count, 512, 1)\n",
    "            \n",
    "            # softmax over the attentions for the comment\n",
    "            attention_weights = self.softmax(comment_attentions) # (segment_count, 512, 1)\n",
    "            \n",
    "            # Calculate the total embedding as a weighted sum over the embeddings of the comment\n",
    "            weighted_embeddings = attention_weights * embeddings[a:b] # (segment_count, 512, bert_hidden_size)\n",
    "            vec = torch.sum(weighted_embeddings.view(-1, weighted_embeddings.shape[-1]), dim=0, keepdim=True) # (segment_count, bert_hidden_size)\n",
    "            \n",
    "            vecs.append(vec)\n",
    "            \n",
    "        # Stack the total embedding vectors and give them to the prediction network to calculate the output\n",
    "        return self.prediction(torch.cat(vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset that does the data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    # file_format is a Python format string with a single variable to be inserted. It's used to get the paths of the files\n",
    "    # normalize indicates whether to perform data normalization procedure\n",
    "    def __init__(self, file_format, normalize):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load the data from files\n",
    "        input_ids = torch.load(file_format.format(\"input_ids\"))\n",
    "        positions = torch.load(file_format.format(\"positions\"))\n",
    "        comment_ids = torch.load(file_format.format(\"ids\"))\n",
    "        targets = torch.load(file_format.format(\"targets\"))\n",
    "\n",
    "        # Treat the targets as binary to separate the possible outputs\n",
    "        target_ids = torch.sum(torch.Tensor([32, 16, 8, 4, 2, 1]) * targets, axis=1)\n",
    "\n",
    "        # Store the data according to the target. Useful for normalization\n",
    "        self.data = [[] for i in range(64)]\n",
    "\n",
    "        # Load the data into the array\n",
    "        start_index = 0\n",
    "        end_index = 0\n",
    "        n_comments = comment_ids.shape[0]\n",
    "        # Group the items by which comment they're part of\n",
    "        while start_index < n_comments:\n",
    "            # Get the current comment id\n",
    "            curr_id = comment_ids[start_index]\n",
    "            \n",
    "            # Find end_index such that input_ids[end_index-1] is the last segment of the comment\n",
    "            while end_index < n_comments and comment_ids[end_index] == comment_ids[start_index]:\n",
    "                end_index += 1\n",
    "            \n",
    "            # Get the number with a binary representation that's the same as the comment's true labels\n",
    "            target_id = int(target_ids[curr_id].item())\n",
    "            \n",
    "            # Get the comment as a tuple containing (token ids, positions, true labels)\n",
    "            data = (input_ids[start_index:end_index], positions[start_index:end_index], targets[curr_id])\n",
    "            self.data[target_id].append(data)\n",
    "\n",
    "            start_index = end_index\n",
    "\n",
    "        # Remove the empty arrays from the data\n",
    "        self.data = [data for data in self.data if data]\n",
    "        \n",
    "        # Calculate how many comments are nontoxic\n",
    "        n_nontoxic = len(self.data[0])\n",
    "        \n",
    "        # Calculate how many comments there need to be with each combination of labels\n",
    "        # The goal is for there to be as many toxic comments as nontoxic comments\n",
    "        # Also, each combination of labels should be present an equal number of times\n",
    "        n_of_each = n_nontoxic // (len(self.data)-1)\n",
    "        \n",
    "        # Calculate how many copies need to be made from each combination of labels\n",
    "        n_copies = np.array([1]+[n_of_each // len(self.data[i]) for i in range(1,len(self.data))])\n",
    "        \n",
    "        # If normalization shouldn't be done, just replace n_copies with a bunch of ones so there is one copy of each comment\n",
    "        if not normalize:\n",
    "            n_copies = np.ones_like(n_copies)\n",
    "        \n",
    "        # The number of comments with each combination of labels\n",
    "        self.data_length = np.array([len(data) for data in self.data])\n",
    "\n",
    "        # The data is organized into segments each of which contains some number of copies of the comments with a specific combination of labels\n",
    "        segment_lengths = n_copies*self.data_length\n",
    "\n",
    "        # The total length of the normalized dataset\n",
    "        self.length = int(np.sum(segment_lengths))\n",
    "\n",
    "        # The indices bounding the segments\n",
    "        self.boundaries = np.zeros(segment_lengths.shape[0]+1)\n",
    "        self.boundaries[1:] = np.cumsum(segment_lengths)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Find the segment that the index is in\n",
    "        for i in range(self.boundaries.shape[0] - 1):\n",
    "            if index >= self.boundaries[i] and index < self.boundaries[i+1]:\n",
    "                # index - self.boundaries[i] calculates the index into the segment\n",
    "                # The segment is a bunch of copies of the same data, but it's inefficient to actually copy the data\n",
    "                # Therefore, \"% self.data.length[i]\" is used to convert the index into the segment into the index into the data\n",
    "                inner_index = int((index - self.boundaries[i]) % self.data_length[i])\n",
    "                return self.data[i][inner_index]\n",
    "        print(\"Index: \", index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Kaiming normal weight initialization when possibe and when not, just use normal initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model, stdv):\n",
    "    for param in model.parameters():\n",
    "        if len(param.shape) >= 2:\n",
    "            nn.init.kaiming_normal_(param.data)\n",
    "        else:\n",
    "            param.data.normal_(0.0, stdv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate AUC given predicted and true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_auc(pred, target):\n",
    "    result = []\n",
    "    for i in range(pred.shape[1]):\n",
    "        # If the true labels only has one value in a column, add a fake item to make AUC a valid operation\n",
    "        if len(np.unique(target[:,i])) == 2:\n",
    "            result.append(roc_auc_score(target[:,i], pred[:,i], labels=[0,1]))\n",
    "        else:\n",
    "            extra = np.array([1-target[0,i]])\n",
    "            target_i = np.concatenate((target[:,i], extra))\n",
    "            pred_i = np.concatenate((pred[:,i], extra))\n",
    "            result.append(roc_auc_score(target_i, pred_i, labels=[0,1]))\n",
    "            \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to save and load the model. Because ADAM has its own internal state, it's saved as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(tag):\n",
    "    model.load_state_dict(torch.load(model_path.format(tag)))\n",
    "    optimizer.load_state_dict(torch.load(adam_path.format(tag)))\n",
    "    \n",
    "def save_model(tag):\n",
    "    torch.save(model.state_dict(), model_path.format(tag))\n",
    "    torch.save(optimizer.state_dict(), adam_path.format(tag))\n",
    "    \n",
    "def load_bert(tag):\n",
    "    load_model(tag)\n",
    "    bert.load_state_dict(torch.load(bert_path.format(tag)))\n",
    "    \n",
    "def save_bert(tag):\n",
    "    save_model(tag)\n",
    "    torch.save(bert.state_dict(), bert_path.format(tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. \n",
    "\n",
    "The test data isn't normalized to give a realistic view of the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275845, 153164)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = MyDataset(\"train_{}.pt\", True)\n",
    "test_dataset = MyDataset(\"test_{}.pt\", False)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the data into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to format the data in a way that the model can use\n",
    "# batch is a list of tuples (tokenized_comment, positions, true_labels)\n",
    "def collate_samples(batch):\n",
    "    # Split the tuples into three variables\n",
    "    split_comments, positions, targets = zip(*batch)\n",
    "    \n",
    "    # An array of tuples (a,b) so that input_ids[a:b] are all part of a single comment\n",
    "    comment_bounds = np.zeros(len(split_comments)+1, dtype=np.int32)\n",
    "    comment_bounds[1:] = np.cumsum(list(map(len, split_comments)))\n",
    "    comment_bounds = np.array([comment_bounds[:-1], comment_bounds[1:]], dtype=np.int32).transpose()\n",
    "    \n",
    "    # For parallelism, stack the inputs into single tensors\n",
    "    input_ids = torch.cat(split_comments, dim=0)\n",
    "    \n",
    "    # Stack the position embeddings as well so they can be easily concatenated with the BERT output\n",
    "    encoded_positions = torch.cat([\n",
    "                          # Use the position array as indices into the position embedding\n",
    "                          pos_embed[position_arr]\n",
    "                          # For each comment in the batch\n",
    "                          for position_arr in positions                     \n",
    "                      ])\n",
    "  \n",
    "    # Stack the true labels\n",
    "    targets = torch.stack(targets)\n",
    "    return input_ids, encoded_positions, comment_bounds, targets\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train = DataLoader(train_dataset, \n",
    "                   batch_size = batch_size,\n",
    "                   shuffle=True,\n",
    "                   collate_fn = collate_samples)\n",
    "\n",
    "test = DataLoader(test_dataset, \n",
    "                   batch_size = batch_size,\n",
    "                   shuffle=True,\n",
    "                   collate_fn = collate_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512]) torch.Size([10, 512, 256]) (10, 2) torch.Size([10, 6])\n"
     ]
    }
   ],
   "source": [
    "for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(train):\n",
    "    print(input_ids.shape, encoded_position.shape, comment_bounds.shape, target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format strings used to generate the model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/model_{}.pt\"\n",
    "bert_path = \"./bert/bert_{}.pt\"\n",
    "adam_path = \"./adam/adam_{}.pt\"\n",
    "train_loss_path = \"./train_loss_unfrozen.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "bert_learning_rate = 5e-6\n",
    "unfrozen_layers = 3\n",
    "\n",
    "bert_hidden_size = 768\n",
    "\n",
    "# Put BERT on the GPU\n",
    "bert = bert.to(device)\n",
    "\n",
    "for param in bert.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for i in range(12-unfrozen_layers):\n",
    "    for param in bert.encoder.layer[i].parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "params = []\n",
    "params += list(bert.pooler.parameters())\n",
    "for j in range(12-unfrozen_layers, 12):\n",
    "    params += list(bert.encoder.layer[j].parameters())\n",
    "\n",
    "bert_optimizer = optim.Adam(params, lr=bert_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.encoder.layer[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The epoch to start at\n",
    "first_epoch = 32\n",
    "\n",
    "# The last epoch number\n",
    "epochs = 60\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model = Classifier(bert_hidden_size, position_encoding_size)\n",
    "init_weights(model, 0.2)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to load saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = \"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(32, 600, 0.1658)\n",
    "\n",
    "if load is not None:\n",
    "    load_model(load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training prints too many lines, so this class is used to write them to a file after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger:\n",
    "    def __init__(self, path):\n",
    "        self.file = open(path, \"w\")\n",
    "    \n",
    "    def log(self, string):\n",
    "        print(string)\n",
    "        self.file.write(string)\n",
    "        self.file.write(\"\\n\")\n",
    "    \n",
    "    def close(self):\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 32: 00000/1200 ( 1259.9s remaining)\t BCE Loss: 0.2318\n",
      "\tAUC: Toxic: 0.7083 Severe Toxic: 1.0000 Obscene: 0.7917 Threat: 0.9167 Insult: 0.8333 Identity-based Hate: 1.0000\n",
      "\tAccuracy: Toxic: 0.7000 Severe Toxic: 1.0000 Obscene: 0.8000 Threat: 0.9000 Insult: 0.9000 Identity-based Hate: 1.0000\n",
      "Epoch 32: 00010/1200 (  935.9s remaining)\t BCE Loss: 0.0492\n",
      "\tAUC: Toxic: 0.9587 Severe Toxic: 0.9474 Obscene: 0.9282 Threat: 0.9467 Insult: 0.9052 Identity-based Hate: 0.9318\n",
      "\tAccuracy: Toxic: 0.9636 Severe Toxic: 0.9818 Obscene: 0.9545 Threat: 0.9636 Insult: 0.9364 Identity-based Hate: 0.9727\n",
      "Epoch 32: 00020/1200 ( 1034.2s remaining)\t BCE Loss: 0.0747\n",
      "\tAUC: Toxic: 0.9548 Severe Toxic: 0.9583 Obscene: 0.9530 Threat: 0.9691 Insult: 0.9232 Identity-based Hate: 0.9478\n",
      "\tAccuracy: Toxic: 0.9619 Severe Toxic: 0.9762 Obscene: 0.9714 Threat: 0.9762 Insult: 0.9524 Identity-based Hate: 0.9619\n",
      "Epoch 32: 00030/1200 (  975.3s remaining)\t BCE Loss: 0.1585\n",
      "\tAUC: Toxic: 0.9646 Severe Toxic: 0.9577 Obscene: 0.9520 Threat: 0.9768 Insult: 0.9135 Identity-based Hate: 0.9450\n",
      "\tAccuracy: Toxic: 0.9677 Severe Toxic: 0.9710 Obscene: 0.9710 Threat: 0.9806 Insult: 0.9419 Identity-based Hate: 0.9645\n",
      "Epoch 32: 00040/1200 (  963.5s remaining)\t BCE Loss: 0.0303\n",
      "\tAUC: Toxic: 0.9697 Severe Toxic: 0.9549 Obscene: 0.9633 Threat: 0.9760 Insult: 0.9345 Identity-based Hate: 0.9425\n",
      "\tAccuracy: Toxic: 0.9732 Severe Toxic: 0.9732 Obscene: 0.9780 Threat: 0.9805 Insult: 0.9537 Identity-based Hate: 0.9634\n",
      "Epoch 32: 00050/1200 (  988.5s remaining)\t BCE Loss: 0.0215\n",
      "\tAUC: Toxic: 0.9626 Severe Toxic: 0.9609 Obscene: 0.9610 Threat: 0.9784 Insult: 0.9296 Identity-based Hate: 0.9343\n",
      "\tAccuracy: Toxic: 0.9667 Severe Toxic: 0.9745 Obscene: 0.9765 Threat: 0.9804 Insult: 0.9529 Identity-based Hate: 0.9549\n",
      "Epoch 32: 00060/1200 (  980.3s remaining)\t BCE Loss: 0.1920\n",
      "\tAUC: Toxic: 0.9622 Severe Toxic: 0.9501 Obscene: 0.9565 Threat: 0.9783 Insult: 0.9273 Identity-based Hate: 0.9354\n",
      "\tAccuracy: Toxic: 0.9639 Severe Toxic: 0.9689 Obscene: 0.9754 Threat: 0.9820 Insult: 0.9508 Identity-based Hate: 0.9574\n",
      "Epoch 32: 00070/1200 (  998.2s remaining)\t BCE Loss: 0.1140\n",
      "\tAUC: Toxic: 0.9587 Severe Toxic: 0.9445 Obscene: 0.9519 Threat: 0.9815 Insult: 0.9222 Identity-based Hate: 0.9259\n",
      "\tAccuracy: Toxic: 0.9634 Severe Toxic: 0.9690 Obscene: 0.9704 Threat: 0.9845 Insult: 0.9479 Identity-based Hate: 0.9549\n",
      "Epoch 32: 00080/1200 (  974.0s remaining)\t BCE Loss: 0.0637\n",
      "\tAUC: Toxic: 0.9619 Severe Toxic: 0.9454 Obscene: 0.9546 Threat: 0.9832 Insult: 0.9153 Identity-based Hate: 0.9294\n",
      "\tAccuracy: Toxic: 0.9667 Severe Toxic: 0.9704 Obscene: 0.9716 Threat: 0.9864 Insult: 0.9432 Identity-based Hate: 0.9556\n",
      "Epoch 32: 00090/1200 (  950.9s remaining)\t BCE Loss: 0.0914\n",
      "\tAUC: Toxic: 0.9583 Severe Toxic: 0.9443 Obscene: 0.9524 Threat: 0.9850 Insult: 0.9188 Identity-based Hate: 0.9249\n",
      "\tAccuracy: Toxic: 0.9648 Severe Toxic: 0.9681 Obscene: 0.9703 Threat: 0.9879 Insult: 0.9451 Identity-based Hate: 0.9560\n",
      "Epoch 32: 00100/1200 (  940.9s remaining)\t BCE Loss: 0.0689\n",
      "\tAUC: Toxic: 0.9606 Severe Toxic: 0.9375 Obscene: 0.9508 Threat: 0.9856 Insult: 0.9194 Identity-based Hate: 0.9168\n",
      "\tAccuracy: Toxic: 0.9673 Severe Toxic: 0.9663 Obscene: 0.9673 Threat: 0.9881 Insult: 0.9465 Identity-based Hate: 0.9515\n",
      "Epoch 32: 00110/1200 (  941.4s remaining)\t BCE Loss: 0.1847\n",
      "\tAUC: Toxic: 0.9585 Severe Toxic: 0.9439 Obscene: 0.9487 Threat: 0.9846 Insult: 0.9206 Identity-based Hate: 0.9125\n",
      "\tAccuracy: Toxic: 0.9649 Severe Toxic: 0.9685 Obscene: 0.9667 Threat: 0.9883 Insult: 0.9477 Identity-based Hate: 0.9505\n",
      "Epoch 32: 00120/1200 (  911.8s remaining)\t BCE Loss: 0.0453\n",
      "\tAUC: Toxic: 0.9576 Severe Toxic: 0.9451 Obscene: 0.9469 Threat: 0.9835 Insult: 0.9225 Identity-based Hate: 0.9149\n",
      "\tAccuracy: Toxic: 0.9636 Severe Toxic: 0.9686 Obscene: 0.9653 Threat: 0.9876 Insult: 0.9496 Identity-based Hate: 0.9512\n",
      "Epoch 32: 00130/1200 (  910.6s remaining)\t BCE Loss: 0.0409\n",
      "\tAUC: Toxic: 0.9583 Severe Toxic: 0.9379 Obscene: 0.9501 Threat: 0.9833 Insult: 0.9263 Identity-based Hate: 0.9179\n",
      "\tAccuracy: Toxic: 0.9641 Severe Toxic: 0.9656 Obscene: 0.9664 Threat: 0.9863 Insult: 0.9511 Identity-based Hate: 0.9496\n",
      "Epoch 32: 00190/1200 (  799.8s remaining)\t BCE Loss: 0.1091\n",
      "\tAUC: Toxic: 0.9563 Severe Toxic: 0.9449 Obscene: 0.9448 Threat: 0.9794 Insult: 0.9189 Identity-based Hate: 0.9257\n",
      "\tAccuracy: Toxic: 0.9628 Severe Toxic: 0.9670 Obscene: 0.9649 Threat: 0.9843 Insult: 0.9455 Identity-based Hate: 0.9550\n",
      "Epoch 32: 00200/1200 (  774.7s remaining)\t BCE Loss: 0.2763\n",
      "\tAUC: Toxic: 0.9572 Severe Toxic: 0.9429 Obscene: 0.9463 Threat: 0.9799 Insult: 0.9194 Identity-based Hate: 0.9256\n",
      "\tAccuracy: Toxic: 0.9632 Severe Toxic: 0.9667 Obscene: 0.9657 Threat: 0.9841 Insult: 0.9453 Identity-based Hate: 0.9537\n",
      "Epoch 32: 00210/1200 (  780.8s remaining)\t BCE Loss: 0.1316\n",
      "\tAUC: Toxic: 0.9574 Severe Toxic: 0.9459 Obscene: 0.9454 Threat: 0.9802 Insult: 0.9190 Identity-based Hate: 0.9295\n",
      "\tAccuracy: Toxic: 0.9635 Severe Toxic: 0.9682 Obscene: 0.9654 Threat: 0.9839 Insult: 0.9460 Identity-based Hate: 0.9550\n"
     ]
    }
   ],
   "source": [
    "# How often to print\n",
    "log_frequency = 10\n",
    "\n",
    "# How many batches to run between saving the model\n",
    "save_frequency = 200\n",
    "bert_save_frequency = 600\n",
    "\n",
    "# How many epochs to run before clearing the output\n",
    "clear_frequency_batches = 300\n",
    "clear_frequency_epochs = 1\n",
    "\n",
    "# Maximum batches per epoch\n",
    "batches_per_train_epoch = 1200\n",
    "batches_per_test_epoch = 600\n",
    "\n",
    "batches_per_train_epoch = min(batches_per_train_epoch, len(train))\n",
    "batches_per_test_epoch = min(batches_per_test_epoch, len(test))\n",
    "\n",
    "# The format string used to generate paths for the log files\n",
    "log_path = \"./logs/epoch_{}.txt\"\n",
    "\n",
    "# Store the times to help forecast how long training will take\n",
    "epoch_times = []\n",
    "batch_times = []\n",
    "\n",
    "# Store the training loss after each batch\n",
    "train_losses = []\n",
    "for epoch in range(first_epoch, epochs):\n",
    "    # initialize the logger object\n",
    "    logger = EpochLogger(log_path.format(epoch))\n",
    "    \n",
    "    # Record the time at the start of the epoch\n",
    "    epoch_start = time()\n",
    "    \n",
    "    # Predictions and true labels used to calculate the AUC\n",
    "    predicted = None\n",
    "    true = None\n",
    "    \n",
    "    # Recording how many were correct and the total number of predictions to calculate the accuracy\n",
    "    n_correct = 0\n",
    "    n_processed = 0\n",
    "    print(\"Training\")\n",
    "    for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(train):\n",
    "        batch_start = time()\n",
    "        # Get the BERT output\n",
    "        encoded_comments = bert(input_ids.to(device))[0]\n",
    "    \n",
    "        # Get the outputs from the network\n",
    "        output = model(encoded_comments, encoded_position.to(device), comment_bounds)\n",
    "\n",
    "        # Gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        bert_optimizer.zero_grad()\n",
    "        loss = criterion(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        bert_optimizer.step()\n",
    "        \n",
    "        # Store how long this batch took to run\n",
    "        batch_times.append(time() - batch_start)\n",
    "        \n",
    "        # Only use the last 100 batches to estimate the time remaining\n",
    "        batch_times = batch_times[-100:]\n",
    "        \n",
    "        # Calculate the predicted labels by rounding to zero or 1\n",
    "        pred = torch.round(output.cpu().detach())\n",
    "        \n",
    "        # Add the predicted and true labels to the arrays\n",
    "        if predicted is None:\n",
    "            predicted = pred.clone()\n",
    "            true = target.clone()\n",
    "        else:\n",
    "            predicted = torch.cat((predicted, pred), dim=0)\n",
    "            true = torch.cat((true, target), dim=0)\n",
    "\n",
    "        # Calculate the number that were correct\n",
    "        n_correct += torch.sum(pred == target, axis=0).numpy()\n",
    "        n_processed += pred.shape[0]\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "        # Print debugging information\n",
    "        if i % log_frequency == 0:\n",
    "            logger.log(\"Epoch {}: {:05d}/{} ({:7.01f}s remaining)\\t BCE Loss: {:.04f}\".format(epoch, i, batches_per_train_epoch, np.mean(batch_times)*(batches_per_train_epoch - i), loss.item()))\n",
    "            auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "            logger.log(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "            acc = n_correct / n_processed\n",
    "            logger.log(\"\\tAccuracy: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "        # Save the model\n",
    "        if i % save_frequency == 0:\n",
    "            save_model(\"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(epoch, i, loss.item()))\n",
    "            torch.save(torch.Tensor(train_losses), train_loss_path)\n",
    "        if i % bert_save_frequency == 0:\n",
    "            save_bert(\"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(epoch, i, loss.item()))\n",
    "            torch.save(torch.Tensor(train_losses), train_loss_path)\n",
    "        if i % clear_frequency_batches == 0 and i != 0:\n",
    "            clear_output()\n",
    "        # Break early \n",
    "        if i % batches_per_train_epoch == 0 and i != 0:\n",
    "            break\n",
    "    \n",
    "    # Make sure that the model is saved at the end of the epoch\n",
    "    save_model(\"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(epoch, i, loss.item()))\n",
    "    \n",
    "    # Save the training losses\n",
    "    torch.save(torch.Tensor(train_losses), train_loss_path)\n",
    "    \n",
    "    # Test the model\n",
    "    with torch.no_grad():\n",
    "        print(\"Testing\")\n",
    "        predicted = None\n",
    "        for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(test):\n",
    "            batch_start = time()\n",
    "            # Get the BERT output\n",
    "            encoded_comments = bert(input_ids.to(device))[0]\n",
    "\n",
    "            # Get the outputs from the network\n",
    "            output = model(encoded_comments, encoded_position.to(device), comment_bounds)\n",
    "\n",
    "            pred = torch.round(output.cpu().detach())\n",
    "            if predicted is None:\n",
    "                predicted = pred.clone()\n",
    "                true = target.clone()\n",
    "            else:\n",
    "                predicted = torch.cat((predicted, pred), dim=0)\n",
    "                true = torch.cat((true, target), dim=0)\n",
    "\n",
    "            n_correct += torch.sum(pred == target, axis=0).numpy()\n",
    "            n_processed += pred.shape[0]\n",
    "\n",
    "            if i % log_frequency == 0:\n",
    "                logger.log(\"Epoch {}: {:05d}/{} ({:7.01f}s remaining)\\t BCE Loss: {:.04f}\".format(epoch, i, batches_per_test_epoch, np.mean(batch_times)*(batches_per_test_epoch - i), loss.item()))\n",
    "                auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "                logger.log(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "                acc = n_correct / n_processed\n",
    "                logger.log(\"\\tAccuracy: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "            if i % batches_per_test_epoch == 0 and i != 0:\n",
    "                break\n",
    "                \n",
    "        auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "        acc = n_correct / n_processed\n",
    "        epoch_time = time() - epoch_start\n",
    "        logger.log(\"Epoch {} took {:7.01f}s. Test Values:\".format(epoch, epoch_time))\n",
    "        \n",
    "        logger.log(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "        \n",
    "        logger.log(\"\\tAccuracy:  Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "    logger.close()\n",
    "    if epoch % clear_frequency_epochs == 0 and epoch != 0:\n",
    "        clear_output()\n",
    "    epoch_times.append(epoch_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-98f6a1ca628c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "logger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
