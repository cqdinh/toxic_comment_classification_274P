{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 256])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_encoding_size = 256\n",
    "\n",
    "def position_embeddings(max_pos, size):\n",
    "    embeddings = np.zeros((max_pos, size))\n",
    "    w = 1 / (10000 ** (2*np.arange(size // 2 )/size))\n",
    "    for pos in range(max_pos):\n",
    "        embeddings[pos,0::2] = np.sin(w*pos)\n",
    "        embeddings[pos,1::2] = np.cos(w*pos)\n",
    "    return torch.Tensor(embeddings)\n",
    "    \n",
    "pos_embed = position_embeddings(10000, position_encoding_size)\n",
    "pos_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, bert_size, position_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = nn.Linear(bert_size + position_size, 1)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(bert_size, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 6),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    '''\n",
    "    embeddings: shape (segment count, 512, bert_hidden_size)\n",
    "    position_encodings:  shape (segment count, 512, position_encoding_size)\n",
    "    comment_bounds: Array of tuples of the form [(start, end)]. comment_bounds[i] = (a, b) indicates that comment i's embeddings can be extracted as embeddings[a:b]\n",
    "    '''\n",
    "    def forward(self, embeddings, position_encodings, comment_bounds = None):\n",
    "        attention_input = torch.cat([embeddings, position_encodings], dim=2) # (batch, 512, position_size + bert_hidden_size)\n",
    "\n",
    "        # (batch, 512, 1)\n",
    "        attentions = self.attention(attention_input)\n",
    "        if comment_bounds is None:\n",
    "            attentions = self.softmax(attentions) # (batch, 512, 1)\n",
    "            vecs = torch.sum(attentions * embeddings, dim=1) # (batch, bert_hidden_size)\n",
    "            return self.prediction(vecs) # (batch, 1)\n",
    "\n",
    "        vecs = []\n",
    "        for (a,b) in comment_bounds:\n",
    "            comment_embeddings = embeddings[a:b] # (segment_count, 512, bert_hidden_size)\n",
    "            comment_attentions = attentions[a:b] # (segment_count, 512, 1)\n",
    "            attention_weights = self.softmax(comment_attentions) # (segment_count, 512, 1)\n",
    "            weighted_embeddings = attention_weights * embeddings[a:b] # (segment_count, 512, bert_hidden_size)\n",
    "            vec = torch.sum(weighted_embeddings.view(-1, weighted_embeddings.shape[-1]), dim=0, keepdim=True) # (segment_count, bert_hidden_size)\n",
    "            vecs.append(vec)\n",
    "        return self.prediction(torch.cat(vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_format, normalize):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load the data from files\n",
    "        input_ids = torch.load(file_format.format(\"input_ids\"))\n",
    "        positions = torch.load(file_format.format(\"positions\"))\n",
    "        comment_ids = torch.load(file_format.format(\"ids\"))\n",
    "        targets = torch.load(file_format.format(\"targets\"))\n",
    "\n",
    "        # Treat the targets as binary to separate the possible outputs\n",
    "        target_ids = torch.sum(torch.Tensor([32, 16, 8, 4, 2, 1]) * targets, axis=1)\n",
    "\n",
    "        # Store the data according to the target. Useful for normalization\n",
    "        self.data = [[] for i in range(64)]\n",
    "\n",
    "        # Load the data into the array\n",
    "        curr_id = 0\n",
    "        start_index = 0\n",
    "        for i in range(comment_ids.shape[0]):\n",
    "            if comment_ids[i] != curr_id:\n",
    "                target_id = int(target_ids[curr_id].item())\n",
    "                data = (input_ids[start_index:i], positions[start_index:i], targets[curr_id])\n",
    "                self.data[target_id].append(data)\n",
    "\n",
    "                curr_id = comment_ids[i]\n",
    "                start_index = i\n",
    "\n",
    "        target_id = int(target_ids[curr_id].item())\n",
    "        data = (input_ids[start_index:i], positions[start_index:i], targets[curr_id])\n",
    "        self.data[target_id].append(data)\n",
    "\n",
    "        n_nontoxic = len(self.data[0])\n",
    "\n",
    "        n_of_each = n_nontoxic // (len(self.data)-1)\n",
    "\n",
    "        # Remove the empty arrays from the data\n",
    "        self.data = [data for data in self.data if data]\n",
    "\n",
    "        n_copies = np.array([1]+[n_of_each // len(self.data[i]) for i in range(1,len(self.data))])\n",
    "        \n",
    "        if not normalize:\n",
    "            n_copies = np.ones_like(n_copies)\n",
    "        \n",
    "        self.data_length = np.array([len(data) for data in self.data])\n",
    "\n",
    "        segment_lengths = n_copies*self.data_length\n",
    "\n",
    "        self.length = int(np.sum(segment_lengths))\n",
    "\n",
    "        self.boundaries = np.zeros(segment_lengths.shape[0]+1)\n",
    "        self.boundaries[1:] = np.cumsum(segment_lengths)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        for i in range(self.boundaries.shape[0] - 1):\n",
    "            if index >= self.boundaries[i] and index < self.boundaries[i+1]:\n",
    "                inner_index = int((index - self.boundaries[i]) % self.data_length[i])\n",
    "                return self.data[i][inner_index]\n",
    "        print(\"Index: \", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63978"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MyDataset(\"test_{}.pt\", False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterable(obj):\n",
    "    try:\n",
    "        iter(obj)\n",
    "    except Exception:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def collate_samples(batch):\n",
    "    for i, item in enumerate(batch):\n",
    "        if not iterable(item):\n",
    "            print(i, item)\n",
    "    split_comments, positions, targets = zip(*batch)\n",
    "    input_ids = []\n",
    "    comment_bounds = []\n",
    "    start = 0\n",
    "    for comment in split_comments:\n",
    "        input_ids += comment\n",
    "        comment_bounds.append((start, start+len(comment)))\n",
    "        start += len(comment)\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    encoded_positions = torch.cat([\n",
    "                          # Use the position array as indices into the position embedding\n",
    "                          pos_embed[position_arr]\n",
    "                          # For each comment in the batch\n",
    "                          for position_arr in positions                     \n",
    "                      ])\n",
    "  \n",
    "    targets = [target.view(1, -1) for target in targets]\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    comment_bounds = np.array(comment_bounds, dtype=np.int32)\n",
    "    return input_ids, encoded_positions, comment_bounds, targets\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "test = DataLoader(test_dataset, \n",
    "                   batch_size = batch_size,\n",
    "                   shuffle=True,\n",
    "                   collate_fn = collate_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([129, 512]) torch.Size([129, 512, 256]) (128, 2) torch.Size([128, 6])\n"
     ]
    }
   ],
   "source": [
    "for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(test):\n",
    "    print(input_ids.shape, encoded_position.shape, comment_bounds.shape, target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_auc(pred, target):\n",
    "    result = []\n",
    "    for i in range(pred.shape[1]):\n",
    "        if len(np.unique(target[:,i])) == 2:\n",
    "            result.append(roc_auc_score(target[:,i], pred[:,i], labels=[0,1]))\n",
    "        else:\n",
    "            extra = np.array([1-target[0,i]])\n",
    "            target_i = np.concatenate((target[:,i], extra))\n",
    "            pred_i = np.concatenate((pred[:,i], extra))\n",
    "            result.append(roc_auc_score(target_i, pred_i, labels=[0,1]))\n",
    "            \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/model_{}.pt\"\n",
    "bert_path = \"./bert/bert_{}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "bert_hidden_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.eval()\n",
    "device = torch.device(\"cuda:0\")\n",
    "bert = bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (attention): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (prediction): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1024, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=1024, out_features=6, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier(bert_hidden_size, position_encoding_size)\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = \"epoch_{:04d}_batch_{:04d}_bce_{:.04f}\".format(44, 1200, 0.0141)\n",
    "load_bert = True\n",
    "\n",
    "if load is not None:\n",
    "    model.load_state_dict(torch.load(model_path.format(load)))\n",
    "    if load_bert:\n",
    "        bert.load_state_dict(torch.load(bert_path.format(load)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger:\n",
    "    def __init__(self, path):\n",
    "        self.file = open(path, \"w\")\n",
    "    \n",
    "    def log(self, string):\n",
    "        print(string)\n",
    "        self.file.write(string)\n",
    "        self.file.write(\"\\n\")\n",
    "    \n",
    "    def close(self):\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 00401/500 (  890.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9463 Severe Toxic: 0.9686 Obscene: 0.9531 Threat: 0.9797 Insult: 0.9430 Identity-based Hate: 0.9641\n",
      "\tAccuracy: Toxic: 0.9258 Severe Toxic: 0.9798 Obscene: 0.9370 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9676\n",
      "Batch 00402/500 (  881.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9462 Severe Toxic: 0.9687 Obscene: 0.9531 Threat: 0.9798 Insult: 0.9429 Identity-based Hate: 0.9641\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9798 Obscene: 0.9370 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9675\n",
      "Batch 00403/500 (  872.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9461 Severe Toxic: 0.9687 Obscene: 0.9530 Threat: 0.9798 Insult: 0.9429 Identity-based Hate: 0.9637\n",
      "\tAccuracy: Toxic: 0.9258 Severe Toxic: 0.9798 Obscene: 0.9370 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9675\n",
      "Batch 00404/500 (  863.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9461 Severe Toxic: 0.9677 Obscene: 0.9531 Threat: 0.9798 Insult: 0.9429 Identity-based Hate: 0.9636\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9798 Obscene: 0.9370 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9676\n",
      "Batch 00405/500 (  854.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9462 Severe Toxic: 0.9678 Obscene: 0.9531 Threat: 0.9800 Insult: 0.9429 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9258 Severe Toxic: 0.9798 Obscene: 0.9371 Threat: 0.9936 Insult: 0.9224 Identity-based Hate: 0.9675\n",
      "Batch 00406/500 (  845.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9462 Severe Toxic: 0.9678 Obscene: 0.9531 Threat: 0.9800 Insult: 0.9429 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9798 Obscene: 0.9372 Threat: 0.9936 Insult: 0.9225 Identity-based Hate: 0.9675\n",
      "Batch 00407/500 (  836.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9460 Severe Toxic: 0.9678 Obscene: 0.9529 Threat: 0.9799 Insult: 0.9429 Identity-based Hate: 0.9636\n",
      "\tAccuracy: Toxic: 0.9256 Severe Toxic: 0.9798 Obscene: 0.9372 Threat: 0.9936 Insult: 0.9224 Identity-based Hate: 0.9675\n",
      "Batch 00408/500 (  827.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9460 Severe Toxic: 0.9678 Obscene: 0.9530 Threat: 0.9799 Insult: 0.9429 Identity-based Hate: 0.9636\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9372 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9675\n",
      "Batch 00409/500 (  817.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9460 Severe Toxic: 0.9678 Obscene: 0.9530 Threat: 0.9799 Insult: 0.9430 Identity-based Hate: 0.9636\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9371 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9675\n",
      "Batch 00410/500 (  808.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9460 Severe Toxic: 0.9679 Obscene: 0.9529 Threat: 0.9800 Insult: 0.9429 Identity-based Hate: 0.9637\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9371 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9676\n",
      "Batch 00411/500 (  799.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9461 Severe Toxic: 0.9680 Obscene: 0.9530 Threat: 0.9801 Insult: 0.9430 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9797 Obscene: 0.9371 Threat: 0.9935 Insult: 0.9223 Identity-based Hate: 0.9676\n",
      "Batch 00412/500 (  790.8s remaining)\t \n",
      "\tAUC: Toxic: 0.9461 Severe Toxic: 0.9680 Obscene: 0.9531 Threat: 0.9801 Insult: 0.9430 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9797 Obscene: 0.9371 Threat: 0.9935 Insult: 0.9223 Identity-based Hate: 0.9675\n",
      "Batch 00413/500 (  781.8s remaining)\t \n",
      "\tAUC: Toxic: 0.9461 Severe Toxic: 0.9680 Obscene: 0.9530 Threat: 0.9801 Insult: 0.9431 Identity-based Hate: 0.9636\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9797 Obscene: 0.9371 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9675\n",
      "Batch 00414/500 (  772.8s remaining)\t \n",
      "\tAUC: Toxic: 0.9460 Severe Toxic: 0.9681 Obscene: 0.9530 Threat: 0.9798 Insult: 0.9430 Identity-based Hate: 0.9637\n",
      "\tAccuracy: Toxic: 0.9254 Severe Toxic: 0.9797 Obscene: 0.9370 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9675\n",
      "Batch 00415/500 (  763.7s remaining)\t \n",
      "\tAUC: Toxic: 0.9461 Severe Toxic: 0.9681 Obscene: 0.9531 Threat: 0.9798 Insult: 0.9431 Identity-based Hate: 0.9637\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9796 Obscene: 0.9370 Threat: 0.9935 Insult: 0.9223 Identity-based Hate: 0.9674\n",
      "Batch 00416/500 (  754.7s remaining)\t \n",
      "\tAUC: Toxic: 0.9461 Severe Toxic: 0.9681 Obscene: 0.9530 Threat: 0.9799 Insult: 0.9431 Identity-based Hate: 0.9637\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9796 Obscene: 0.9369 Threat: 0.9935 Insult: 0.9223 Identity-based Hate: 0.9674\n",
      "Batch 00417/500 (  745.6s remaining)\t \n",
      "\tAUC: Toxic: 0.9462 Severe Toxic: 0.9682 Obscene: 0.9528 Threat: 0.9799 Insult: 0.9431 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9796 Obscene: 0.9370 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9673\n",
      "Batch 00418/500 (  736.6s remaining)\t \n",
      "\tAUC: Toxic: 0.9462 Severe Toxic: 0.9682 Obscene: 0.9527 Threat: 0.9801 Insult: 0.9432 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9797 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9224 Identity-based Hate: 0.9673\n",
      "Batch 00419/500 (  727.6s remaining)\t \n",
      "\tAUC: Toxic: 0.9462 Severe Toxic: 0.9683 Obscene: 0.9527 Threat: 0.9802 Insult: 0.9432 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9254 Severe Toxic: 0.9797 Obscene: 0.9369 Threat: 0.9935 Insult: 0.9223 Identity-based Hate: 0.9673\n",
      "Batch 00420/500 (  718.6s remaining)\t \n",
      "\tAUC: Toxic: 0.9462 Severe Toxic: 0.9683 Obscene: 0.9527 Threat: 0.9802 Insult: 0.9432 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9254 Severe Toxic: 0.9797 Obscene: 0.9369 Threat: 0.9935 Insult: 0.9222 Identity-based Hate: 0.9673\n",
      "Batch 00421/500 (  709.6s remaining)\t \n",
      "\tAUC: Toxic: 0.9462 Severe Toxic: 0.9683 Obscene: 0.9526 Threat: 0.9802 Insult: 0.9431 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9254 Severe Toxic: 0.9797 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9674\n",
      "Batch 00422/500 (  700.6s remaining)\t \n",
      "\tAUC: Toxic: 0.9462 Severe Toxic: 0.9683 Obscene: 0.9526 Threat: 0.9802 Insult: 0.9431 Identity-based Hate: 0.9636\n",
      "\tAccuracy: Toxic: 0.9253 Severe Toxic: 0.9796 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9674\n",
      "Batch 00423/500 (  691.7s remaining)\t \n",
      "\tAUC: Toxic: 0.9462 Severe Toxic: 0.9683 Obscene: 0.9527 Threat: 0.9802 Insult: 0.9432 Identity-based Hate: 0.9636\n",
      "\tAccuracy: Toxic: 0.9254 Severe Toxic: 0.9796 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9674\n",
      "Batch 00424/500 (  682.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9463 Severe Toxic: 0.9685 Obscene: 0.9528 Threat: 0.9803 Insult: 0.9429 Identity-based Hate: 0.9636\n",
      "\tAccuracy: Toxic: 0.9253 Severe Toxic: 0.9796 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9673\n",
      "Batch 00425/500 (  673.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9462 Severe Toxic: 0.9686 Obscene: 0.9527 Threat: 0.9804 Insult: 0.9429 Identity-based Hate: 0.9636\n",
      "\tAccuracy: Toxic: 0.9253 Severe Toxic: 0.9796 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9673\n",
      "Batch 00426/500 (  664.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9462 Severe Toxic: 0.9687 Obscene: 0.9526 Threat: 0.9803 Insult: 0.9430 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9254 Severe Toxic: 0.9797 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9673\n",
      "Batch 00427/500 (  655.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9463 Severe Toxic: 0.9687 Obscene: 0.9526 Threat: 0.9803 Insult: 0.9430 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9254 Severe Toxic: 0.9797 Obscene: 0.9367 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9673\n",
      "Batch 00428/500 (  647.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9463 Severe Toxic: 0.9688 Obscene: 0.9526 Threat: 0.9803 Insult: 0.9430 Identity-based Hate: 0.9633\n",
      "\tAccuracy: Toxic: 0.9254 Severe Toxic: 0.9797 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9673\n",
      "Batch 00429/500 (  638.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9464 Severe Toxic: 0.9688 Obscene: 0.9526 Threat: 0.9803 Insult: 0.9430 Identity-based Hate: 0.9633\n",
      "\tAccuracy: Toxic: 0.9254 Severe Toxic: 0.9797 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9673\n",
      "Batch 00430/500 (  629.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9464 Severe Toxic: 0.9689 Obscene: 0.9528 Threat: 0.9803 Insult: 0.9431 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9797 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 00431/500 (  620.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9463 Severe Toxic: 0.9690 Obscene: 0.9528 Threat: 0.9803 Insult: 0.9432 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9254 Severe Toxic: 0.9797 Obscene: 0.9369 Threat: 0.9937 Insult: 0.9223 Identity-based Hate: 0.9673\n",
      "Batch 00432/500 (  611.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9463 Severe Toxic: 0.9690 Obscene: 0.9529 Threat: 0.9803 Insult: 0.9432 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9254 Severe Toxic: 0.9797 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9673\n",
      "Batch 00433/500 (  602.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9464 Severe Toxic: 0.9690 Obscene: 0.9528 Threat: 0.9802 Insult: 0.9432 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9797 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9673\n",
      "Batch 00434/500 (  593.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9463 Severe Toxic: 0.9690 Obscene: 0.9528 Threat: 0.9802 Insult: 0.9432 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9797 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9672\n",
      "Batch 00435/500 (  584.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9464 Severe Toxic: 0.9688 Obscene: 0.9528 Threat: 0.9802 Insult: 0.9432 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9796 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9673\n",
      "Batch 00436/500 (  575.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9463 Severe Toxic: 0.9689 Obscene: 0.9527 Threat: 0.9802 Insult: 0.9432 Identity-based Hate: 0.9633\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9797 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9672\n",
      "Batch 00437/500 (  566.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9464 Severe Toxic: 0.9690 Obscene: 0.9528 Threat: 0.9802 Insult: 0.9433 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9797 Obscene: 0.9368 Threat: 0.9937 Insult: 0.9222 Identity-based Hate: 0.9673\n",
      "Batch 00438/500 (  557.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9463 Severe Toxic: 0.9691 Obscene: 0.9527 Threat: 0.9802 Insult: 0.9433 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9797 Obscene: 0.9367 Threat: 0.9936 Insult: 0.9223 Identity-based Hate: 0.9673\n",
      "Batch 00439/500 (  548.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9463 Severe Toxic: 0.9692 Obscene: 0.9526 Threat: 0.9802 Insult: 0.9431 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9366 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9673\n",
      "Batch 00440/500 (  539.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9463 Severe Toxic: 0.9691 Obscene: 0.9527 Threat: 0.9802 Insult: 0.9431 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9366 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9672\n",
      "Batch 00441/500 (  530.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9464 Severe Toxic: 0.9691 Obscene: 0.9527 Threat: 0.9802 Insult: 0.9431 Identity-based Hate: 0.9633\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9367 Threat: 0.9937 Insult: 0.9222 Identity-based Hate: 0.9671\n",
      "Batch 00442/500 (  521.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9464 Severe Toxic: 0.9691 Obscene: 0.9527 Threat: 0.9802 Insult: 0.9431 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9367 Threat: 0.9937 Insult: 0.9222 Identity-based Hate: 0.9672\n",
      "Batch 00443/500 (  512.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9465 Severe Toxic: 0.9691 Obscene: 0.9525 Threat: 0.9801 Insult: 0.9430 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9367 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00444/500 (  503.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9465 Severe Toxic: 0.9691 Obscene: 0.9525 Threat: 0.9802 Insult: 0.9431 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9367 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9672\n",
      "Batch 00445/500 (  494.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9466 Severe Toxic: 0.9691 Obscene: 0.9526 Threat: 0.9802 Insult: 0.9431 Identity-based Hate: 0.9633\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9367 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9671\n",
      "Batch 00446/500 (  485.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9466 Severe Toxic: 0.9691 Obscene: 0.9526 Threat: 0.9802 Insult: 0.9432 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9254 Severe Toxic: 0.9798 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00447/500 (  476.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9466 Severe Toxic: 0.9692 Obscene: 0.9526 Threat: 0.9803 Insult: 0.9432 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00448/500 (  467.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9467 Severe Toxic: 0.9691 Obscene: 0.9526 Threat: 0.9804 Insult: 0.9431 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00449/500 (  458.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9467 Severe Toxic: 0.9691 Obscene: 0.9526 Threat: 0.9804 Insult: 0.9431 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00450/500 (  449.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9468 Severe Toxic: 0.9691 Obscene: 0.9527 Threat: 0.9800 Insult: 0.9432 Identity-based Hate: 0.9636\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00451/500 (  440.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9467 Severe Toxic: 0.9692 Obscene: 0.9527 Threat: 0.9801 Insult: 0.9433 Identity-based Hate: 0.9637\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9671\n",
      "Batch 00452/500 (  431.2s remaining)\t \n",
      "\tAUC: Toxic: 0.9467 Severe Toxic: 0.9692 Obscene: 0.9528 Threat: 0.9801 Insult: 0.9432 Identity-based Hate: 0.9637\n",
      "\tAccuracy: Toxic: 0.9254 Severe Toxic: 0.9798 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9671\n",
      "Batch 00453/500 (  422.1s remaining)\t \n",
      "\tAUC: Toxic: 0.9467 Severe Toxic: 0.9692 Obscene: 0.9529 Threat: 0.9801 Insult: 0.9432 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9370 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9671\n",
      "Batch 00454/500 (  413.2s remaining)\t \n",
      "\tAUC: Toxic: 0.9467 Severe Toxic: 0.9694 Obscene: 0.9529 Threat: 0.9801 Insult: 0.9434 Identity-based Hate: 0.9639\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9672\n",
      "Batch 00455/500 (  404.2s remaining)\t \n",
      "\tAUC: Toxic: 0.9467 Severe Toxic: 0.9694 Obscene: 0.9527 Threat: 0.9802 Insult: 0.9434 Identity-based Hate: 0.9639\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9799 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9672\n",
      "Batch 00456/500 (  395.2s remaining)\t \n",
      "\tAUC: Toxic: 0.9466 Severe Toxic: 0.9694 Obscene: 0.9526 Threat: 0.9802 Insult: 0.9434 Identity-based Hate: 0.9639\n",
      "\tAccuracy: Toxic: 0.9254 Severe Toxic: 0.9799 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9672\n",
      "Batch 00457/500 (  386.2s remaining)\t \n",
      "\tAUC: Toxic: 0.9466 Severe Toxic: 0.9694 Obscene: 0.9526 Threat: 0.9803 Insult: 0.9435 Identity-based Hate: 0.9639\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9799 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9671\n",
      "Batch 00458/500 (  377.3s remaining)\t \n",
      "\tAUC: Toxic: 0.9467 Severe Toxic: 0.9696 Obscene: 0.9526 Threat: 0.9804 Insult: 0.9435 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9671\n",
      "Batch 00459/500 (  368.3s remaining)\t \n",
      "\tAUC: Toxic: 0.9468 Severe Toxic: 0.9698 Obscene: 0.9524 Threat: 0.9754 Insult: 0.9436 Identity-based Hate: 0.9639\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9367 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9671\n",
      "Batch 00460/500 (  359.4s remaining)\t \n",
      "\tAUC: Toxic: 0.9467 Severe Toxic: 0.9698 Obscene: 0.9523 Threat: 0.9755 Insult: 0.9436 Identity-based Hate: 0.9640\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9799 Obscene: 0.9366 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 00461/500 (  350.4s remaining)\t \n",
      "\tAUC: Toxic: 0.9467 Severe Toxic: 0.9697 Obscene: 0.9524 Threat: 0.9755 Insult: 0.9436 Identity-based Hate: 0.9639\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9366 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9672\n",
      "Batch 00462/500 (  341.4s remaining)\t \n",
      "\tAUC: Toxic: 0.9467 Severe Toxic: 0.9697 Obscene: 0.9524 Threat: 0.9755 Insult: 0.9437 Identity-based Hate: 0.9640\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9367 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9672\n",
      "Batch 00463/500 (  332.4s remaining)\t \n",
      "\tAUC: Toxic: 0.9468 Severe Toxic: 0.9697 Obscene: 0.9523 Threat: 0.9755 Insult: 0.9437 Identity-based Hate: 0.9641\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9798 Obscene: 0.9367 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9672\n",
      "Batch 00464/500 (  323.4s remaining)\t \n",
      "\tAUC: Toxic: 0.9468 Severe Toxic: 0.9698 Obscene: 0.9523 Threat: 0.9756 Insult: 0.9437 Identity-based Hate: 0.9637\n",
      "\tAccuracy: Toxic: 0.9256 Severe Toxic: 0.9798 Obscene: 0.9367 Threat: 0.9935 Insult: 0.9222 Identity-based Hate: 0.9672\n",
      "Batch 00465/500 (  314.5s remaining)\t \n",
      "\tAUC: Toxic: 0.9468 Severe Toxic: 0.9698 Obscene: 0.9523 Threat: 0.9756 Insult: 0.9437 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9256 Severe Toxic: 0.9798 Obscene: 0.9367 Threat: 0.9936 Insult: 0.9222 Identity-based Hate: 0.9672\n",
      "Batch 00466/500 (  305.4s remaining)\t \n",
      "\tAUC: Toxic: 0.9469 Severe Toxic: 0.9698 Obscene: 0.9524 Threat: 0.9757 Insult: 0.9437 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9256 Severe Toxic: 0.9798 Obscene: 0.9367 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9672\n",
      "Batch 00467/500 (  296.5s remaining)\t \n",
      "\tAUC: Toxic: 0.9469 Severe Toxic: 0.9698 Obscene: 0.9524 Threat: 0.9757 Insult: 0.9438 Identity-based Hate: 0.9639\n",
      "\tAccuracy: Toxic: 0.9256 Severe Toxic: 0.9798 Obscene: 0.9368 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00468/500 (  287.5s remaining)\t \n",
      "\tAUC: Toxic: 0.9469 Severe Toxic: 0.9698 Obscene: 0.9524 Threat: 0.9757 Insult: 0.9438 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9256 Severe Toxic: 0.9798 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00469/500 (  278.5s remaining)\t \n",
      "\tAUC: Toxic: 0.9469 Severe Toxic: 0.9699 Obscene: 0.9522 Threat: 0.9757 Insult: 0.9439 Identity-based Hate: 0.9639\n",
      "\tAccuracy: Toxic: 0.9256 Severe Toxic: 0.9798 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00470/500 (  269.6s remaining)\t \n",
      "\tAUC: Toxic: 0.9468 Severe Toxic: 0.9699 Obscene: 0.9522 Threat: 0.9758 Insult: 0.9436 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9256 Severe Toxic: 0.9798 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00471/500 (  260.6s remaining)\t \n",
      "\tAUC: Toxic: 0.9469 Severe Toxic: 0.9700 Obscene: 0.9522 Threat: 0.9758 Insult: 0.9436 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9256 Severe Toxic: 0.9799 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9670\n",
      "Batch 00472/500 (  251.6s remaining)\t \n",
      "\tAUC: Toxic: 0.9468 Severe Toxic: 0.9700 Obscene: 0.9522 Threat: 0.9759 Insult: 0.9437 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9255 Severe Toxic: 0.9799 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9670\n",
      "Batch 00473/500 (  242.6s remaining)\t \n",
      "\tAUC: Toxic: 0.9468 Severe Toxic: 0.9701 Obscene: 0.9522 Threat: 0.9758 Insult: 0.9437 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9256 Severe Toxic: 0.9799 Obscene: 0.9369 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9670\n",
      "Batch 00474/500 (  233.6s remaining)\t \n",
      "\tAUC: Toxic: 0.9468 Severe Toxic: 0.9702 Obscene: 0.9522 Threat: 0.9759 Insult: 0.9437 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9798 Obscene: 0.9370 Threat: 0.9937 Insult: 0.9221 Identity-based Hate: 0.9670\n",
      "Batch 00475/500 (  224.7s remaining)\t \n",
      "\tAUC: Toxic: 0.9469 Severe Toxic: 0.9702 Obscene: 0.9522 Threat: 0.9758 Insult: 0.9436 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9798 Obscene: 0.9370 Threat: 0.9936 Insult: 0.9221 Identity-based Hate: 0.9670\n",
      "Batch 00476/500 (  215.7s remaining)\t \n",
      "\tAUC: Toxic: 0.9469 Severe Toxic: 0.9701 Obscene: 0.9523 Threat: 0.9759 Insult: 0.9435 Identity-based Hate: 0.9634\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9797 Obscene: 0.9371 Threat: 0.9936 Insult: 0.9220 Identity-based Hate: 0.9670\n",
      "Batch 00477/500 (  206.7s remaining)\t \n",
      "\tAUC: Toxic: 0.9469 Severe Toxic: 0.9701 Obscene: 0.9523 Threat: 0.9761 Insult: 0.9436 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9797 Obscene: 0.9371 Threat: 0.9936 Insult: 0.9220 Identity-based Hate: 0.9670\n",
      "Batch 00478/500 (  197.7s remaining)\t \n",
      "\tAUC: Toxic: 0.9469 Severe Toxic: 0.9701 Obscene: 0.9524 Threat: 0.9761 Insult: 0.9436 Identity-based Hate: 0.9635\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9797 Obscene: 0.9371 Threat: 0.9936 Insult: 0.9220 Identity-based Hate: 0.9670\n",
      "Batch 00479/500 (  188.7s remaining)\t \n",
      "\tAUC: Toxic: 0.9469 Severe Toxic: 0.9700 Obscene: 0.9523 Threat: 0.9760 Insult: 0.9437 Identity-based Hate: 0.9636\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9796 Obscene: 0.9371 Threat: 0.9937 Insult: 0.9220 Identity-based Hate: 0.9670\n",
      "Batch 00480/500 (  179.7s remaining)\t \n",
      "\tAUC: Toxic: 0.9470 Severe Toxic: 0.9697 Obscene: 0.9523 Threat: 0.9760 Insult: 0.9437 Identity-based Hate: 0.9636\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9796 Obscene: 0.9370 Threat: 0.9936 Insult: 0.9220 Identity-based Hate: 0.9670\n",
      "Batch 00481/500 (  170.8s remaining)\t \n",
      "\tAUC: Toxic: 0.9470 Severe Toxic: 0.9698 Obscene: 0.9523 Threat: 0.9760 Insult: 0.9437 Identity-based Hate: 0.9637\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9796 Obscene: 0.9370 Threat: 0.9937 Insult: 0.9220 Identity-based Hate: 0.9670\n",
      "Batch 00482/500 (  161.8s remaining)\t \n",
      "\tAUC: Toxic: 0.9470 Severe Toxic: 0.9699 Obscene: 0.9523 Threat: 0.9761 Insult: 0.9438 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9797 Obscene: 0.9371 Threat: 0.9937 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00483/500 (  152.8s remaining)\t \n",
      "\tAUC: Toxic: 0.9471 Severe Toxic: 0.9699 Obscene: 0.9523 Threat: 0.9761 Insult: 0.9438 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9797 Obscene: 0.9371 Threat: 0.9937 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00484/500 (  143.8s remaining)\t \n",
      "\tAUC: Toxic: 0.9471 Severe Toxic: 0.9700 Obscene: 0.9522 Threat: 0.9761 Insult: 0.9438 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9258 Severe Toxic: 0.9797 Obscene: 0.9370 Threat: 0.9937 Insult: 0.9222 Identity-based Hate: 0.9671\n",
      "Batch 00485/500 (  134.8s remaining)\t \n",
      "\tAUC: Toxic: 0.9471 Severe Toxic: 0.9700 Obscene: 0.9523 Threat: 0.9761 Insult: 0.9439 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9258 Severe Toxic: 0.9797 Obscene: 0.9370 Threat: 0.9937 Insult: 0.9222 Identity-based Hate: 0.9670\n",
      "Batch 00486/500 (  125.8s remaining)\t \n",
      "\tAUC: Toxic: 0.9472 Severe Toxic: 0.9702 Obscene: 0.9523 Threat: 0.9762 Insult: 0.9439 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9258 Severe Toxic: 0.9797 Obscene: 0.9369 Threat: 0.9937 Insult: 0.9222 Identity-based Hate: 0.9670\n",
      "Batch 00487/500 (  116.8s remaining)\t \n",
      "\tAUC: Toxic: 0.9471 Severe Toxic: 0.9702 Obscene: 0.9523 Threat: 0.9762 Insult: 0.9437 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9797 Obscene: 0.9368 Threat: 0.9937 Insult: 0.9221 Identity-based Hate: 0.9670\n",
      "Batch 00488/500 (  107.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9471 Severe Toxic: 0.9702 Obscene: 0.9523 Threat: 0.9763 Insult: 0.9438 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9258 Severe Toxic: 0.9796 Obscene: 0.9369 Threat: 0.9937 Insult: 0.9221 Identity-based Hate: 0.9670\n",
      "Batch 00489/500 (   98.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9471 Severe Toxic: 0.9701 Obscene: 0.9522 Threat: 0.9763 Insult: 0.9438 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9796 Obscene: 0.9369 Threat: 0.9937 Insult: 0.9221 Identity-based Hate: 0.9670\n",
      "Batch 00490/500 (   89.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9471 Severe Toxic: 0.9703 Obscene: 0.9523 Threat: 0.9764 Insult: 0.9439 Identity-based Hate: 0.9638\n",
      "\tAccuracy: Toxic: 0.9258 Severe Toxic: 0.9796 Obscene: 0.9369 Threat: 0.9937 Insult: 0.9221 Identity-based Hate: 0.9671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 00491/500 (   80.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9471 Severe Toxic: 0.9703 Obscene: 0.9523 Threat: 0.9764 Insult: 0.9438 Identity-based Hate: 0.9639\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9796 Obscene: 0.9368 Threat: 0.9937 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00492/500 (   71.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9472 Severe Toxic: 0.9704 Obscene: 0.9523 Threat: 0.9763 Insult: 0.9438 Identity-based Hate: 0.9640\n",
      "\tAccuracy: Toxic: 0.9258 Severe Toxic: 0.9797 Obscene: 0.9369 Threat: 0.9937 Insult: 0.9221 Identity-based Hate: 0.9670\n",
      "Batch 00493/500 (   62.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9472 Severe Toxic: 0.9704 Obscene: 0.9524 Threat: 0.9764 Insult: 0.9439 Identity-based Hate: 0.9640\n",
      "\tAccuracy: Toxic: 0.9258 Severe Toxic: 0.9796 Obscene: 0.9369 Threat: 0.9937 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00494/500 (   53.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9472 Severe Toxic: 0.9704 Obscene: 0.9525 Threat: 0.9764 Insult: 0.9439 Identity-based Hate: 0.9639\n",
      "\tAccuracy: Toxic: 0.9258 Severe Toxic: 0.9797 Obscene: 0.9370 Threat: 0.9937 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00495/500 (   44.9s remaining)\t \n",
      "\tAUC: Toxic: 0.9471 Severe Toxic: 0.9704 Obscene: 0.9524 Threat: 0.9765 Insult: 0.9439 Identity-based Hate: 0.9639\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9796 Obscene: 0.9369 Threat: 0.9937 Insult: 0.9221 Identity-based Hate: 0.9671\n",
      "Batch 00496/500 (   36.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9472 Severe Toxic: 0.9704 Obscene: 0.9524 Threat: 0.9765 Insult: 0.9439 Identity-based Hate: 0.9639\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9796 Obscene: 0.9369 Threat: 0.9937 Insult: 0.9222 Identity-based Hate: 0.9671\n",
      "Batch 00497/500 (   27.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9472 Severe Toxic: 0.9704 Obscene: 0.9525 Threat: 0.9765 Insult: 0.9439 Identity-based Hate: 0.9639\n",
      "\tAccuracy: Toxic: 0.9258 Severe Toxic: 0.9796 Obscene: 0.9369 Threat: 0.9937 Insult: 0.9223 Identity-based Hate: 0.9671\n",
      "Batch 00498/500 (   18.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9471 Severe Toxic: 0.9703 Obscene: 0.9525 Threat: 0.9765 Insult: 0.9436 Identity-based Hate: 0.9640\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9796 Obscene: 0.9369 Threat: 0.9937 Insult: 0.9222 Identity-based Hate: 0.9671\n",
      "Batch 00499/500 (    9.0s remaining)\t \n",
      "\tAUC: Toxic: 0.9470 Severe Toxic: 0.9705 Obscene: 0.9526 Threat: 0.9766 Insult: 0.9437 Identity-based Hate: 0.9641\n",
      "\tAccuracy: Toxic: 0.9257 Severe Toxic: 0.9796 Obscene: 0.9369 Threat: 0.9937 Insult: 0.9223 Identity-based Hate: 0.9672\n",
      "Test Values:\n",
      "\tAUC: Toxic: 0.9470 Severe Toxic: 0.9705 Obscene: 0.9526 Threat: 0.9766 Insult: 0.9437 Identity-based Hate: 0.9641\n",
      "\tAccuracy:  Toxic: 0.9257 Severe Toxic: 0.9796 Obscene: 0.9369 Threat: 0.9937 Insult: 0.9223 Identity-based Hate: 0.9672\n"
     ]
    }
   ],
   "source": [
    "log_frequency = 1\n",
    "\n",
    "clear_frequency = 100\n",
    "\n",
    "log_path = \"./logs/val_output_unfrozen.txt\"\n",
    "\n",
    "epoch_times = []\n",
    "batch_times = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "logger = EpochLogger(log_path)\n",
    "predicted = None\n",
    "true = None\n",
    "n_correct = 0\n",
    "n_processed = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"Testing\")\n",
    "    predicted = None\n",
    "    for i, (input_ids, encoded_position, comment_bounds, target) in enumerate(test):\n",
    "        batch_start = time()\n",
    "        # Get the BERT output\n",
    "        encoded_comments = bert(input_ids.to(device))[0]\n",
    "\n",
    "        # Get the outputs from the network\n",
    "        output = model(encoded_comments, encoded_position.to(device), comment_bounds)\n",
    "\n",
    "        #print(output[0], target[0])\n",
    "        # Gradient descent\n",
    "        pred = output.cpu().detach()\n",
    "        if predicted is None:\n",
    "            predicted = pred.clone()\n",
    "            true = target.clone()\n",
    "        else:\n",
    "            predicted = torch.cat((predicted, pred), dim=0)\n",
    "            true = torch.cat((true, target), dim=0)\n",
    "\n",
    "        n_correct += torch.sum(torch.round(pred) == target, axis=0).numpy()\n",
    "        n_processed += pred.shape[0]\n",
    "        \n",
    "        batch_times.append(time() - batch_start)\n",
    "\n",
    "        if i % log_frequency == 0:\n",
    "            logger.log(\"Batch {:05d}/{} ({:7.01f}s remaining)\\t \".format(i, len(test), np.mean(batch_times)*(len(test) - i)))\n",
    "            auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "            logger.log(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "            acc = n_correct / n_processed\n",
    "            logger.log(\"\\tAccuracy: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "\n",
    "        if i % clear_frequency == 0 and i != 0:\n",
    "            clear_output()\n",
    "\n",
    "    auc = calc_auc(predicted.numpy(), true.numpy())\n",
    "    acc = n_correct / n_processed\n",
    "    logger.log(\"Test Values:\")\n",
    "\n",
    "    logger.log(\"\\tAUC: Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*auc))\n",
    "\n",
    "    logger.log(\"\\tAccuracy:  Toxic: {:.04f} Severe Toxic: {:.04f} Obscene: {:.04f} Threat: {:.04f} Insult: {:.04f} Identity-based Hate: {:.04f}\".format(*acc))\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>fffcd0960ee309b5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>fffd7a9a6eb32c16</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>fffda9e8d6fafa9e</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>fffe8f1340a79fc2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>ffffce3fb183ee80</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0       00001cee341fdb12     -1            -1       -1      -1      -1   \n",
       "1       0000247867823ef7     -1            -1       -1      -1      -1   \n",
       "2       00013b17ad220c46     -1            -1       -1      -1      -1   \n",
       "3       00017563c3f7919a     -1            -1       -1      -1      -1   \n",
       "4       00017695ad8997eb     -1            -1       -1      -1      -1   \n",
       "...                  ...    ...           ...      ...     ...     ...   \n",
       "153159  fffcd0960ee309b5     -1            -1       -1      -1      -1   \n",
       "153160  fffd7a9a6eb32c16     -1            -1       -1      -1      -1   \n",
       "153161  fffda9e8d6fafa9e     -1            -1       -1      -1      -1   \n",
       "153162  fffe8f1340a79fc2     -1            -1       -1      -1      -1   \n",
       "153163  ffffce3fb183ee80     -1            -1       -1      -1      -1   \n",
       "\n",
       "        identity_hate  \n",
       "0                  -1  \n",
       "1                  -1  \n",
       "2                  -1  \n",
       "3                  -1  \n",
       "4                  -1  \n",
       "...               ...  \n",
       "153159             -1  \n",
       "153160             -1  \n",
       "153161             -1  \n",
       "153162             -1  \n",
       "153163             -1  \n",
       "\n",
       "[153164 rows x 7 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = pd.read_csv(\"./test_labels.csv\")\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_indices = test_labels.toxic != -1\n",
    "columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_pred = np.zeros((len(test_labels), 6))\n",
    "masked_pred[score_indices] = predicted.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[columns] = masked_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>6.557048e-08</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>0.032808</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.012107</td>\n",
       "      <td>4.137385e-04</td>\n",
       "      <td>0.039888</td>\n",
       "      <td>0.050892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0002f87b16116a7f</td>\n",
       "      <td>0.947678</td>\n",
       "      <td>0.061841</td>\n",
       "      <td>0.965423</td>\n",
       "      <td>9.469392e-01</td>\n",
       "      <td>0.108079</td>\n",
       "      <td>0.023580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0003e1cccfd5a40a</td>\n",
       "      <td>0.996707</td>\n",
       "      <td>0.152949</td>\n",
       "      <td>0.773939</td>\n",
       "      <td>2.561683e-03</td>\n",
       "      <td>0.731999</td>\n",
       "      <td>0.999454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00059ace3e3e9a53</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.003412</td>\n",
       "      <td>5.981569e-04</td>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153150</th>\n",
       "      <td>fff8f64043129fa2</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>1.547969e-04</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153151</th>\n",
       "      <td>fff9d70fe0722906</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.014651</td>\n",
       "      <td>2.412718e-06</td>\n",
       "      <td>0.092024</td>\n",
       "      <td>0.007202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153154</th>\n",
       "      <td>fffa8a11c4378854</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>7.742472e-07</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153155</th>\n",
       "      <td>fffac2a094c8e0e2</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>1.342758e-05</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153156</th>\n",
       "      <td>fffb5451268fb5ba</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>1.103259e-06</td>\n",
       "      <td>0.002841</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63978 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id     toxic  severe_toxic   obscene        threat  \\\n",
       "5       0001ea8717f6de06  0.000009      0.000017  0.000079  6.557048e-08   \n",
       "7       000247e83dcc1211  0.032808      0.001187  0.012107  4.137385e-04   \n",
       "11      0002f87b16116a7f  0.947678      0.061841  0.965423  9.469392e-01   \n",
       "13      0003e1cccfd5a40a  0.996707      0.152949  0.773939  2.561683e-03   \n",
       "14      00059ace3e3e9a53  0.000248      0.000021  0.003412  5.981569e-04   \n",
       "...                  ...       ...           ...       ...           ...   \n",
       "153150  fff8f64043129fa2  0.000081      0.000007  0.002478  1.547969e-04   \n",
       "153151  fff9d70fe0722906  0.010101      0.000959  0.014651  2.412718e-06   \n",
       "153154  fffa8a11c4378854  0.000020      0.000019  0.000014  7.742472e-07   \n",
       "153155  fffac2a094c8e0e2  0.000020      0.000002  0.000737  1.342758e-05   \n",
       "153156  fffb5451268fb5ba  0.000123      0.000010  0.005950  1.103259e-06   \n",
       "\n",
       "          insult  identity_hate  \n",
       "5       0.000125       0.000001  \n",
       "7       0.039888       0.050892  \n",
       "11      0.108079       0.023580  \n",
       "13      0.731999       0.999454  \n",
       "14      0.002161       0.000008  \n",
       "...          ...            ...  \n",
       "153150  0.002450       0.000035  \n",
       "153151  0.092024       0.007202  \n",
       "153154  0.000042       0.000005  \n",
       "153155  0.000976       0.000012  \n",
       "153156  0.002841       0.000310  \n",
       "\n",
       "[63978 rows x 7 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[score_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.to_csv(\"results_frozen.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
